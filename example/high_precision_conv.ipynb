{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/binary_weights/example/high_precision_conv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv9XZ7bzwLIR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOyd0FJMbWki"
      },
      "source": [
        "# **SNN with binary weights**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "S733HbldaB40"
      },
      "source": [
        "# Gradient-based Learning in Convolutional Spiking Neural Networks\n",
        "In this tutorial, we'll use a convolutional neural network (CNN) to classify the MNIST dataset.\n",
        "We will use the backpropagation through time (BPTT) algorithm to do so. This tutorial is largely the same as tutorial 2, just with a different network architecture to show how to integrate convolutions with snnTorch.\n",
        "\n",
        "If running in Google Colab:\n",
        "* Ensure you are connected to GPU by checking Runtime > Change runtime type > Hardware accelerator: GPU\n",
        "* Next, install the Test PyPi distribution of snnTorch by clicking into the following cell and pressing `Shift+Enter`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YgGUk8EJaB41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb8fffa-c3bb-46b2-a145-eb9eefa62e73"
      },
      "source": [
        "# Install the test PyPi Distribution of snntorch\n",
        "!pip install snntorch\n",
        "# !pip uninstall snntorch\n",
        "# !pip install git+https://github.com/jeshraghian/snntorch.git@binary_weights#egg=snntorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting snntorch\n",
            "  Downloading https://files.pythonhosted.org/packages/80/22/002ee501405b978a581730adba87ecc7c409dd0c460c625f7a2e903a1dcc/snntorch-0.2.9-py2.py3-none-any.whl\n",
            "Collecting celluloid\n",
            "  Downloading https://files.pythonhosted.org/packages/60/a7/7fbe80721c6f1b7370c4e50c77abe31b4d5cfeb58873d4d32f48ae5a0bae/celluloid-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from snntorch) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.1.5)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->snntorch) (2018.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->snntorch) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->snntorch) (1.15.0)\n",
            "Installing collected packages: celluloid, snntorch\n",
            "Successfully installed celluloid-0.2.0 snntorch-0.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_UAKn7UoaB41"
      },
      "source": [
        "## 1. Setting up the Static MNIST Dataset\n",
        "### 1.1. Import packages and setup environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JGneJnSiaB42"
      },
      "source": [
        "import snntorch as snn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PWL3FQSSaB42"
      },
      "source": [
        "### 1.2 Define network and SNN parameters\n",
        "We will use a 2conv-2MaxPool-FCN architecture for a sequence of 25 time steps.\n",
        "\n",
        "* `alpha` is the decay rate of the synaptic current of a neuron\n",
        "* `beta` is the decay rate of the membrane potential of a neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kvzf29tIaB42"
      },
      "source": [
        "# Network Architecture\n",
        "num_inputs = 28*28\n",
        "num_outputs = 10\n",
        "num_hidden = 1000\n",
        "# Training Parameters\n",
        "batch_size=128\n",
        "data_path='/data/mnist'\n",
        "\n",
        "# Temporal Dynamics\n",
        "num_steps = 25\n",
        "time_step = 1e-3\n",
        "# tau_mem = 6.5e-4\n",
        "# tau_syn = 5.5e-4\n",
        "# alpha = float(np.exp(-time_step/tau_syn))\n",
        "# beta = float(np.exp(-time_step/tau_mem))\n",
        "alpha = 0.3\n",
        "beta = 0.3\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "4_5wuLk0aB43"
      },
      "source": [
        "### 1.3 Download MNIST Dataset\n",
        "To see how to construct a validation set, refer to Tutorial 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WJO7iy-7aB43"
      },
      "source": [
        "# Define a transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6_guKA__aB44"
      },
      "source": [
        "### 1.4 Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cQzrUSKRaB44"
      },
      "source": [
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jHBwr32UaB44"
      },
      "source": [
        "## 2. Define Network\n",
        "To define our network, we will import two functions from the `snntorch.neuron` module, which contains a series of neuron models and related functions.\n",
        "snnTorch treats neurons as activations with recurrent connections, so that it integrates smoothly with PyTorch's pre-existing layer functions.\n",
        "* `snntorch.neuron.LIF` is a simple Leaky Integrate and Fire (LIF) neuron. Specifically, it uses Stein's model which assumes instantaneous rise times for synaptic current and membrane potential.\n",
        "* `snntorch.neuron.FastSigmoidSurrogate` defines separate forward and backward functions. The forward function is a Heaviside step function for spike generation. The backward function is the derivative of a fast sigmoid function, to ensure continuous differentiability.\n",
        "FSS is mostly derived from:\n",
        "\n",
        ">Neftci, E. O., Mostafa, H., and Zenke, F. (2019) Surrogate Gradient Learning in Spiking Neural Networks. https://arxiv.org/abs/1901/09948\n",
        "\n",
        "`snn.neuron.slope` is a variable that defines the slope of the backward surrogate.\n",
        "TO-DO: Include visualisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2JKR_90daB45",
        "outputId": "541baa2d-61ae-40e5-c362-1b8d0ca384b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "spike_grad = snn.FastSigmoidSurrogate.apply\n",
        "snn.slope = 50"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1502cfba5f20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspike_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastSigmoidSurrogate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'snntorch' has no attribute 'FastSigmoidSurrogate'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LaXzsI6baB45"
      },
      "source": [
        "Now we can define our SNN. Defining an instance of `LIF` requires three arguments: 1) the surrogate spiking function, 2) $I_{syn}$ decay rate, $\\alpha$, and 3) $V_{mem}$ decay rate, $\\beta$.\n",
        "\n",
        "The LIF neuron is simply treated as a recurrent activation. It requires initialization of the post-synaptic spikes `spk1` and `spk2`, the synaptic current `syn1` and `syn2`, and the membrane potential `mem1` and `mem2`.\n",
        "\n",
        "We will use the final layer spikes and membrane for determining loss and accuracy, so we will record and return their histories in `spk3_rec` and `mem3_rec`.\n",
        "\n",
        "Keep in mind, the dataset we are using is just static MNIST. I.e., it is *not* time-varying.\n",
        "Therefore, we pass the same MNIST sample to the input at each time step.\n",
        "This is handled in the line `cur1 = F.max_pool2d(self.conv1(x), 2)`, where `x` is the same input over the whole for-loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zAnnWfGyfhkf"
      },
      "source": [
        "### Binarized Layer Modules\n",
        "``Binarize`` converts weights to {-1, 1}.\n",
        "Remove `.mul_(2).add_(1)` for {0, 1}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "blVwJakcaB46"
      },
      "source": [
        "import pdb\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def Binarize(tensor,quant_mode='det'):\n",
        "    if quant_mode=='det':\n",
        "        return tensor.sign()\n",
        "        # tmp = tensor.clone()\n",
        "        # tmp[tensor>0] = 1\n",
        "        # tmp[tensor==0] = 0\n",
        "        # tmp[tensor<0] = -1\n",
        "        # return tmp\n",
        "    else:\n",
        "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
        "\n",
        "\n",
        "class HingeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HingeLoss,self).__init__()\n",
        "        self.margin=1.0\n",
        "\n",
        "    def hinge_loss(self,input,target):\n",
        "            #import pdb; pdb.set_trace()\n",
        "            output=self.margin-input.mul(target)\n",
        "            output[output.le(0)]=0\n",
        "            return output.mean()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return self.hinge_loss(input,target)\n",
        "\n",
        "class SqrtHingeLossFunction(Function):\n",
        "    def __init__(self):\n",
        "        super(SqrtHingeLossFunction,self).__init__()\n",
        "        self.margin=1.0\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        output=self.margin-input.mul(target)\n",
        "        output[output.le(0)]=0\n",
        "        self.save_for_backward(input, target)\n",
        "        loss=output.mul(output).sum(0).sum(1).div(target.numel())\n",
        "        return loss\n",
        "\n",
        "    def backward(self,grad_output):\n",
        "       input, target = self.saved_tensors\n",
        "       output=self.margin-input.mul(target)\n",
        "       output[output.le(0)]=0\n",
        "       import pdb; pdb.set_trace()\n",
        "       grad_output.resize_as_(input).copy_(target).mul_(-2).mul_(output)\n",
        "       grad_output.mul_(output.ne(0).float())\n",
        "       grad_output.div_(input.numel())\n",
        "       return grad_output,grad_output\n",
        "\n",
        "def Quantize(tensor,quant_mode='det',  params=None, numBits=8):\n",
        "    tensor.clamp_(-2**(numBits-1),2**(numBits-1))\n",
        "    if quant_mode=='det':\n",
        "        tensor=tensor.mul(2**(numBits-1)).round().div(2**(numBits-1))\n",
        "    else:\n",
        "        tensor=tensor.mul(2**(numBits-1)).round().add(torch.rand(tensor.size()).add(-0.5)).div(2**(numBits-1))\n",
        "        quant_fixed(tensor, params)\n",
        "    return tensor\n",
        "\n",
        "# import torch.nn._functions as tnnf\n",
        "\n",
        "\n",
        "class BinarizeLinear(nn.Linear):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        if input.size(1) != 784:\n",
        "            input.data=Binarize(input.data)\n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()\n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "        out = nn.functional.linear(input, self.weight)\n",
        "        if not self.bias is None:\n",
        "            self.bias.org=self.bias.data.clone()\n",
        "            out += self.bias.view(1, -1).expand_as(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BinarizeConv2d(nn.Conv2d):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(BinarizeConv2d, self).__init__(*kargs, **kwargs)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.size(1) != 3:\n",
        "            input.data = Binarize(input.data)\n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()\n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "\n",
        "        out = nn.functional.conv2d(input, self.weight, None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "\n",
        "        if not self.bias is None:\n",
        "            self.bias.org=self.bias.data.clone()\n",
        "            out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ZNySTB6wfhkk"
      },
      "source": [
        "Network: Low precision forward pass, high precision backprop (Additional Mod in Optimization method)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuKfWeZrh50P"
      },
      "source": [
        "class LIF(nn.Module):\n",
        "    \"\"\"Parent class for leaky integrate and fire neuron models.\"\"\"\n",
        "    instances = []\n",
        "    def __init__(self, alpha, beta, threshold=1.0, spike_grad=None):\n",
        "        super(LIF, self).__init__()\n",
        "        LIF.instances.append(self)\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.threshold = threshold\n",
        "\n",
        "        if spike_grad is None:\n",
        "            self.spike_grad = self.Heaviside.apply\n",
        "        else:\n",
        "            self.spike_grad = spike_grad\n",
        "\n",
        "    def fire(self, mem):\n",
        "        \"\"\"Generates spike if mem > threshold.\n",
        "        Returns spk and reset.\"\"\"\n",
        "        mem_shift = mem - self.threshold\n",
        "        spk = self.spike_grad(mem_shift).to(device)\n",
        "        reset = torch.zeros_like(mem)\n",
        "        spk_idx = (mem_shift > 0)\n",
        "        reset[spk_idx] = torch.ones_like(mem)[spk_idx]\n",
        "        return spk, reset\n",
        "\n",
        "    def fire_single(self, mem):\n",
        "        \"\"\"Generates spike if mem > threshold.\n",
        "        Returns spk and reset.\"\"\"\n",
        "        mem_shift = mem - self.threshold\n",
        "        \n",
        "        index = torch.argmax(mem_shift, dim=-1)\n",
        "        \n",
        "        spk_tmp = self.spike_grad(mem_shift)\n",
        "\n",
        "        mask_spk1 = torch.zeros_like(spk_tmp)\n",
        "        # print(mem.size())\n",
        "        # print(index.size())\n",
        "        mask_spk1[torch.arange(mem_shift.size()[0]), index] = 1\n",
        "        spk = (spk_tmp * mask_spk1).to(device)\n",
        "        # print(spk[0])\n",
        "        reset = torch.zeros_like(mem)\n",
        "        spk_idx = (mem_shift > 0)\n",
        "        reset[spk_idx] = torch.ones_like(mem)[spk_idx]\n",
        "        return spk, reset\n",
        "\n",
        "    @classmethod\n",
        "    def clear_instances(cls):\n",
        "      cls.instances = []\n",
        "\n",
        "    @staticmethod\n",
        "    def init_stein(batch_size, *args):\n",
        "        \"\"\"Used to initialize syn, mem and spk.\n",
        "        *args are the input feature dimensions.\n",
        "        E.g., batch_size=128 and input feature of size=1x28x28 would require init_hidden(128, 1, 28, 28).\"\"\"\n",
        "        syn = torch.zeros((batch_size, *args), device=device, dtype=dtype)\n",
        "        mem = torch.zeros((batch_size, *args), device=device, dtype=dtype)\n",
        "        spk = torch.zeros((batch_size, *args), device=device, dtype=dtype)\n",
        "\n",
        "        return spk, syn, mem\n",
        "\n",
        "    @staticmethod\n",
        "    def init_srm0(batch_size, *args):\n",
        "        \"\"\"Used to initialize syn_pre, syn_post, mem and spk.\n",
        "        *args are the input feature dimensions.\n",
        "        E.g., batch_size=128 and input feature of size=1x28x28 would require init_hidden(128, 1, 28, 28).\"\"\"\n",
        "        syn_pre = torch.zeros((batch_size, *args), device=device, dtype=dtype)\n",
        "        syn_post = torch.zeros((batch_size, *args), device=device, dtype=dtype)\n",
        "        mem = torch.zeros((batch_size, *args), device=device, dtype=dtype)\n",
        "        spk = torch.zeros((batch_size, *args), device=device, dtype=dtype)\n",
        "\n",
        "        return spk, syn_pre, syn_post, mem\n",
        "\n",
        "    @staticmethod\n",
        "    def detach(*args):\n",
        "        \"\"\"Used to detach input arguments from the current graph.\n",
        "        Intended for use in truncated backpropagation through time where hidden state variables are global variables.\"\"\"\n",
        "        for state in args:\n",
        "            state.detach_()\n",
        "\n",
        "    @staticmethod\n",
        "    def zeros(*args):\n",
        "        \"\"\"Used to clear hidden state variables to zero.\n",
        "            Intended for use where hidden state variables are global variables.\"\"\"\n",
        "        for state in args:\n",
        "            state = torch.zeros_like(state)\n",
        "\n",
        "    @staticmethod\n",
        "    class Heaviside(torch.autograd.Function):\n",
        "        \"\"\"Default and non-approximate spiking function for neuron.\n",
        "        Forward pass: Heaviside step function.\n",
        "        Backward pass: Dirac Delta clipped to 1 at x>0 instead of inf at x=1.\n",
        "        This assumption holds true on the basis that a spike occurs as long as x>0 and the following time step incurs a reset.\"\"\"\n",
        "\n",
        "        @staticmethod\n",
        "        def forward(ctx, input_):\n",
        "            ctx.save_for_backward(input_)\n",
        "            out = torch.zeros_like(input_)\n",
        "            out[input_ > 0] = 1.0\n",
        "            return out\n",
        "\n",
        "        @staticmethod\n",
        "        def backward(ctx, grad_output):\n",
        "            input_, = ctx.saved_tensors\n",
        "            grad_input = grad_output.clone()\n",
        "            grad_input[input_ < 0] = 0.0\n",
        "            grad = grad_input\n",
        "            return grad\n",
        "\n",
        "class Stein_single(LIF):\n",
        "    \"\"\"\n",
        "    Stein's model of the leaky integrate and fire neuron.\n",
        "    The synaptic current jumps upon spike arrival, which causes a jump in membrane potential.\n",
        "    Synaptic current and membrane potential decay exponentially with rates of alpha and beta, respectively.\n",
        "    For mem[T] > threshold, spk[T+1] = 0 to account for axonal delay.\n",
        "\n",
        "    For further reading, see:\n",
        "    R. B. Stein (1965) A theoretical analysis of neuron variability. Biophys. J. 5, pp. 173-194.\n",
        "    R. B. Stein (1967) Some models of neuronal variability. Biophys. J. 7. pp. 37-68.\"\"\"\n",
        "\n",
        "    def __init__(self, alpha, beta, threshold=1.0, num_inputs=False, spike_grad=None, batch_size=False, hidden_init=False):\n",
        "        super(Stein_single, self).__init__(alpha, beta, threshold, spike_grad)\n",
        "\n",
        "        self.num_inputs = num_inputs\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_init = hidden_init\n",
        "\n",
        "        if self.hidden_init:\n",
        "            if not self.num_inputs:\n",
        "                raise ValueError(\"num_inputs must be specified to initialize hidden states as instance variables.\")\n",
        "            elif not self.batch_size:\n",
        "                raise ValueError(\"batch_size must be specified to initialize hidden states as instance variables.\")\n",
        "            elif hasattr(self.num_inputs, '__iter__'):\n",
        "                self.spk, self.syn, self.mem = self.init_stein(self.batch_size, *(self.num_inputs)) # need to automatically call batch_size\n",
        "            else:\n",
        "                self.spk, self.syn, self.mem = self.init_stein(self.batch_size, self.num_inputs)\n",
        "\n",
        "    def forward(self, input_, syn, mem):\n",
        "        if not self.hidden_init:\n",
        "            spk, reset = self.fire(mem)\n",
        "            # input_[input_>1] = 1\n",
        "            # input_[input_<-1] = -1\n",
        "            syn = self.alpha * syn + input_\n",
        "            mem = self.beta * mem + syn - reset\n",
        "\n",
        "            return spk, syn, mem\n",
        "\n",
        "        # intended for truncated-BPTT where instance variables are hidden states\n",
        "        if self.hidden_init:\n",
        "            self.spk, self.reset = self.fire(self.mem)\n",
        "            self.syn = self.alpha * self.syn + input_\n",
        "            self.mem = self.beta * self.mem + self.syn - self.reset\n",
        "\n",
        "            return self.spk, self.syn, self.mem\n",
        "\n",
        "    @classmethod\n",
        "    def detach_hidden(cls):\n",
        "        \"\"\"Used to detach hidden states from the current graph.\n",
        "        Intended for use in truncated backpropagation through time where hidden state variables are instance variables.\"\"\"\n",
        "\n",
        "        for layer in range(len(cls.instances)):\n",
        "            cls.instances[layer].spk.detach_()\n",
        "            cls.instances[layer].syn.detach_()\n",
        "            cls.instances[layer].mem.detach_()\n",
        "\n",
        "    @classmethod\n",
        "    def zeros_hidden(cls):\n",
        "        \"\"\"Used to clear hidden state variables to zero.\n",
        "        Intended for use where hidden state variables are instance variables.\"\"\"\n",
        "\n",
        "        for layer in range(len(cls.instances)):\n",
        "            cls.instances[layer].spk = torch.zeros_like(cls.instances[layer].spk)\n",
        "            cls.instances[layer].syn = torch.zeros_like(cls.instances[layer].syn)\n",
        "            cls.instances[layer].mem = torch.zeros_like(cls.instances[layer].mem)\n"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPquZP_ZSMrP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZAxmcjuefhkn"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # initialize layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "        self.lif1 = Stein_single(alpha=alpha, beta=beta)\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=64, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "        self.lif2 = Stein_single(alpha=alpha, beta=beta)\n",
        "        self.fc2 = nn.Linear(64*5*5, 10, bias= False)\n",
        "        self.lif3 = Stein_single(alpha=alpha, beta=beta)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize LIF state variables and spike output tensors\n",
        "        spk1, syn1, mem1 = self.lif1.init_stein(batch_size, 12, 13, 13)\n",
        "        spk2, syn2, mem2 = self.lif1.init_stein(batch_size, 64, 5, 5)\n",
        "        spk3, syn3, mem3 = self.lif2.init_stein(batch_size, 10)\n",
        "\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = F.max_pool2d(self.conv1(x), 2)\n",
        "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "\n",
        "            cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
        "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "\n",
        "            cur3 = self.fc2(self.dropout(spk2.view(batch_size, -1)))\n",
        "            spk3, syn3, mem3 = self.lif3(cur3, syn3, mem3)\n",
        "\n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "net = Net().to(device)"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FOJTim88fhkn"
      },
      "source": [
        "Old network (if using above net, don't run the following code block)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1CFB6LAJaB47"
      },
      "source": [
        "## 3. Training\n",
        "Time for training! Let's first define a couple of functions to print out test/train accuracy for each minibatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7V0_NGKSaB48"
      },
      "source": [
        "def print_batch_accuracy(data, targets, train=False):\n",
        "    # output, _ = net(data.view(batch_size, -1))\n",
        "    output, _ = net(data.view(batch_size, 1, 28, 28))\n",
        "    _, am = output.sum(dim=0).max(1)\n",
        "    acc = np.mean((targets == am). detach().cpu().numpy())\n",
        "    if train is True:\n",
        "        print(f\"Train Set Accuracy: {acc}\")\n",
        "    else:\n",
        "        print(f\"Test Set Accuracy: {acc}\")\n",
        "\n",
        "def train_printer():\n",
        "    print(f\"Epoch {epoch}, Minibatch {minibatch_counter}\")\n",
        "    print(f\"Train Set Loss: {loss_hist[counter]}\")\n",
        "    print(f\"Test Set Loss: {test_loss_hist[counter]}\")\n",
        "    print_batch_accuracy(data_it, targets_it, train=True)\n",
        "    print_batch_accuracy(testdata_it, testtargets_it, train=False)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9uVWFKdXaB48"
      },
      "source": [
        "### 3.1 Optimizer & Loss\n",
        "We'll apply the softmax function to the membrane potentials of the output layer in calculating a negative log-likelihood loss.\n",
        "The Adam optimizer is used for weight updates.\n",
        "\n",
        "Accuracy is measured by counting the spikes of the output neurons. The neuron that fires the most frequently will be our predicted class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "I4BNFQkBaB48"
      },
      "source": [
        "lr = 3e-3\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
        "loss_fn = nn.NLLLoss()\n"
      ],
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0enleDrOaB49"
      },
      "source": [
        "### 3.2 Training Loop\n",
        "Now just sit back, relax, and wait for convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "JpdYo0G6fhkp"
      },
      "source": [
        "High precision BPTT training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "P225z9fafhkq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37204ea1-c626-4ad3-83d8-a9b78c31133e"
      },
      "source": [
        "import math\n",
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "print(f\"initial_lr: {lr}\")\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    if epoch < 5: \n",
        "      new_lr = lr * (0.85 ** epoch)\n",
        "      # new_lr = lr * (0.95**epoch)\n",
        "    else: new_lr = lr * (0.85**4) * (0.8**(epoch-4))\n",
        "    # lr = lr\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(10):\n",
        "\n",
        "    minibatch_counter = 0\n",
        "    train_batch = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data_it, targets_it in train_batch:\n",
        "        data_it = data_it.to(device)\n",
        "        targets_it = targets_it.to(device)\n",
        "\n",
        "        # output, mem_rec = net(data_it.view(batch_size, -1))\n",
        "        output, mem_rec = net(data_it.view(batch_size, 1, 28, 28)) # [28x28] or [1x28x28]?\n",
        "        log_p_y = log_softmax_fn(mem_rec)\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "\n",
        "        # Sum loss over time steps to perform BPTT\n",
        "        for step in range(num_steps):\n",
        "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
        "\n",
        "        # adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "        # BNN OPTimization\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        # for p in list(net.parameters()):\n",
        "        #         if hasattr(p,'org'):\n",
        "        #             p.data.copy_(p.org)\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        # for p in list(net.parameters()):\n",
        "        #         if hasattr(p,'org'):\n",
        "        #             p.org.copy_(p.data.clamp_(-1,1))\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        test_data = itertools.cycle(test_loader)\n",
        "        testdata_it, testtargets_it = next(test_data)\n",
        "        testdata_it = testdata_it.to(device)\n",
        "        testtargets_it = testtargets_it.to(device)\n",
        "\n",
        "        # Test set forward pass\n",
        "        # test_output, test_mem_rec = net(testdata_it.view(batch_size, -1))\n",
        "        test_output, test_mem_rec = net(testdata_it.view(batch_size, 1, 28, 28))\n",
        "\n",
        "        # Test set loss\n",
        "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
        "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
        "        loss_val_test = loss_fn(log_p_ytest, testtargets_it)\n",
        "        test_loss_hist.append(loss_val_test.item())\n",
        "\n",
        "        # Print test/train loss/accuracy\n",
        "        if counter % 50 == 0:\n",
        "          train_printer()\n",
        "        minibatch_counter += 1\n",
        "        counter += 1\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      net.eval()\n",
        "      for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # If current batch matches batch_size, just do the usual thing\n",
        "        if images.size()[0] == batch_size:\n",
        "          # outputs, _ = net(images.view(batch_size, -1))\n",
        "          outputs, _ = net(images.view(batch_size, 1, 28, 28))\n",
        "\n",
        "        # If current batch does not match batch_size (e.g., is the final minibatch),\n",
        "        # modify batch_size in a temp variable and restore it at the end of the else block\n",
        "        else:\n",
        "          temp_bs = batch_size\n",
        "          batch_size = images.size()[0]\n",
        "          # outputs, _ = net(images.view(images.size()[0], -1))\n",
        "          outputs, _ = net(images.view(images.size()[0], 1, 28, 28))\n",
        "          batch_size = temp_bs\n",
        "\n",
        "        _, predicted = outputs.sum(dim=0).max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
        "    print(f\"Test Set Accuracy: {100 * correct / total}%\")\n",
        "\n",
        "loss_hist_true_grad = loss_hist\n",
        "test_loss_hist_true_grad = test_loss_hist"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial_lr: 0.003\n",
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 57.56440734863281\n",
            "Test Set Loss: 57.57048797607422\n",
            "Train Set Accuracy: 0.1328125\n",
            "Test Set Accuracy: 0.0859375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 50\n",
            "Train Set Loss: 33.98584747314453\n",
            "Test Set Loss: 34.311134338378906\n",
            "Train Set Accuracy: 0.109375\n",
            "Test Set Accuracy: 0.1640625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 100\n",
            "Train Set Loss: 33.4441032409668\n",
            "Test Set Loss: 31.04863739013672\n",
            "Train Set Accuracy: 0.125\n",
            "Test Set Accuracy: 0.09375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 150\n",
            "Train Set Loss: 33.2514533996582\n",
            "Test Set Loss: 30.19699478149414\n",
            "Train Set Accuracy: 0.125\n",
            "Test Set Accuracy: 0.1875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 200\n",
            "Train Set Loss: 31.092679977416992\n",
            "Test Set Loss: 27.07733726501465\n",
            "Train Set Accuracy: 0.109375\n",
            "Test Set Accuracy: 0.109375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 250\n",
            "Train Set Loss: 29.47588539123535\n",
            "Test Set Loss: 24.846925735473633\n",
            "Train Set Accuracy: 0.1484375\n",
            "Test Set Accuracy: 0.09375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 300\n",
            "Train Set Loss: 29.2012882232666\n",
            "Test Set Loss: 25.331663131713867\n",
            "Train Set Accuracy: 0.109375\n",
            "Test Set Accuracy: 0.2109375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 350\n",
            "Train Set Loss: 28.27756690979004\n",
            "Test Set Loss: 25.52053451538086\n",
            "Train Set Accuracy: 0.109375\n",
            "Test Set Accuracy: 0.125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 400\n",
            "Train Set Loss: 25.377429962158203\n",
            "Test Set Loss: 25.955810546875\n",
            "Train Set Accuracy: 0.0625\n",
            "Test Set Accuracy: 0.1171875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 450\n",
            "Train Set Loss: 28.821693420410156\n",
            "Test Set Loss: 23.92669677734375\n",
            "Train Set Accuracy: 0.1171875\n",
            "Test Set Accuracy: 0.109375\n",
            "\n",
            "\n",
            "Total correctly classified test set images: 1308/10000\n",
            "Test Set Accuracy: 13.08%\n",
            "Epoch 1, Minibatch 32\n",
            "Train Set Loss: 27.15849494934082\n",
            "Test Set Loss: 26.178504943847656\n",
            "Train Set Accuracy: 0.1484375\n",
            "Test Set Accuracy: 0.171875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 82\n",
            "Train Set Loss: 26.25025177001953\n",
            "Test Set Loss: 20.750003814697266\n",
            "Train Set Accuracy: 0.078125\n",
            "Test Set Accuracy: 0.109375\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-258-d27a864a9066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Test set forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# test_output, test_mem_rec = net(testdata_it.view(batch_size, -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mem_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata_it\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Test set loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-255-1d553f409be3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mcur2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mspk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlif2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mcur3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-242-61dcaa4cb50f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, syn, mem)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_init\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mspk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;31m# input_[input_>1] = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# input_[input_<-1] = -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-242-61dcaa4cb50f>\u001b[0m in \u001b[0;36mfire\u001b[0;34m(self, mem)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmem_shift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mspk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspike_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mspk_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmem_shift\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mreset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspk_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspk_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1KZ7WVqhaB4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a631f3-0c9e-4181-9665-099d302519bb"
      },
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "  net.eval()\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # If current batch matches batch_size, just do the usual thing\n",
        "    if images.size()[0] == batch_size:\n",
        "      # outputs, _ = net(images.view(batch_size, -1))\n",
        "      outputs, _ = net(images.view(batch_size, 1, 28, 28))\n",
        "\n",
        "    # If current batch does not match batch_size (e.g., is the final minibatch),\n",
        "    # modify batch_size in a temp variable and restore it at the end of the else block\n",
        "    else:\n",
        "      temp_bs = batch_size\n",
        "      batch_size = images.size()[0]\n",
        "      # outputs, _ = net(images.view(images.size()[0], -1))\n",
        "      outputs, _ = net(images.view(images.size()[0], 1, 28, 28))\n",
        "      batch_size = temp_bs\n",
        "\n",
        "    _, predicted = outputs.sum(dim=0).max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
        "print(f\"Test Set Accuracy: {100 * correct / total}%\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total correctly classified test set images: 7929/10000\n",
            "Test Set Accuracy: 79.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbK4B8kmq6kb",
        "outputId": "a799ff97-614f-433f-cd40-334314b5a2b6"
      },
      "source": [
        "def adjust_learning_rate(ini_lr, lr_decay, optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    # if epoch< 4:\n",
        "    new_lr = ini_lr * (lr_decay ** epoch)\n",
        "    \n",
        "    # else: new_lr = lr\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "\n",
        "for lr_factor in range(5):\n",
        "  lr_decay = (95 - lr_factor * 5) / 100\n",
        "  net = Net().to(device)\n",
        "  lr = 2e-4\n",
        "  loss_hist = []\n",
        "  test_loss_hist = []\n",
        "  counter = 0\n",
        "  print(f\"initial_lr: {lr}\")\n",
        "  print(f\"lr_decay: {lr_decay}\")\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "  log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
        "  loss_fn = nn.NLLLoss()\n",
        "  for epoch in range(10):\n",
        "\n",
        "      minibatch_counter = 0\n",
        "      train_batch = iter(train_loader)\n",
        "\n",
        "      # Minibatch training loop\n",
        "      for data_it, targets_it in train_batch:\n",
        "          data_it = data_it.to(device)\n",
        "          targets_it = targets_it.to(device)\n",
        "\n",
        "          # output, mem_rec = net(data_it.view(batch_size, -1))\n",
        "          output, mem_rec = net(data_it.view(batch_size, 1, 28, 28)) # [28x28] or [1x28x28]?\n",
        "          log_p_y = log_softmax_fn(mem_rec)\n",
        "          loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "\n",
        "          # Sum loss over time steps to perform BPTT\n",
        "          for step in range(num_steps):\n",
        "            loss_val += loss_fn(log_p_y[step], targets_it)\n",
        "\n",
        "          adjust_learning_rate(lr, lr_decay, optimizer, epoch)\n",
        "\n",
        "          # BNN OPTimization\n",
        "          optimizer.zero_grad()\n",
        "          loss_val.backward()\n",
        "          for p in list(net.parameters()):\n",
        "                  if hasattr(p,'org'):\n",
        "                      p.data.copy_(p.org)\n",
        "          nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "          optimizer.step()\n",
        "          for p in list(net.parameters()):\n",
        "                  if hasattr(p,'org'):\n",
        "                      p.org.copy_(p.data.clamp_(-1,1))\n",
        "\n",
        "          # Store loss history for future plotting\n",
        "          loss_hist.append(loss_val.item())\n",
        "\n",
        "          # Test set\n",
        "          test_data = itertools.cycle(test_loader)\n",
        "          testdata_it, testtargets_it = next(test_data)\n",
        "          testdata_it = testdata_it.to(device)\n",
        "          testtargets_it = testtargets_it.to(device)\n",
        "\n",
        "          # Test set forward pass\n",
        "          # test_output, test_mem_rec = net(testdata_it.view(batch_size, -1))\n",
        "          test_output, test_mem_rec = net(testdata_it.view(batch_size, 1, 28, 28))\n",
        "\n",
        "          # Test set loss\n",
        "          log_p_ytest = log_softmax_fn(test_mem_rec)\n",
        "          log_p_ytest = log_p_ytest.sum(dim=0)\n",
        "          loss_val_test = loss_fn(log_p_ytest, testtargets_it)\n",
        "          test_loss_hist.append(loss_val_test.item())\n",
        "\n",
        "          # Print test/train loss/accuracy\n",
        "          if counter % 50 == 0:\n",
        "            train_printer()\n",
        "          minibatch_counter += 1\n",
        "          counter += 1\n",
        "\n",
        "  loss_hist_true_grad = loss_hist\n",
        "  test_loss_hist_true_grad = test_loss_hist\n",
        "\n",
        "  # test accuracy\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    net.eval()\n",
        "    for data in test_loader:\n",
        "      images, labels = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # If current batch matches batch_size, just do the usual thing\n",
        "      if images.size()[0] == batch_size:\n",
        "        # outputs, _ = net(images.view(batch_size, -1))\n",
        "        outputs, _ = net(images.view(batch_size, 1, 28, 28))\n",
        "\n",
        "      # If current batch does not match batch_size (e.g., is the final minibatch),\n",
        "      # modify batch_size in a temp variable and restore it at the end of the else block\n",
        "      else:\n",
        "        temp_bs = batch_size\n",
        "        batch_size = images.size()[0]\n",
        "        # outputs, _ = net(images.view(images.size()[0], -1))\n",
        "        outputs, _ = net(images.view(images.size()[0], 1, 28, 28))\n",
        "        batch_size = temp_bs\n",
        "\n",
        "      _, predicted = outputs.sum(dim=0).max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  print(f\"initial_lr: {lr}\")\n",
        "  print(f\"lr_decay: {lr_decay}\")\n",
        "  print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
        "  print(f\"Test Set Accuracy: {100 * correct / total}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial_lr: 0.0002\n",
            "lr_decay: 0.95\n",
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 1479.259521484375\n",
            "Test Set Loss: 1371.9903564453125\n",
            "Train Set Accuracy: 0.109375\n",
            "Test Set Accuracy: 0.0390625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 50\n",
            "Train Set Loss: 331.1580810546875\n",
            "Test Set Loss: 282.7850036621094\n",
            "Train Set Accuracy: 0.2109375\n",
            "Test Set Accuracy: 0.2890625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 100\n",
            "Train Set Loss: 167.80873107910156\n",
            "Test Set Loss: 134.56951904296875\n",
            "Train Set Accuracy: 0.28125\n",
            "Test Set Accuracy: 0.3671875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 150\n",
            "Train Set Loss: 114.34632110595703\n",
            "Test Set Loss: 132.60487365722656\n",
            "Train Set Accuracy: 0.4140625\n",
            "Test Set Accuracy: 0.4375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 200\n",
            "Train Set Loss: 92.9863510131836\n",
            "Test Set Loss: 66.52826690673828\n",
            "Train Set Accuracy: 0.328125\n",
            "Test Set Accuracy: 0.3125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 250\n",
            "Train Set Loss: 72.05693054199219\n",
            "Test Set Loss: 63.03389358520508\n",
            "Train Set Accuracy: 0.4296875\n",
            "Test Set Accuracy: 0.3671875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 300\n",
            "Train Set Loss: 58.906246185302734\n",
            "Test Set Loss: 58.209171295166016\n",
            "Train Set Accuracy: 0.3359375\n",
            "Test Set Accuracy: 0.375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 350\n",
            "Train Set Loss: 44.67284393310547\n",
            "Test Set Loss: 39.43819046020508\n",
            "Train Set Accuracy: 0.53125\n",
            "Test Set Accuracy: 0.59375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 400\n",
            "Train Set Loss: 32.45919418334961\n",
            "Test Set Loss: 33.43693161010742\n",
            "Train Set Accuracy: 0.609375\n",
            "Test Set Accuracy: 0.5703125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 450\n",
            "Train Set Loss: 32.509246826171875\n",
            "Test Set Loss: 29.50379180908203\n",
            "Train Set Accuracy: 0.6328125\n",
            "Test Set Accuracy: 0.6328125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 32\n",
            "Train Set Loss: 29.859405517578125\n",
            "Test Set Loss: 27.723573684692383\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 82\n",
            "Train Set Loss: 25.48333168029785\n",
            "Test Set Loss: 25.361982345581055\n",
            "Train Set Accuracy: 0.7109375\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 132\n",
            "Train Set Loss: 28.18881607055664\n",
            "Test Set Loss: 25.85014533996582\n",
            "Train Set Accuracy: 0.6328125\n",
            "Test Set Accuracy: 0.671875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 182\n",
            "Train Set Loss: 29.07065773010254\n",
            "Test Set Loss: 26.69437026977539\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 232\n",
            "Train Set Loss: 28.198810577392578\n",
            "Test Set Loss: 27.412391662597656\n",
            "Train Set Accuracy: 0.6875\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 282\n",
            "Train Set Loss: 25.731752395629883\n",
            "Test Set Loss: 30.250450134277344\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 332\n",
            "Train Set Loss: 26.35091209411621\n",
            "Test Set Loss: 24.573556900024414\n",
            "Train Set Accuracy: 0.6953125\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 382\n",
            "Train Set Loss: 29.09280014038086\n",
            "Test Set Loss: 25.90953826904297\n",
            "Train Set Accuracy: 0.625\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 432\n",
            "Train Set Loss: 29.166513442993164\n",
            "Test Set Loss: 19.164875030517578\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 14\n",
            "Train Set Loss: 27.524396896362305\n",
            "Test Set Loss: 27.23805046081543\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 64\n",
            "Train Set Loss: 24.003231048583984\n",
            "Test Set Loss: 24.391878128051758\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 114\n",
            "Train Set Loss: 25.94489288330078\n",
            "Test Set Loss: 28.35602378845215\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 164\n",
            "Train Set Loss: 22.64584732055664\n",
            "Test Set Loss: 25.002962112426758\n",
            "Train Set Accuracy: 0.8046875\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 214\n",
            "Train Set Loss: 25.95480728149414\n",
            "Test Set Loss: 21.132678985595703\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 264\n",
            "Train Set Loss: 25.696535110473633\n",
            "Test Set Loss: 26.161388397216797\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 314\n",
            "Train Set Loss: 20.641950607299805\n",
            "Test Set Loss: 20.721521377563477\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.7734375\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 364\n",
            "Train Set Loss: 30.613462448120117\n",
            "Test Set Loss: 23.01853370666504\n",
            "Train Set Accuracy: 0.625\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 414\n",
            "Train Set Loss: 22.21088981628418\n",
            "Test Set Loss: 26.0622501373291\n",
            "Train Set Accuracy: 0.7109375\n",
            "Test Set Accuracy: 0.671875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 464\n",
            "Train Set Loss: 22.878429412841797\n",
            "Test Set Loss: 20.04193687438965\n",
            "Train Set Accuracy: 0.734375\n",
            "Test Set Accuracy: 0.8203125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 46\n",
            "Train Set Loss: 29.215877532958984\n",
            "Test Set Loss: 27.509531021118164\n",
            "Train Set Accuracy: 0.6953125\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 96\n",
            "Train Set Loss: 20.983444213867188\n",
            "Test Set Loss: 24.34977912902832\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 146\n",
            "Train Set Loss: 21.740447998046875\n",
            "Test Set Loss: 23.607440948486328\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 196\n",
            "Train Set Loss: 15.108902931213379\n",
            "Test Set Loss: 21.837825775146484\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 246\n",
            "Train Set Loss: 22.090259552001953\n",
            "Test Set Loss: 21.529237747192383\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 296\n",
            "Train Set Loss: 30.04534149169922\n",
            "Test Set Loss: 23.022354125976562\n",
            "Train Set Accuracy: 0.6875\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 346\n",
            "Train Set Loss: 18.943164825439453\n",
            "Test Set Loss: 17.84108543395996\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 396\n",
            "Train Set Loss: 28.228544235229492\n",
            "Test Set Loss: 22.763267517089844\n",
            "Train Set Accuracy: 0.6875\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 446\n",
            "Train Set Loss: 21.117204666137695\n",
            "Test Set Loss: 21.220970153808594\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 28\n",
            "Train Set Loss: 21.779499053955078\n",
            "Test Set Loss: 21.441858291625977\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 78\n",
            "Train Set Loss: 21.625905990600586\n",
            "Test Set Loss: 25.629512786865234\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 128\n",
            "Train Set Loss: 20.7373104095459\n",
            "Test Set Loss: 25.384933471679688\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 178\n",
            "Train Set Loss: 31.211946487426758\n",
            "Test Set Loss: 25.589130401611328\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.71875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 228\n",
            "Train Set Loss: 23.70075225830078\n",
            "Test Set Loss: 25.909645080566406\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 278\n",
            "Train Set Loss: 19.93317222595215\n",
            "Test Set Loss: 22.995830535888672\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 328\n",
            "Train Set Loss: 20.709495544433594\n",
            "Test Set Loss: 22.54476547241211\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 378\n",
            "Train Set Loss: 20.72660255432129\n",
            "Test Set Loss: 24.132076263427734\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 428\n",
            "Train Set Loss: 22.6646671295166\n",
            "Test Set Loss: 23.855224609375\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 10\n",
            "Train Set Loss: 22.968063354492188\n",
            "Test Set Loss: 21.608867645263672\n",
            "Train Set Accuracy: 0.7421875\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 60\n",
            "Train Set Loss: 20.355615615844727\n",
            "Test Set Loss: 22.792394638061523\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.7734375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 110\n",
            "Train Set Loss: 17.614194869995117\n",
            "Test Set Loss: 19.53825569152832\n",
            "Train Set Accuracy: 0.84375\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 160\n",
            "Train Set Loss: 22.695926666259766\n",
            "Test Set Loss: 19.713973999023438\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 210\n",
            "Train Set Loss: 20.508514404296875\n",
            "Test Set Loss: 20.565147399902344\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 260\n",
            "Train Set Loss: 23.481603622436523\n",
            "Test Set Loss: 26.729915618896484\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 310\n",
            "Train Set Loss: 25.972776412963867\n",
            "Test Set Loss: 24.1164608001709\n",
            "Train Set Accuracy: 0.7109375\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 360\n",
            "Train Set Loss: 17.665138244628906\n",
            "Test Set Loss: 19.025665283203125\n",
            "Train Set Accuracy: 0.828125\n",
            "Test Set Accuracy: 0.8203125\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 410\n",
            "Train Set Loss: 18.382234573364258\n",
            "Test Set Loss: 19.657039642333984\n",
            "Train Set Accuracy: 0.84375\n",
            "Test Set Accuracy: 0.7734375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 460\n",
            "Train Set Loss: 15.031063079833984\n",
            "Test Set Loss: 24.4488582611084\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.765625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 42\n",
            "Train Set Loss: 22.60668182373047\n",
            "Test Set Loss: 18.548776626586914\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.828125\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 92\n",
            "Train Set Loss: 21.972585678100586\n",
            "Test Set Loss: 20.37265968322754\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 142\n",
            "Train Set Loss: 24.281051635742188\n",
            "Test Set Loss: 23.19240951538086\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 192\n",
            "Train Set Loss: 16.93838882446289\n",
            "Test Set Loss: 19.0606746673584\n",
            "Train Set Accuracy: 0.8046875\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 242\n",
            "Train Set Loss: 18.73343849182129\n",
            "Test Set Loss: 21.48207664489746\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 292\n",
            "Train Set Loss: 22.52151870727539\n",
            "Test Set Loss: 25.955154418945312\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 342\n",
            "Train Set Loss: 20.460420608520508\n",
            "Test Set Loss: 20.52412986755371\n",
            "Train Set Accuracy: 0.8359375\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 392\n",
            "Train Set Loss: 17.776256561279297\n",
            "Test Set Loss: 21.72071075439453\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 442\n",
            "Train Set Loss: 22.62177276611328\n",
            "Test Set Loss: 26.27254867553711\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 24\n",
            "Train Set Loss: 25.405181884765625\n",
            "Test Set Loss: 24.195825576782227\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 74\n",
            "Train Set Loss: 19.191673278808594\n",
            "Test Set Loss: 26.089122772216797\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 124\n",
            "Train Set Loss: 23.792476654052734\n",
            "Test Set Loss: 23.88616943359375\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 174\n",
            "Train Set Loss: 28.513813018798828\n",
            "Test Set Loss: 18.457298278808594\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.8203125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 224\n",
            "Train Set Loss: 24.62967300415039\n",
            "Test Set Loss: 18.218116760253906\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.828125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 274\n",
            "Train Set Loss: 19.798377990722656\n",
            "Test Set Loss: 22.460214614868164\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 324\n",
            "Train Set Loss: 22.99371910095215\n",
            "Test Set Loss: 19.75027847290039\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 374\n",
            "Train Set Loss: 27.426918029785156\n",
            "Test Set Loss: 23.531450271606445\n",
            "Train Set Accuracy: 0.734375\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 424\n",
            "Train Set Loss: 17.574193954467773\n",
            "Test Set Loss: 28.155202865600586\n",
            "Train Set Accuracy: 0.8046875\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 6\n",
            "Train Set Loss: 22.971593856811523\n",
            "Test Set Loss: 22.838693618774414\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 56\n",
            "Train Set Loss: 21.121883392333984\n",
            "Test Set Loss: 20.44679069519043\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 106\n",
            "Train Set Loss: 19.810888290405273\n",
            "Test Set Loss: 23.124805450439453\n",
            "Train Set Accuracy: 0.84375\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 156\n",
            "Train Set Loss: 18.992563247680664\n",
            "Test Set Loss: 24.654802322387695\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 206\n",
            "Train Set Loss: 23.27880096435547\n",
            "Test Set Loss: 18.27068328857422\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.8359375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 256\n",
            "Train Set Loss: 17.0489444732666\n",
            "Test Set Loss: 18.016599655151367\n",
            "Train Set Accuracy: 0.84375\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 306\n",
            "Train Set Loss: 26.05741310119629\n",
            "Test Set Loss: 26.42442512512207\n",
            "Train Set Accuracy: 0.6953125\n",
            "Test Set Accuracy: 0.71875\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 356\n",
            "Train Set Loss: 23.72020721435547\n",
            "Test Set Loss: 18.227251052856445\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 406\n",
            "Train Set Loss: 24.16156768798828\n",
            "Test Set Loss: 19.4792537689209\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.8125\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 456\n",
            "Train Set Loss: 23.065462112426758\n",
            "Test Set Loss: 22.298498153686523\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.71875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 38\n",
            "Train Set Loss: 21.953689575195312\n",
            "Test Set Loss: 20.08331298828125\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 88\n",
            "Train Set Loss: 21.239784240722656\n",
            "Test Set Loss: 19.200315475463867\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.8125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 138\n",
            "Train Set Loss: 13.185300827026367\n",
            "Test Set Loss: 17.479244232177734\n",
            "Train Set Accuracy: 0.8515625\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 188\n",
            "Train Set Loss: 21.49961280822754\n",
            "Test Set Loss: 23.793813705444336\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.71875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 238\n",
            "Train Set Loss: 18.56354522705078\n",
            "Test Set Loss: 22.760658264160156\n",
            "Train Set Accuracy: 0.8515625\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 288\n",
            "Train Set Loss: 24.53009796142578\n",
            "Test Set Loss: 19.451244354248047\n",
            "Train Set Accuracy: 0.8046875\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 338\n",
            "Train Set Loss: 21.02535057067871\n",
            "Test Set Loss: 18.66261863708496\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.8125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 388\n",
            "Train Set Loss: 16.47138023376465\n",
            "Test Set Loss: 16.561479568481445\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.8125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 438\n",
            "Train Set Loss: 16.637466430664062\n",
            "Test Set Loss: 16.814891815185547\n",
            "Train Set Accuracy: 0.8359375\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "initial_lr: 0.0002\n",
            "lr_decay: 0.95\n",
            "Total correctly classified test set images: 8039/10000\n",
            "Test Set Accuracy: 80.39%\n",
            "initial_lr: 0.0002\n",
            "lr_decay: 0.9\n",
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 2153.205322265625\n",
            "Test Set Loss: 1795.5889892578125\n",
            "Train Set Accuracy: 0.09375\n",
            "Test Set Accuracy: 0.125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 50\n",
            "Train Set Loss: 375.8057556152344\n",
            "Test Set Loss: 355.1363830566406\n",
            "Train Set Accuracy: 0.1796875\n",
            "Test Set Accuracy: 0.234375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 100\n",
            "Train Set Loss: 192.38002014160156\n",
            "Test Set Loss: 168.2856903076172\n",
            "Train Set Accuracy: 0.203125\n",
            "Test Set Accuracy: 0.265625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 150\n",
            "Train Set Loss: 103.57421875\n",
            "Test Set Loss: 121.42859649658203\n",
            "Train Set Accuracy: 0.1796875\n",
            "Test Set Accuracy: 0.171875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 200\n",
            "Train Set Loss: 101.11804962158203\n",
            "Test Set Loss: 75.89302062988281\n",
            "Train Set Accuracy: 0.3046875\n",
            "Test Set Accuracy: 0.25\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 250\n",
            "Train Set Loss: 50.846961975097656\n",
            "Test Set Loss: 61.4830207824707\n",
            "Train Set Accuracy: 0.421875\n",
            "Test Set Accuracy: 0.453125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 300\n",
            "Train Set Loss: 42.54946517944336\n",
            "Test Set Loss: 37.2540397644043\n",
            "Train Set Accuracy: 0.5078125\n",
            "Test Set Accuracy: 0.453125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 350\n",
            "Train Set Loss: 33.72023010253906\n",
            "Test Set Loss: 40.2418212890625\n",
            "Train Set Accuracy: 0.609375\n",
            "Test Set Accuracy: 0.453125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 400\n",
            "Train Set Loss: 34.12555694580078\n",
            "Test Set Loss: 34.408424377441406\n",
            "Train Set Accuracy: 0.5859375\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 450\n",
            "Train Set Loss: 34.209999084472656\n",
            "Test Set Loss: 36.93949508666992\n",
            "Train Set Accuracy: 0.5859375\n",
            "Test Set Accuracy: 0.53125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 32\n",
            "Train Set Loss: 30.777305603027344\n",
            "Test Set Loss: 35.91211700439453\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 82\n",
            "Train Set Loss: 31.53506088256836\n",
            "Test Set Loss: 34.5118522644043\n",
            "Train Set Accuracy: 0.6953125\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 132\n",
            "Train Set Loss: 28.80360984802246\n",
            "Test Set Loss: 33.777557373046875\n",
            "Train Set Accuracy: 0.6953125\n",
            "Test Set Accuracy: 0.5859375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 182\n",
            "Train Set Loss: 29.58188247680664\n",
            "Test Set Loss: 29.98280906677246\n",
            "Train Set Accuracy: 0.7109375\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 232\n",
            "Train Set Loss: 33.038082122802734\n",
            "Test Set Loss: 28.88482093811035\n",
            "Train Set Accuracy: 0.6171875\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 282\n",
            "Train Set Loss: 24.211267471313477\n",
            "Test Set Loss: 29.536277770996094\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 332\n",
            "Train Set Loss: 31.76058578491211\n",
            "Test Set Loss: 27.58860969543457\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 382\n",
            "Train Set Loss: 22.40989112854004\n",
            "Test Set Loss: 23.045242309570312\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 432\n",
            "Train Set Loss: 33.12856674194336\n",
            "Test Set Loss: 25.855573654174805\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 14\n",
            "Train Set Loss: 27.78276824951172\n",
            "Test Set Loss: 28.29258155822754\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 64\n",
            "Train Set Loss: 21.306764602661133\n",
            "Test Set Loss: 24.43621826171875\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.8125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 114\n",
            "Train Set Loss: 25.532672882080078\n",
            "Test Set Loss: 26.53519058227539\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.765625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 164\n",
            "Train Set Loss: 22.140209197998047\n",
            "Test Set Loss: 21.497844696044922\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 214\n",
            "Train Set Loss: 22.809328079223633\n",
            "Test Set Loss: 23.719078063964844\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 264\n",
            "Train Set Loss: 28.499570846557617\n",
            "Test Set Loss: 24.072742462158203\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.7578125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 314\n",
            "Train Set Loss: 30.104434967041016\n",
            "Test Set Loss: 27.768796920776367\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 364\n",
            "Train Set Loss: 21.04681968688965\n",
            "Test Set Loss: 21.907154083251953\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 414\n",
            "Train Set Loss: 23.106111526489258\n",
            "Test Set Loss: 25.212753295898438\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 464\n",
            "Train Set Loss: 28.07257652282715\n",
            "Test Set Loss: 24.3067626953125\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 46\n",
            "Train Set Loss: 24.59415626525879\n",
            "Test Set Loss: 23.96088218688965\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 96\n",
            "Train Set Loss: 22.214929580688477\n",
            "Test Set Loss: 25.134855270385742\n",
            "Train Set Accuracy: 0.828125\n",
            "Test Set Accuracy: 0.7734375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 146\n",
            "Train Set Loss: 21.618558883666992\n",
            "Test Set Loss: 23.187301635742188\n",
            "Train Set Accuracy: 0.828125\n",
            "Test Set Accuracy: 0.8203125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 196\n",
            "Train Set Loss: 21.538312911987305\n",
            "Test Set Loss: 23.439708709716797\n",
            "Train Set Accuracy: 0.8046875\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 246\n",
            "Train Set Loss: 16.61240577697754\n",
            "Test Set Loss: 23.019514083862305\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 296\n",
            "Train Set Loss: 24.748302459716797\n",
            "Test Set Loss: 19.33688735961914\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 346\n",
            "Train Set Loss: 21.013668060302734\n",
            "Test Set Loss: 24.155113220214844\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 396\n",
            "Train Set Loss: 18.605205535888672\n",
            "Test Set Loss: 20.35108757019043\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.8125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 446\n",
            "Train Set Loss: 23.7490291595459\n",
            "Test Set Loss: 17.78114128112793\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.8359375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 28\n",
            "Train Set Loss: 22.905071258544922\n",
            "Test Set Loss: 25.363449096679688\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 78\n",
            "Train Set Loss: 23.8585205078125\n",
            "Test Set Loss: 20.394908905029297\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 128\n",
            "Train Set Loss: 23.769412994384766\n",
            "Test Set Loss: 22.614919662475586\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.765625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 178\n",
            "Train Set Loss: 22.386049270629883\n",
            "Test Set Loss: 19.10700798034668\n",
            "Train Set Accuracy: 0.8046875\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 228\n",
            "Train Set Loss: 23.100786209106445\n",
            "Test Set Loss: 18.507583618164062\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.8515625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 278\n",
            "Train Set Loss: 23.719345092773438\n",
            "Test Set Loss: 17.677968978881836\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.8203125\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 328\n",
            "Train Set Loss: 25.44247817993164\n",
            "Test Set Loss: 25.60811996459961\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.765625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 378\n",
            "Train Set Loss: 22.744218826293945\n",
            "Test Set Loss: 16.068254470825195\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.84375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 428\n",
            "Train Set Loss: 21.936813354492188\n",
            "Test Set Loss: 20.27373695373535\n",
            "Train Set Accuracy: 0.8046875\n",
            "Test Set Accuracy: 0.84375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 10\n",
            "Train Set Loss: 17.519481658935547\n",
            "Test Set Loss: 19.484159469604492\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.8359375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 60\n",
            "Train Set Loss: 23.539194107055664\n",
            "Test Set Loss: 18.279855728149414\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.8671875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 110\n",
            "Train Set Loss: 23.233327865600586\n",
            "Test Set Loss: 23.226476669311523\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 160\n",
            "Train Set Loss: 23.734663009643555\n",
            "Test Set Loss: 17.75278663635254\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 210\n",
            "Train Set Loss: 20.3616943359375\n",
            "Test Set Loss: 17.953969955444336\n",
            "Train Set Accuracy: 0.734375\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 260\n",
            "Train Set Loss: 20.730052947998047\n",
            "Test Set Loss: 19.54694366455078\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 310\n",
            "Train Set Loss: 17.540081024169922\n",
            "Test Set Loss: 16.0107421875\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.8125\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 360\n",
            "Train Set Loss: 15.120716094970703\n",
            "Test Set Loss: 15.821184158325195\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.8671875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 410\n",
            "Train Set Loss: 22.240009307861328\n",
            "Test Set Loss: 20.109817504882812\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 460\n",
            "Train Set Loss: 24.556093215942383\n",
            "Test Set Loss: 21.270668029785156\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.7578125\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 42\n",
            "Train Set Loss: 24.69900894165039\n",
            "Test Set Loss: 21.927017211914062\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 92\n",
            "Train Set Loss: 18.144296646118164\n",
            "Test Set Loss: 20.66429901123047\n",
            "Train Set Accuracy: 0.828125\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 142\n",
            "Train Set Loss: 18.13876724243164\n",
            "Test Set Loss: 26.723819732666016\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 192\n",
            "Train Set Loss: 20.357362747192383\n",
            "Test Set Loss: 25.275279998779297\n",
            "Train Set Accuracy: 0.84375\n",
            "Test Set Accuracy: 0.71875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 242\n",
            "Train Set Loss: 24.217464447021484\n",
            "Test Set Loss: 23.46409034729004\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 292\n",
            "Train Set Loss: 20.99515724182129\n",
            "Test Set Loss: 23.762800216674805\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 342\n",
            "Train Set Loss: 16.481958389282227\n",
            "Test Set Loss: 15.80150318145752\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.859375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 392\n",
            "Train Set Loss: 16.646156311035156\n",
            "Test Set Loss: 16.117816925048828\n",
            "Train Set Accuracy: 0.828125\n",
            "Test Set Accuracy: 0.8671875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 442\n",
            "Train Set Loss: 25.890151977539062\n",
            "Test Set Loss: 22.80459976196289\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.765625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 24\n",
            "Train Set Loss: 23.2122745513916\n",
            "Test Set Loss: 21.873559951782227\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 74\n",
            "Train Set Loss: 23.048095703125\n",
            "Test Set Loss: 16.892744064331055\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.8515625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 124\n",
            "Train Set Loss: 23.390453338623047\n",
            "Test Set Loss: 23.799734115600586\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.8203125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 174\n",
            "Train Set Loss: 22.050012588500977\n",
            "Test Set Loss: 20.531620025634766\n",
            "Train Set Accuracy: 0.8515625\n",
            "Test Set Accuracy: 0.8203125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 224\n",
            "Train Set Loss: 22.483781814575195\n",
            "Test Set Loss: 25.46278953552246\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 274\n",
            "Train Set Loss: 21.563507080078125\n",
            "Test Set Loss: 19.291603088378906\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 324\n",
            "Train Set Loss: 22.6014404296875\n",
            "Test Set Loss: 20.248394012451172\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 374\n",
            "Train Set Loss: 24.42732048034668\n",
            "Test Set Loss: 16.67100715637207\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 424\n",
            "Train Set Loss: 21.1624698638916\n",
            "Test Set Loss: 21.677143096923828\n",
            "Train Set Accuracy: 0.8359375\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 6\n",
            "Train Set Loss: 21.291461944580078\n",
            "Test Set Loss: 21.087778091430664\n",
            "Train Set Accuracy: 0.828125\n",
            "Test Set Accuracy: 0.828125\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 56\n",
            "Train Set Loss: 26.008031845092773\n",
            "Test Set Loss: 20.950361251831055\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.7734375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 106\n",
            "Train Set Loss: 16.493427276611328\n",
            "Test Set Loss: 19.989641189575195\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 156\n",
            "Train Set Loss: 21.87166404724121\n",
            "Test Set Loss: 21.67084503173828\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 206\n",
            "Train Set Loss: 16.81916618347168\n",
            "Test Set Loss: 21.54435920715332\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.765625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 256\n",
            "Train Set Loss: 21.20534896850586\n",
            "Test Set Loss: 18.667659759521484\n",
            "Train Set Accuracy: 0.8359375\n",
            "Test Set Accuracy: 0.7890625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 306\n",
            "Train Set Loss: 21.926593780517578\n",
            "Test Set Loss: 20.151227951049805\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 356\n",
            "Train Set Loss: 28.740196228027344\n",
            "Test Set Loss: 23.019197463989258\n",
            "Train Set Accuracy: 0.7421875\n",
            "Test Set Accuracy: 0.71875\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 406\n",
            "Train Set Loss: 24.896102905273438\n",
            "Test Set Loss: 15.408063888549805\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.8359375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 456\n",
            "Train Set Loss: 21.513885498046875\n",
            "Test Set Loss: 23.009334564208984\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 38\n",
            "Train Set Loss: 24.87133026123047\n",
            "Test Set Loss: 19.047374725341797\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.8359375\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 88\n",
            "Train Set Loss: 24.980545043945312\n",
            "Test Set Loss: 23.42501449584961\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 138\n",
            "Train Set Loss: 23.144512176513672\n",
            "Test Set Loss: 25.200027465820312\n",
            "Train Set Accuracy: 0.703125\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 188\n",
            "Train Set Loss: 20.325244903564453\n",
            "Test Set Loss: 20.68984603881836\n",
            "Train Set Accuracy: 0.7890625\n",
            "Test Set Accuracy: 0.8125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 238\n",
            "Train Set Loss: 22.15302848815918\n",
            "Test Set Loss: 19.19708251953125\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.8359375\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 288\n",
            "Train Set Loss: 15.959973335266113\n",
            "Test Set Loss: 17.720617294311523\n",
            "Train Set Accuracy: 0.8671875\n",
            "Test Set Accuracy: 0.828125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 338\n",
            "Train Set Loss: 17.20716094970703\n",
            "Test Set Loss: 20.23701286315918\n",
            "Train Set Accuracy: 0.8515625\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 388\n",
            "Train Set Loss: 20.362735748291016\n",
            "Test Set Loss: 28.825042724609375\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 438\n",
            "Train Set Loss: 19.14742660522461\n",
            "Test Set Loss: 22.37627601623535\n",
            "Train Set Accuracy: 0.84375\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "initial_lr: 0.0002\n",
            "lr_decay: 0.9\n",
            "Total correctly classified test set images: 7954/10000\n",
            "Test Set Accuracy: 79.54%\n",
            "initial_lr: 0.0002\n",
            "lr_decay: 0.85\n",
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 1449.545166015625\n",
            "Test Set Loss: 1406.990234375\n",
            "Train Set Accuracy: 0.109375\n",
            "Test Set Accuracy: 0.1171875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 50\n",
            "Train Set Loss: 202.52439880371094\n",
            "Test Set Loss: 240.2404327392578\n",
            "Train Set Accuracy: 0.1875\n",
            "Test Set Accuracy: 0.1953125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 100\n",
            "Train Set Loss: 146.81430053710938\n",
            "Test Set Loss: 143.95947265625\n",
            "Train Set Accuracy: 0.296875\n",
            "Test Set Accuracy: 0.28125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 150\n",
            "Train Set Loss: 71.61800384521484\n",
            "Test Set Loss: 61.25712203979492\n",
            "Train Set Accuracy: 0.34375\n",
            "Test Set Accuracy: 0.40625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 200\n",
            "Train Set Loss: 53.66368103027344\n",
            "Test Set Loss: 60.766868591308594\n",
            "Train Set Accuracy: 0.328125\n",
            "Test Set Accuracy: 0.359375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 250\n",
            "Train Set Loss: 43.95139694213867\n",
            "Test Set Loss: 44.84193420410156\n",
            "Train Set Accuracy: 0.3828125\n",
            "Test Set Accuracy: 0.40625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 300\n",
            "Train Set Loss: 37.446163177490234\n",
            "Test Set Loss: 42.449440002441406\n",
            "Train Set Accuracy: 0.5625\n",
            "Test Set Accuracy: 0.4609375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 350\n",
            "Train Set Loss: 44.08795928955078\n",
            "Test Set Loss: 38.309425354003906\n",
            "Train Set Accuracy: 0.484375\n",
            "Test Set Accuracy: 0.515625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 400\n",
            "Train Set Loss: 43.956851959228516\n",
            "Test Set Loss: 41.567142486572266\n",
            "Train Set Accuracy: 0.5\n",
            "Test Set Accuracy: 0.484375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 450\n",
            "Train Set Loss: 41.81908416748047\n",
            "Test Set Loss: 39.35924530029297\n",
            "Train Set Accuracy: 0.4609375\n",
            "Test Set Accuracy: 0.546875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 32\n",
            "Train Set Loss: 32.85634231567383\n",
            "Test Set Loss: 35.15920639038086\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.546875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 82\n",
            "Train Set Loss: 38.315269470214844\n",
            "Test Set Loss: 38.085994720458984\n",
            "Train Set Accuracy: 0.53125\n",
            "Test Set Accuracy: 0.5390625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 132\n",
            "Train Set Loss: 29.586069107055664\n",
            "Test Set Loss: 37.33665084838867\n",
            "Train Set Accuracy: 0.59375\n",
            "Test Set Accuracy: 0.546875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 182\n",
            "Train Set Loss: 31.517473220825195\n",
            "Test Set Loss: 34.73748779296875\n",
            "Train Set Accuracy: 0.578125\n",
            "Test Set Accuracy: 0.53125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 232\n",
            "Train Set Loss: 35.32115173339844\n",
            "Test Set Loss: 32.29148864746094\n",
            "Train Set Accuracy: 0.5703125\n",
            "Test Set Accuracy: 0.578125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 282\n",
            "Train Set Loss: 28.380203247070312\n",
            "Test Set Loss: 32.56193923950195\n",
            "Train Set Accuracy: 0.7109375\n",
            "Test Set Accuracy: 0.6171875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 332\n",
            "Train Set Loss: 32.363929748535156\n",
            "Test Set Loss: 30.23168182373047\n",
            "Train Set Accuracy: 0.6171875\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 382\n",
            "Train Set Loss: 31.617624282836914\n",
            "Test Set Loss: 36.45174789428711\n",
            "Train Set Accuracy: 0.6875\n",
            "Test Set Accuracy: 0.5625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 432\n",
            "Train Set Loss: 27.915878295898438\n",
            "Test Set Loss: 30.720903396606445\n",
            "Train Set Accuracy: 0.6953125\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 14\n",
            "Train Set Loss: 29.42945098876953\n",
            "Test Set Loss: 33.372901916503906\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.5703125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 64\n",
            "Train Set Loss: 33.60689926147461\n",
            "Test Set Loss: 25.661710739135742\n",
            "Train Set Accuracy: 0.59375\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 114\n",
            "Train Set Loss: 30.241044998168945\n",
            "Test Set Loss: 34.461456298828125\n",
            "Train Set Accuracy: 0.6484375\n",
            "Test Set Accuracy: 0.546875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 164\n",
            "Train Set Loss: 33.300167083740234\n",
            "Test Set Loss: 27.29385757446289\n",
            "Train Set Accuracy: 0.6875\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 214\n",
            "Train Set Loss: 31.48699378967285\n",
            "Test Set Loss: 26.9167423248291\n",
            "Train Set Accuracy: 0.6953125\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 264\n",
            "Train Set Loss: 31.90566062927246\n",
            "Test Set Loss: 31.824434280395508\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 314\n",
            "Train Set Loss: 36.50140380859375\n",
            "Test Set Loss: 32.08110046386719\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 364\n",
            "Train Set Loss: 33.23676681518555\n",
            "Test Set Loss: 35.0889778137207\n",
            "Train Set Accuracy: 0.6171875\n",
            "Test Set Accuracy: 0.609375\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 414\n",
            "Train Set Loss: 30.303417205810547\n",
            "Test Set Loss: 31.57552719116211\n",
            "Train Set Accuracy: 0.59375\n",
            "Test Set Accuracy: 0.640625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 464\n",
            "Train Set Loss: 28.301971435546875\n",
            "Test Set Loss: 29.807170867919922\n",
            "Train Set Accuracy: 0.6484375\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 46\n",
            "Train Set Loss: 28.94222068786621\n",
            "Test Set Loss: 25.703622817993164\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 96\n",
            "Train Set Loss: 25.8106746673584\n",
            "Test Set Loss: 29.004348754882812\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 146\n",
            "Train Set Loss: 29.331436157226562\n",
            "Test Set Loss: 31.252103805541992\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.671875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 196\n",
            "Train Set Loss: 27.670969009399414\n",
            "Test Set Loss: 28.704540252685547\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 246\n",
            "Train Set Loss: 21.18708038330078\n",
            "Test Set Loss: 23.710445404052734\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 296\n",
            "Train Set Loss: 25.937992095947266\n",
            "Test Set Loss: 35.7138786315918\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.65625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 346\n",
            "Train Set Loss: 29.151660919189453\n",
            "Test Set Loss: 30.250463485717773\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 396\n",
            "Train Set Loss: 31.992597579956055\n",
            "Test Set Loss: 26.3725643157959\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.640625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 446\n",
            "Train Set Loss: 33.58494567871094\n",
            "Test Set Loss: 25.20711898803711\n",
            "Train Set Accuracy: 0.6015625\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 28\n",
            "Train Set Loss: 30.91189193725586\n",
            "Test Set Loss: 31.235464096069336\n",
            "Train Set Accuracy: 0.6328125\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 78\n",
            "Train Set Loss: 30.34944725036621\n",
            "Test Set Loss: 32.006900787353516\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 128\n",
            "Train Set Loss: 31.60897445678711\n",
            "Test Set Loss: 33.941104888916016\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 178\n",
            "Train Set Loss: 28.955120086669922\n",
            "Test Set Loss: 29.225385665893555\n",
            "Train Set Accuracy: 0.703125\n",
            "Test Set Accuracy: 0.6328125\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 228\n",
            "Train Set Loss: 31.194215774536133\n",
            "Test Set Loss: 25.624483108520508\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 278\n",
            "Train Set Loss: 26.967897415161133\n",
            "Test Set Loss: 22.988155364990234\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 328\n",
            "Train Set Loss: 24.488452911376953\n",
            "Test Set Loss: 26.540916442871094\n",
            "Train Set Accuracy: 0.7109375\n",
            "Test Set Accuracy: 0.671875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 378\n",
            "Train Set Loss: 26.82335662841797\n",
            "Test Set Loss: 30.586017608642578\n",
            "Train Set Accuracy: 0.703125\n",
            "Test Set Accuracy: 0.640625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 428\n",
            "Train Set Loss: 29.936655044555664\n",
            "Test Set Loss: 30.440540313720703\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.671875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 10\n",
            "Train Set Loss: 26.931711196899414\n",
            "Test Set Loss: 27.815479278564453\n",
            "Train Set Accuracy: 0.734375\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 60\n",
            "Train Set Loss: 26.71330451965332\n",
            "Test Set Loss: 25.44158363342285\n",
            "Train Set Accuracy: 0.6953125\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 110\n",
            "Train Set Loss: 31.26283836364746\n",
            "Test Set Loss: 30.318939208984375\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 160\n",
            "Train Set Loss: 28.978069305419922\n",
            "Test Set Loss: 27.28115463256836\n",
            "Train Set Accuracy: 0.6875\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 210\n",
            "Train Set Loss: 31.481243133544922\n",
            "Test Set Loss: 33.585693359375\n",
            "Train Set Accuracy: 0.65625\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 260\n",
            "Train Set Loss: 23.623321533203125\n",
            "Test Set Loss: 27.268817901611328\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 310\n",
            "Train Set Loss: 30.15009307861328\n",
            "Test Set Loss: 27.900794982910156\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 360\n",
            "Train Set Loss: 29.973310470581055\n",
            "Test Set Loss: 30.374588012695312\n",
            "Train Set Accuracy: 0.6953125\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 410\n",
            "Train Set Loss: 35.14361572265625\n",
            "Test Set Loss: 27.163095474243164\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 460\n",
            "Train Set Loss: 28.032630920410156\n",
            "Test Set Loss: 24.764957427978516\n",
            "Train Set Accuracy: 0.6015625\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 42\n",
            "Train Set Loss: 23.420480728149414\n",
            "Test Set Loss: 28.376848220825195\n",
            "Train Set Accuracy: 0.734375\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 92\n",
            "Train Set Loss: 28.113283157348633\n",
            "Test Set Loss: 28.224416732788086\n",
            "Train Set Accuracy: 0.65625\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 142\n",
            "Train Set Loss: 23.43678092956543\n",
            "Test Set Loss: 27.04363441467285\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 192\n",
            "Train Set Loss: 21.968976974487305\n",
            "Test Set Loss: 27.3189640045166\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 242\n",
            "Train Set Loss: 28.384580612182617\n",
            "Test Set Loss: 25.695798873901367\n",
            "Train Set Accuracy: 0.7109375\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 292\n",
            "Train Set Loss: 32.560699462890625\n",
            "Test Set Loss: 27.13309097290039\n",
            "Train Set Accuracy: 0.6484375\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 342\n",
            "Train Set Loss: 28.126121520996094\n",
            "Test Set Loss: 23.02215003967285\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 392\n",
            "Train Set Loss: 28.580041885375977\n",
            "Test Set Loss: 27.54514503479004\n",
            "Train Set Accuracy: 0.65625\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 442\n",
            "Train Set Loss: 23.406497955322266\n",
            "Test Set Loss: 26.234329223632812\n",
            "Train Set Accuracy: 0.7421875\n",
            "Test Set Accuracy: 0.671875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 24\n",
            "Train Set Loss: 28.82086944580078\n",
            "Test Set Loss: 34.61460494995117\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 74\n",
            "Train Set Loss: 30.667476654052734\n",
            "Test Set Loss: 29.677940368652344\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.671875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 124\n",
            "Train Set Loss: 23.57293701171875\n",
            "Test Set Loss: 30.600826263427734\n",
            "Train Set Accuracy: 0.8046875\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 174\n",
            "Train Set Loss: 29.16141128540039\n",
            "Test Set Loss: 29.353832244873047\n",
            "Train Set Accuracy: 0.6875\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 224\n",
            "Train Set Loss: 28.368669509887695\n",
            "Test Set Loss: 28.966571807861328\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 274\n",
            "Train Set Loss: 31.70989990234375\n",
            "Test Set Loss: 24.610166549682617\n",
            "Train Set Accuracy: 0.6875\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 324\n",
            "Train Set Loss: 26.504016876220703\n",
            "Test Set Loss: 27.190757751464844\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 374\n",
            "Train Set Loss: 24.13736915588379\n",
            "Test Set Loss: 19.855072021484375\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.8203125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 424\n",
            "Train Set Loss: 27.273696899414062\n",
            "Test Set Loss: 21.052099227905273\n",
            "Train Set Accuracy: 0.734375\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 6\n",
            "Train Set Loss: 27.39028549194336\n",
            "Test Set Loss: 31.880191802978516\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 56\n",
            "Train Set Loss: 25.935100555419922\n",
            "Test Set Loss: 22.006635665893555\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 106\n",
            "Train Set Loss: 22.81887435913086\n",
            "Test Set Loss: 21.894874572753906\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 156\n",
            "Train Set Loss: 27.692636489868164\n",
            "Test Set Loss: 32.013389587402344\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 206\n",
            "Train Set Loss: 28.564374923706055\n",
            "Test Set Loss: 23.31086540222168\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 256\n",
            "Train Set Loss: 35.614891052246094\n",
            "Test Set Loss: 29.599651336669922\n",
            "Train Set Accuracy: 0.65625\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 306\n",
            "Train Set Loss: 30.002412796020508\n",
            "Test Set Loss: 25.828571319580078\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 356\n",
            "Train Set Loss: 23.466503143310547\n",
            "Test Set Loss: 29.51717185974121\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 406\n",
            "Train Set Loss: 20.876182556152344\n",
            "Test Set Loss: 21.817947387695312\n",
            "Train Set Accuracy: 0.8046875\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 456\n",
            "Train Set Loss: 21.911867141723633\n",
            "Test Set Loss: 23.696365356445312\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.765625\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 38\n",
            "Train Set Loss: 26.20811653137207\n",
            "Test Set Loss: 24.73836326599121\n",
            "Train Set Accuracy: 0.7421875\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 88\n",
            "Train Set Loss: 20.51490592956543\n",
            "Test Set Loss: 23.744796752929688\n",
            "Train Set Accuracy: 0.765625\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 138\n",
            "Train Set Loss: 26.21369171142578\n",
            "Test Set Loss: 25.68564796447754\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 188\n",
            "Train Set Loss: 26.778701782226562\n",
            "Test Set Loss: 24.933759689331055\n",
            "Train Set Accuracy: 0.7421875\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 238\n",
            "Train Set Loss: 22.569194793701172\n",
            "Test Set Loss: 26.76007652282715\n",
            "Train Set Accuracy: 0.796875\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 288\n",
            "Train Set Loss: 27.007427215576172\n",
            "Test Set Loss: 25.54483413696289\n",
            "Train Set Accuracy: 0.78125\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 338\n",
            "Train Set Loss: 24.862674713134766\n",
            "Test Set Loss: 23.244369506835938\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.7578125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 388\n",
            "Train Set Loss: 26.667034149169922\n",
            "Test Set Loss: 25.063034057617188\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 438\n",
            "Train Set Loss: 23.73923683166504\n",
            "Test Set Loss: 22.195728302001953\n",
            "Train Set Accuracy: 0.7734375\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "initial_lr: 0.0002\n",
            "lr_decay: 0.85\n",
            "Total correctly classified test set images: 7315/10000\n",
            "Test Set Accuracy: 73.15%\n",
            "initial_lr: 0.0002\n",
            "lr_decay: 0.8\n",
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 1185.3143310546875\n",
            "Test Set Loss: 1123.7525634765625\n",
            "Train Set Accuracy: 0.1171875\n",
            "Test Set Accuracy: 0.0859375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 50\n",
            "Train Set Loss: 230.07008361816406\n",
            "Test Set Loss: 241.0816650390625\n",
            "Train Set Accuracy: 0.359375\n",
            "Test Set Accuracy: 0.25\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 100\n",
            "Train Set Loss: 151.72207641601562\n",
            "Test Set Loss: 163.63888549804688\n",
            "Train Set Accuracy: 0.265625\n",
            "Test Set Accuracy: 0.328125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 150\n",
            "Train Set Loss: 115.11199951171875\n",
            "Test Set Loss: 93.89549255371094\n",
            "Train Set Accuracy: 0.3515625\n",
            "Test Set Accuracy: 0.375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 200\n",
            "Train Set Loss: 101.56715393066406\n",
            "Test Set Loss: 101.63639068603516\n",
            "Train Set Accuracy: 0.4375\n",
            "Test Set Accuracy: 0.375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 250\n",
            "Train Set Loss: 86.36836242675781\n",
            "Test Set Loss: 65.97411346435547\n",
            "Train Set Accuracy: 0.4296875\n",
            "Test Set Accuracy: 0.3671875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 300\n",
            "Train Set Loss: 44.30387878417969\n",
            "Test Set Loss: 45.44818878173828\n",
            "Train Set Accuracy: 0.53125\n",
            "Test Set Accuracy: 0.4296875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 350\n",
            "Train Set Loss: 44.61310577392578\n",
            "Test Set Loss: 46.89988708496094\n",
            "Train Set Accuracy: 0.4296875\n",
            "Test Set Accuracy: 0.484375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 400\n",
            "Train Set Loss: 41.217716217041016\n",
            "Test Set Loss: 44.61198806762695\n",
            "Train Set Accuracy: 0.4765625\n",
            "Test Set Accuracy: 0.4453125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 450\n",
            "Train Set Loss: 41.54832077026367\n",
            "Test Set Loss: 41.44587707519531\n",
            "Train Set Accuracy: 0.4375\n",
            "Test Set Accuracy: 0.5078125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 32\n",
            "Train Set Loss: 44.3424072265625\n",
            "Test Set Loss: 46.219844818115234\n",
            "Train Set Accuracy: 0.4296875\n",
            "Test Set Accuracy: 0.40625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 82\n",
            "Train Set Loss: 38.58550262451172\n",
            "Test Set Loss: 37.80521774291992\n",
            "Train Set Accuracy: 0.5390625\n",
            "Test Set Accuracy: 0.5390625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 132\n",
            "Train Set Loss: 42.76358413696289\n",
            "Test Set Loss: 38.16932678222656\n",
            "Train Set Accuracy: 0.5\n",
            "Test Set Accuracy: 0.484375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 182\n",
            "Train Set Loss: 38.60337448120117\n",
            "Test Set Loss: 41.46710968017578\n",
            "Train Set Accuracy: 0.5546875\n",
            "Test Set Accuracy: 0.453125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 232\n",
            "Train Set Loss: 42.83990478515625\n",
            "Test Set Loss: 37.116920471191406\n",
            "Train Set Accuracy: 0.546875\n",
            "Test Set Accuracy: 0.515625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 282\n",
            "Train Set Loss: 40.50057601928711\n",
            "Test Set Loss: 42.317352294921875\n",
            "Train Set Accuracy: 0.53125\n",
            "Test Set Accuracy: 0.46875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 332\n",
            "Train Set Loss: 36.41645431518555\n",
            "Test Set Loss: 43.859214782714844\n",
            "Train Set Accuracy: 0.5625\n",
            "Test Set Accuracy: 0.4921875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 382\n",
            "Train Set Loss: 42.22964859008789\n",
            "Test Set Loss: 36.615875244140625\n",
            "Train Set Accuracy: 0.5234375\n",
            "Test Set Accuracy: 0.5859375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 432\n",
            "Train Set Loss: 42.01072692871094\n",
            "Test Set Loss: 33.88025665283203\n",
            "Train Set Accuracy: 0.5078125\n",
            "Test Set Accuracy: 0.59375\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 14\n",
            "Train Set Loss: 40.43636703491211\n",
            "Test Set Loss: 38.19488525390625\n",
            "Train Set Accuracy: 0.578125\n",
            "Test Set Accuracy: 0.546875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 64\n",
            "Train Set Loss: 40.75707244873047\n",
            "Test Set Loss: 39.70868682861328\n",
            "Train Set Accuracy: 0.609375\n",
            "Test Set Accuracy: 0.5078125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 114\n",
            "Train Set Loss: 37.29887008666992\n",
            "Test Set Loss: 38.580535888671875\n",
            "Train Set Accuracy: 0.5703125\n",
            "Test Set Accuracy: 0.5703125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 164\n",
            "Train Set Loss: 35.6473503112793\n",
            "Test Set Loss: 35.37864685058594\n",
            "Train Set Accuracy: 0.46875\n",
            "Test Set Accuracy: 0.5703125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 214\n",
            "Train Set Loss: 42.56732177734375\n",
            "Test Set Loss: 40.83808898925781\n",
            "Train Set Accuracy: 0.5390625\n",
            "Test Set Accuracy: 0.5625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 264\n",
            "Train Set Loss: 41.771820068359375\n",
            "Test Set Loss: 42.613712310791016\n",
            "Train Set Accuracy: 0.5546875\n",
            "Test Set Accuracy: 0.5078125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 314\n",
            "Train Set Loss: 34.86286544799805\n",
            "Test Set Loss: 35.78890609741211\n",
            "Train Set Accuracy: 0.5859375\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 364\n",
            "Train Set Loss: 35.78467559814453\n",
            "Test Set Loss: 34.541221618652344\n",
            "Train Set Accuracy: 0.5625\n",
            "Test Set Accuracy: 0.6015625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 414\n",
            "Train Set Loss: 39.490779876708984\n",
            "Test Set Loss: 41.13747787475586\n",
            "Train Set Accuracy: 0.6015625\n",
            "Test Set Accuracy: 0.515625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 464\n",
            "Train Set Loss: 38.96434783935547\n",
            "Test Set Loss: 33.36105728149414\n",
            "Train Set Accuracy: 0.5703125\n",
            "Test Set Accuracy: 0.6015625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 46\n",
            "Train Set Loss: 41.18000793457031\n",
            "Test Set Loss: 34.38595962524414\n",
            "Train Set Accuracy: 0.5703125\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 96\n",
            "Train Set Loss: 42.65254592895508\n",
            "Test Set Loss: 37.017513275146484\n",
            "Train Set Accuracy: 0.53125\n",
            "Test Set Accuracy: 0.609375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 146\n",
            "Train Set Loss: 36.35002899169922\n",
            "Test Set Loss: 34.83555221557617\n",
            "Train Set Accuracy: 0.546875\n",
            "Test Set Accuracy: 0.59375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 196\n",
            "Train Set Loss: 28.522045135498047\n",
            "Test Set Loss: 26.730175018310547\n",
            "Train Set Accuracy: 0.6171875\n",
            "Test Set Accuracy: 0.6328125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 246\n",
            "Train Set Loss: 40.47268295288086\n",
            "Test Set Loss: 35.43220901489258\n",
            "Train Set Accuracy: 0.4765625\n",
            "Test Set Accuracy: 0.515625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 296\n",
            "Train Set Loss: 31.4117374420166\n",
            "Test Set Loss: 33.351871490478516\n",
            "Train Set Accuracy: 0.6484375\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "DpTsZZLmaB4-"
      },
      "source": [
        "## 4. Results\n",
        "### 4.1 Plot Training/Test Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Yw9hpEi7aB4-"
      },
      "source": [
        "# Plot Loss\n",
        "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
        "plt.plot(loss_hist)\n",
        "plt.plot(test_loss_hist)\n",
        "plt.legend([\"Test Loss\", \"Train Loss\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6ww2lt6paB4_"
      },
      "source": [
        "### 4.2 Test Set Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0mLwBng9fhkn"
      },
      "source": [
        "def Binarize(tensor):\n",
        "    tensor[tensor > 0] = 1\n",
        "    tensor[tensor == 0] = 0\n",
        "    tensor[tensor < 0] = -1\n",
        "    return tensor\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#     # initialize layers\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "#         self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=12, out_channels=64, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "#         self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.fc2 = nn.Linear(64*5*5, 10, bias= False)\n",
        "#         self.lif3 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Initialize LIF state variables and spike output tensors\n",
        "#         spk1, syn1, mem1 = self.lif1.init_stein(batch_size, 12, 13, 13)\n",
        "#         spk2, syn2, mem2 = self.lif1.init_stein(batch_size, 64, 5, 5)\n",
        "#         spk3, syn3, mem3 = self.lif2.init_stein(batch_size, 10)\n",
        "\n",
        "#         spk3_rec = []\n",
        "#         mem3_rec = []\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             conv1_bin_weight = self.conv1.weight.data.clone()\n",
        "#             self.conv1.weight.data = Binarize(conv1_bin_weight)\n",
        "            \n",
        "#             # cur1 = F.max_pool2d(self.conv1(x), 2)\n",
        "#             cur1 = F.max_pool2d(F.conv2d(x, self.conv1.weight, bias=None, stride=1,\n",
        "#                                    padding=1), 2)\n",
        "            \n",
        "#             spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "\n",
        "#             conv2_bin_weight = self.conv2.weight.data.clone()\n",
        "#             self.conv2.weight.data = Binarize(conv2_bin_weight)\n",
        "\n",
        "#             # cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
        "#             cur2 = F.max_pool2d(F.conv2d(spk1, self.conv2.weight, bias=None, stride=1,\n",
        "#                                    padding=1), 2)\n",
        "            \n",
        "#             spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "\n",
        "#             fc_bin_weight = self.fc2.weight.data.clone()\n",
        "#             self.fc2.weight.data = Binarize(fc_bin_weight)\n",
        "\n",
        "#             # cur3 = self.fc2(spk2.view(batch_size, -1))\n",
        "#             cur3 = F.linear(spk2.view(batch_size, -1), self.fc2.weight)\n",
        "#             spk3, syn3, mem3 = self.lif3(cur3, syn3, mem3)\n",
        "\n",
        "#             spk3_rec.append(spk3)\n",
        "#             mem3_rec.append(mem3)\n",
        "\n",
        "#         return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "# net = Net().to(device)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden, bias=False)\n",
        "        self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs, bias=False)\n",
        "        self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spk1, syn1, mem1 = self.lif1.init_stein(batch_size, num_hidden)\n",
        "        spk2, syn2, mem2 = self.lif2.init_stein(batch_size, num_outputs)\n",
        "\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            fc1_bin_weight = self.fc1.weight.data.clone()\n",
        "            self.fc1.weight.data = Binarize(fc1_bin_weight)\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "\n",
        "            fc2_bin_weight = self.fc2.weight.data.clone()\n",
        "            self.fc2.weight.data = Binarize(fc2_bin_weight)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "            # print(spk2.shape)\n",
        "\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "net = Net().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "g5y4NZAgaB49",
        "outputId": "68e0e0ec-0a40-4682-c014-a17fc8258d23"
      },
      "source": [
        "Old Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ghbg4Ga2fhkq"
      },
      "source": [
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(4):\n",
        "\n",
        "    minibatch_counter = 0\n",
        "    train_batch = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data_it, targets_it in train_batch:\n",
        "        data_it = data_it.to(device)\n",
        "        targets_it = targets_it.to(device)\n",
        "\n",
        "        output, mem_rec = net(data_it.view(batch_size, -1))\n",
        "        # output, mem_rec = net(data_it.view(batch_size, 1, 28, 28)) # [28x28] or [1x28x28]?\n",
        "        log_p_y = log_softmax_fn(mem_rec)\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "\n",
        "        # Sum loss over time steps to perform BPTT\n",
        "        for step in range(num_steps):\n",
        "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
        "\n",
        "        # Gradient calculation\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward(retain_graph=True)\n",
        "\n",
        "        # Weight Update\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        test_data = itertools.cycle(test_loader)\n",
        "        testdata_it, testtargets_it = next(test_data)\n",
        "        testdata_it = testdata_it.to(device)\n",
        "        testtargets_it = testtargets_it.to(device)\n",
        "\n",
        "        # Test set forward pass\n",
        "        test_output, test_mem_rec = net(testdata_it.view(batch_size, -1))\n",
        "        # test_output, test_mem_rec = net(testdata_it.view(batch_size, 1, 28, 28))\n",
        "\n",
        "        # Test set loss\n",
        "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
        "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
        "        loss_val_test = loss_fn(log_p_ytest, testtargets_it)\n",
        "        test_loss_hist.append(loss_val_test.item())\n",
        "\n",
        "        # Print test/train loss/accuracy\n",
        "        if counter % 50 == 0:\n",
        "          train_printer()\n",
        "        minibatch_counter += 1\n",
        "        counter += 1\n",
        "\n",
        "loss_hist_true_grad = loss_hist\n",
        "test_loss_hist_true_grad = test_loss_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "hDQg3UsUaB4_"
      },
      "source": [
        "Voila! That's it for static MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Rh7ZwFs4aB4_"
      },
      "source": [
        "## 5. Spiking MNIST\n",
        "Part of the appeal of SNNs is their ability to handle time-varying spiking data. So let's use rate-coding to convert MNIST into spiking MNIST using the `spikegen` module in Tutorial 1, and train our network with that instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DYGblNEgaB5A",
        "outputId": "32af4464-b708-4e2a-e43c-544e59f87182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "from snntorch import spikegen\n",
        "\n",
        "# MNIST to spiking-MNIST\n",
        "spike_data, spike_targets = spikegen.rate(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps,\n",
        "                                                      gain=1, offset=0, convert_targets=False, temporal_targets=False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e4c5a5200b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# MNIST to spiking-MNIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m spike_data, spike_targets = spikegen.rate(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps,\n\u001b[0;32m----> 5\u001b[0;31m                                                       gain=1, offset=0, convert_targets=False, temporal_targets=False)\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: rate() got an unexpected keyword argument 'convert_targets'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "xgRdm4jRaB5A"
      },
      "source": [
        "### 5.1 Visualiser\n",
        "Just so you're damn sure it's a spiking input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "geVqzyksaB5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "566e2607-1bd1-4ae4-d37f-f3cf315abbce"
      },
      "source": [
        "!pip install celluloid # animating matplotlib plots made easy"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: celluloid in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from celluloid) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->celluloid) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->celluloid) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->celluloid) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->celluloid) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->celluloid) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->celluloid) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PbLMA0pgaB5A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "c259ef36-48a9-47f0-ef48-65da532776c7"
      },
      "source": [
        "from celluloid import Camera\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Animator\n",
        "spike_data_sample = spike_data[:, 0, 0].cpu()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "camera = Camera(fig)\n",
        "plt.axis('off')\n",
        "\n",
        "for step in range(num_steps):\n",
        "    im = ax.imshow(spike_data_sample[step, :, :], cmap='plasma')\n",
        "    camera.snap()\n",
        "\n",
        "# interval=40 specifies 40ms delay between frames\n",
        "a = camera.animate(interval=40)\n",
        "HTML(a.to_html5_video())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e5d5f5d9c954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Animator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mspike_data_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspike_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spike_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "tWw9n9H0aB5A"
      },
      "source": [
        "## 6. Define Network\n",
        "The network is the same as before. The one difference is that the for-loop iterates through the first dimension of the input:\n",
        "`cur1 = F.max_pool2d(self.conv1(x[step]), 2)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9rOLmqFPjc5k"
      },
      "source": [
        "### Binarized Layer Modules\n",
        "``Binarize`` converts weights to {-1, 1}.\n",
        "Remove `.mul_(2).add_(1)` for {0, 1}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7Z92BOo4j7Ul"
      },
      "source": [
        "import pdb\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def Binarize(tensor,quant_mode='det'):\n",
        "    if quant_mode=='det':\n",
        "        return tensor.sign()\n",
        "        # tmp = tensor.clone()\n",
        "        # tmp[tensor>0] = 1\n",
        "        # tmp[tensor==0] = 0\n",
        "        # tmp[tensor<0] = -1\n",
        "        # return tmp\n",
        "    else:\n",
        "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
        "\n",
        "\n",
        "class HingeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HingeLoss,self).__init__()\n",
        "        self.margin=1.0\n",
        "\n",
        "    def hinge_loss(self,input,target):\n",
        "            #import pdb; pdb.set_trace()\n",
        "            output=self.margin-input.mul(target)\n",
        "            output[output.le(0)]=0\n",
        "            return output.mean()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return self.hinge_loss(input,target)\n",
        "\n",
        "class SqrtHingeLossFunction(Function):\n",
        "    def __init__(self):\n",
        "        super(SqrtHingeLossFunction,self).__init__()\n",
        "        self.margin=1.0\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        output=self.margin-input.mul(target)\n",
        "        output[output.le(0)]=0\n",
        "        self.save_for_backward(input, target)\n",
        "        loss=output.mul(output).sum(0).sum(1).div(target.numel())\n",
        "        return loss\n",
        "\n",
        "    def backward(self,grad_output):\n",
        "       input, target = self.saved_tensors\n",
        "       output=self.margin-input.mul(target)\n",
        "       output[output.le(0)]=0\n",
        "       import pdb; pdb.set_trace()\n",
        "       grad_output.resize_as_(input).copy_(target).mul_(-2).mul_(output)\n",
        "       grad_output.mul_(output.ne(0).float())\n",
        "       grad_output.div_(input.numel())\n",
        "       return grad_output,grad_output\n",
        "\n",
        "def Quantize(tensor,quant_mode='det',  params=None, numBits=8):\n",
        "    tensor.clamp_(-2**(numBits-1),2**(numBits-1))\n",
        "    if quant_mode=='det':\n",
        "        tensor=tensor.mul(2**(numBits-1)).round().div(2**(numBits-1))\n",
        "    else:\n",
        "        tensor=tensor.mul(2**(numBits-1)).round().add(torch.rand(tensor.size()).add(-0.5)).div(2**(numBits-1))\n",
        "        quant_fixed(tensor, params)\n",
        "    return tensor\n",
        "\n",
        "# import torch.nn._functions as tnnf\n",
        "\n",
        "\n",
        "class BinarizeLinear(nn.Linear):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        if input.size(1) != 784:\n",
        "            input.data=Binarize(input.data)\n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()\n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "        out = nn.functional.linear(input, self.weight)\n",
        "        if not self.bias is None:\n",
        "            self.bias.org=self.bias.data.clone()\n",
        "            out += self.bias.view(1, -1).expand_as(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BinarizeConv2d(nn.Conv2d):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(BinarizeConv2d, self).__init__(*kargs, **kwargs)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.size(1) != 3:\n",
        "            input.data = Binarize(input.data)\n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()\n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "\n",
        "        out = nn.functional.conv2d(input, self.weight, None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "\n",
        "        if not self.bias is None:\n",
        "            self.bias.org=self.bias.data.clone()\n",
        "            out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QO_sFhGsj_9Y"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # initialize layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "        self.lif1 = Stein_single(alpha=alpha, beta=beta)\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=64, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "        self.lif2 = Stein_single(alpha=alpha, beta=beta)\n",
        "        self.fc2 = nn.Linear(64*5*5, 10, bias= False)\n",
        "        self.lif3 = Stein_single(alpha=alpha, beta=beta)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "    def forward(self, x):\n",
        "        # Initialize LIF state variables and spike output tensors\n",
        "        spk1, syn1, mem1 = self.lif1.init_stein(batch_size, 12, 13, 13)\n",
        "        spk2, syn2, mem2 = self.lif1.init_stein(batch_size, 64, 5, 5)\n",
        "        spk3, syn3, mem3 = self.lif2.init_stein(batch_size, 10)\n",
        "\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = F.max_pool2d(self.conv1(x[step]), 2)\n",
        "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "\n",
        "            cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
        "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "\n",
        "            cur3 = self.fc2(self.dropout(spk2.view(batch_size, -1)))\n",
        "            spk3, syn3, mem3 = self.lif3(cur3, syn3, mem3)\n",
        "\n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "net = Net().to(device)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nzpf57Q1aB5B"
      },
      "source": [
        "## 7. Training\n",
        "We make a slight modification to our print-out functions to handle the new first dimension of the input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "w1saOFIFaB5B"
      },
      "source": [
        "def print_batch_accuracy(data, targets, train=False):\n",
        "    # output, _ = net(data.view(num_steps, batch_size, -1))\n",
        "    output, _ = net(data.view(num_steps, batch_size, 1, 28, 28))\n",
        "    _, idx = output.sum(dim=0).max(1)\n",
        "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
        "\n",
        "    if train:\n",
        "        print(f\"Train Set Accuracy: {acc}\")\n",
        "    else:\n",
        "        print(f\"Test Set Accuracy: {acc}\")\n",
        "\n",
        "def train_printer():\n",
        "    print(f\"Epoch {epoch}, Minibatch {minibatch_counter}\")\n",
        "    print(f\"Train Set Loss: {loss_hist[counter]}\")\n",
        "    print(f\"Test Set Loss: {test_loss_hist[counter]}\")\n",
        "    print_batch_accuracy(spike_data, spike_targets, train=True)\n",
        "    print_batch_accuracy(test_spike_data, test_spike_targets, train=False)\n",
        "    print(\"\\n\")\n"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "V9qnIq0MaB5B"
      },
      "source": [
        "### 7.1 Optimizer & Loss\n",
        "We'll keep our optimizer and loss the exact same as the static MNIST case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KeWZQlzEaB5B"
      },
      "source": [
        "lr = 1e-3\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
        "loss_fn = nn.NLLLoss()"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "VCHfUz7waB5C"
      },
      "source": [
        "### 7.2 Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "H_P1ffX-kIvR"
      },
      "source": [
        "High precision BPTT training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "noSeE-t4kN33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b05041-13e6-46d5-8aca-7afb8346fd23"
      },
      "source": [
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    if epoch < 5: \n",
        "      new_lr = lr * (0.85 ** epoch)\n",
        "      # new_lr = lr * (0.95**epoch)\n",
        "    else: new_lr = lr * (0.85**4) * (0.8**(epoch-4))\n",
        "    # lr = lr\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(10):\n",
        "\n",
        "    minibatch_counter = 0\n",
        "    train_batch = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data_it, targets_it in train_batch:\n",
        "        data_it = data_it.to(device)\n",
        "        targets_it = targets_it.to(device)\n",
        "\n",
        "        # Spike generator\n",
        "        spike_data, spike_targets = spikegen.rate(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps,\n",
        "                                                  gain=1, offset=0)\n",
        "        \n",
        "\n",
        "        # Forward pass\n",
        "        # output, mem_rec = net(spike_data.view(num_steps, batch_size, -1))\n",
        "        output, mem_rec = net(spike_data.view(num_steps, batch_size, 1, 28, 28))\n",
        "        log_p_y = log_softmax_fn(mem_rec)\n",
        "        loss_val = torch.zeros(1, dtype=dtype, device=device)\n",
        "\n",
        "        # Sum loss over time steps to perform BPTT\n",
        "        for step in range(num_steps):\n",
        "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
        "        \n",
        "        adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "        # BNN OPTimization\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        # for p in list(net.parameters()):\n",
        "        #         if hasattr(p,'org'):\n",
        "        #             p.data.copy_(p.org)\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        # for p in list(net.parameters()):\n",
        "        #         if hasattr(p,'org'):\n",
        "        #             p.org.copy_(p.data.clamp_(-1,1))\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        test_data = itertools.cycle(test_loader)\n",
        "        testdata_it, testtargets_it = next(test_data)\n",
        "        testdata_it = testdata_it.to(device)\n",
        "        testtargets_it = testtargets_it.to(device)\n",
        "\n",
        "        # Test set spike conversion\n",
        "        test_spike_data, test_spike_targets = spikegen.rate(testdata_it, testtargets_it, num_outputs=num_outputs,\n",
        "                                                            num_steps=num_steps, gain=1, offset=0)\n",
        "\n",
        "        # Test set forward pass\n",
        "        # test_output, test_mem_rec = net(test_spike_data.view(num_steps, batch_size, -1))\n",
        "        test_output, test_mem_rec = net(test_spike_data.view(num_steps, batch_size, 1, 28, 28))\n",
        "\n",
        "        # Test set loss\n",
        "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
        "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
        "        loss_val_test = loss_fn(log_p_ytest, testtargets_it)\n",
        "        test_loss_hist.append(loss_val_test.item())\n",
        "\n",
        "        # Print test/train loss/accuracy\n",
        "        if counter % 50 == 0:\n",
        "          train_printer()\n",
        "        minibatch_counter += 1\n",
        "        counter += 1\n",
        "\n",
        "loss_hist_true_grad = loss_hist\n",
        "test_loss_hist_true_grad = test_loss_hist"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 57.558692932128906\n",
            "Test Set Loss: 57.49127197265625\n",
            "Train Set Accuracy: 0.1640625\n",
            "Test Set Accuracy: 0.109375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 50\n",
            "Train Set Loss: 34.72120666503906\n",
            "Test Set Loss: 32.96978759765625\n",
            "Train Set Accuracy: 0.1875\n",
            "Test Set Accuracy: 0.1796875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 100\n",
            "Train Set Loss: 34.23112487792969\n",
            "Test Set Loss: 36.1490592956543\n",
            "Train Set Accuracy: 0.1875\n",
            "Test Set Accuracy: 0.109375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 150\n",
            "Train Set Loss: 37.42290496826172\n",
            "Test Set Loss: 29.965831756591797\n",
            "Train Set Accuracy: 0.0546875\n",
            "Test Set Accuracy: 0.1171875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 200\n",
            "Train Set Loss: 34.34812927246094\n",
            "Test Set Loss: 33.90836715698242\n",
            "Train Set Accuracy: 0.109375\n",
            "Test Set Accuracy: 0.1796875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 250\n",
            "Train Set Loss: 32.98904037475586\n",
            "Test Set Loss: 30.501550674438477\n",
            "Train Set Accuracy: 0.1875\n",
            "Test Set Accuracy: 0.1328125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 300\n",
            "Train Set Loss: 33.53864669799805\n",
            "Test Set Loss: 31.875106811523438\n",
            "Train Set Accuracy: 0.109375\n",
            "Test Set Accuracy: 0.140625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 350\n",
            "Train Set Loss: 29.683025360107422\n",
            "Test Set Loss: 31.606300354003906\n",
            "Train Set Accuracy: 0.1328125\n",
            "Test Set Accuracy: 0.109375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 400\n",
            "Train Set Loss: 32.311798095703125\n",
            "Test Set Loss: 31.192895889282227\n",
            "Train Set Accuracy: 0.1328125\n",
            "Test Set Accuracy: 0.1640625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 450\n",
            "Train Set Loss: 30.12399673461914\n",
            "Test Set Loss: 28.042343139648438\n",
            "Train Set Accuracy: 0.21875\n",
            "Test Set Accuracy: 0.171875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 32\n",
            "Train Set Loss: 30.59907341003418\n",
            "Test Set Loss: 26.96677017211914\n",
            "Train Set Accuracy: 0.140625\n",
            "Test Set Accuracy: 0.1875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 82\n",
            "Train Set Loss: 27.617660522460938\n",
            "Test Set Loss: 30.04707145690918\n",
            "Train Set Accuracy: 0.1796875\n",
            "Test Set Accuracy: 0.1953125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 132\n",
            "Train Set Loss: 28.15633201599121\n",
            "Test Set Loss: 32.12403106689453\n",
            "Train Set Accuracy: 0.2109375\n",
            "Test Set Accuracy: 0.171875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 182\n",
            "Train Set Loss: 32.28910446166992\n",
            "Test Set Loss: 23.478342056274414\n",
            "Train Set Accuracy: 0.203125\n",
            "Test Set Accuracy: 0.2265625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 232\n",
            "Train Set Loss: 30.250526428222656\n",
            "Test Set Loss: 26.041828155517578\n",
            "Train Set Accuracy: 0.2109375\n",
            "Test Set Accuracy: 0.2265625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 282\n",
            "Train Set Loss: 30.500139236450195\n",
            "Test Set Loss: 27.276269912719727\n",
            "Train Set Accuracy: 0.1171875\n",
            "Test Set Accuracy: 0.2109375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 332\n",
            "Train Set Loss: 31.34649658203125\n",
            "Test Set Loss: 30.88158416748047\n",
            "Train Set Accuracy: 0.1640625\n",
            "Test Set Accuracy: 0.1171875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 382\n",
            "Train Set Loss: 26.816160202026367\n",
            "Test Set Loss: 23.538700103759766\n",
            "Train Set Accuracy: 0.21875\n",
            "Test Set Accuracy: 0.1484375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 432\n",
            "Train Set Loss: 21.861427307128906\n",
            "Test Set Loss: 25.947547912597656\n",
            "Train Set Accuracy: 0.25\n",
            "Test Set Accuracy: 0.140625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 14\n",
            "Train Set Loss: 23.28636360168457\n",
            "Test Set Loss: 26.736587524414062\n",
            "Train Set Accuracy: 0.2109375\n",
            "Test Set Accuracy: 0.2109375\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 64\n",
            "Train Set Loss: 23.695552825927734\n",
            "Test Set Loss: 21.833892822265625\n",
            "Train Set Accuracy: 0.1953125\n",
            "Test Set Accuracy: 0.15625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 114\n",
            "Train Set Loss: 22.303272247314453\n",
            "Test Set Loss: 22.911312103271484\n",
            "Train Set Accuracy: 0.2265625\n",
            "Test Set Accuracy: 0.2265625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 164\n",
            "Train Set Loss: 24.750869750976562\n",
            "Test Set Loss: 24.50940704345703\n",
            "Train Set Accuracy: 0.1796875\n",
            "Test Set Accuracy: 0.1875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 214\n",
            "Train Set Loss: 30.686349868774414\n",
            "Test Set Loss: 27.583850860595703\n",
            "Train Set Accuracy: 0.1796875\n",
            "Test Set Accuracy: 0.2265625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 264\n",
            "Train Set Loss: 26.86478614807129\n",
            "Test Set Loss: 25.78297996520996\n",
            "Train Set Accuracy: 0.1796875\n",
            "Test Set Accuracy: 0.140625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 314\n",
            "Train Set Loss: 20.504619598388672\n",
            "Test Set Loss: 26.299802780151367\n",
            "Train Set Accuracy: 0.234375\n",
            "Test Set Accuracy: 0.2109375\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 364\n",
            "Train Set Loss: 24.35398292541504\n",
            "Test Set Loss: 23.56924057006836\n",
            "Train Set Accuracy: 0.2265625\n",
            "Test Set Accuracy: 0.2890625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 414\n",
            "Train Set Loss: 24.441308975219727\n",
            "Test Set Loss: 21.26761245727539\n",
            "Train Set Accuracy: 0.2109375\n",
            "Test Set Accuracy: 0.2890625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 464\n",
            "Train Set Loss: 21.472654342651367\n",
            "Test Set Loss: 20.545129776000977\n",
            "Train Set Accuracy: 0.25\n",
            "Test Set Accuracy: 0.2578125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 46\n",
            "Train Set Loss: 21.34246063232422\n",
            "Test Set Loss: 23.037729263305664\n",
            "Train Set Accuracy: 0.28125\n",
            "Test Set Accuracy: 0.28125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 96\n",
            "Train Set Loss: 20.606565475463867\n",
            "Test Set Loss: 19.522872924804688\n",
            "Train Set Accuracy: 0.3046875\n",
            "Test Set Accuracy: 0.3125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 146\n",
            "Train Set Loss: 21.08740234375\n",
            "Test Set Loss: 17.48580551147461\n",
            "Train Set Accuracy: 0.390625\n",
            "Test Set Accuracy: 0.4453125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 196\n",
            "Train Set Loss: 23.08365249633789\n",
            "Test Set Loss: 22.751441955566406\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.6328125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 246\n",
            "Train Set Loss: 21.775400161743164\n",
            "Test Set Loss: 23.382274627685547\n",
            "Train Set Accuracy: 0.7421875\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 296\n",
            "Train Set Loss: 22.16400146484375\n",
            "Test Set Loss: 17.515422821044922\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.78125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 346\n",
            "Train Set Loss: 15.885464668273926\n",
            "Test Set Loss: 14.48397445678711\n",
            "Train Set Accuracy: 0.8046875\n",
            "Test Set Accuracy: 0.859375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 396\n",
            "Train Set Loss: 16.39508056640625\n",
            "Test Set Loss: 16.89984703063965\n",
            "Train Set Accuracy: 0.84375\n",
            "Test Set Accuracy: 0.8359375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 446\n",
            "Train Set Loss: 14.906195640563965\n",
            "Test Set Loss: 19.76474952697754\n",
            "Train Set Accuracy: 0.8125\n",
            "Test Set Accuracy: 0.7734375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 28\n",
            "Train Set Loss: 16.7442684173584\n",
            "Test Set Loss: 17.425207138061523\n",
            "Train Set Accuracy: 0.7578125\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 78\n",
            "Train Set Loss: 18.146320343017578\n",
            "Test Set Loss: 13.451031684875488\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.859375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 128\n",
            "Train Set Loss: 16.179176330566406\n",
            "Test Set Loss: 16.172826766967773\n",
            "Train Set Accuracy: 0.8359375\n",
            "Test Set Accuracy: 0.8359375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 178\n",
            "Train Set Loss: 17.52499008178711\n",
            "Test Set Loss: 11.960299491882324\n",
            "Train Set Accuracy: 0.8359375\n",
            "Test Set Accuracy: 0.875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 228\n",
            "Train Set Loss: 17.834692001342773\n",
            "Test Set Loss: 16.769142150878906\n",
            "Train Set Accuracy: 0.8359375\n",
            "Test Set Accuracy: 0.8203125\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 278\n",
            "Train Set Loss: 13.580544471740723\n",
            "Test Set Loss: 15.72870922088623\n",
            "Train Set Accuracy: 0.875\n",
            "Test Set Accuracy: 0.8359375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 328\n",
            "Train Set Loss: 14.348274230957031\n",
            "Test Set Loss: 15.26151180267334\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 378\n",
            "Train Set Loss: 19.231477737426758\n",
            "Test Set Loss: 16.90195655822754\n",
            "Train Set Accuracy: 0.8203125\n",
            "Test Set Accuracy: 0.8515625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 428\n",
            "Train Set Loss: 13.814325332641602\n",
            "Test Set Loss: 12.611746788024902\n",
            "Train Set Accuracy: 0.9140625\n",
            "Test Set Accuracy: 0.8671875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 10\n",
            "Train Set Loss: 14.463080406188965\n",
            "Test Set Loss: 16.010765075683594\n",
            "Train Set Accuracy: 0.8828125\n",
            "Test Set Accuracy: 0.8046875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 60\n",
            "Train Set Loss: 13.93007755279541\n",
            "Test Set Loss: 10.602481842041016\n",
            "Train Set Accuracy: 0.8828125\n",
            "Test Set Accuracy: 0.9140625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 110\n",
            "Train Set Loss: 13.87060546875\n",
            "Test Set Loss: 17.030128479003906\n",
            "Train Set Accuracy: 0.875\n",
            "Test Set Accuracy: 0.859375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 160\n",
            "Train Set Loss: 14.396306037902832\n",
            "Test Set Loss: 16.242311477661133\n",
            "Train Set Accuracy: 0.890625\n",
            "Test Set Accuracy: 0.84375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 210\n",
            "Train Set Loss: 16.544374465942383\n",
            "Test Set Loss: 11.989846229553223\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.921875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 260\n",
            "Train Set Loss: 14.824339866638184\n",
            "Test Set Loss: 11.538475036621094\n",
            "Train Set Accuracy: 0.8359375\n",
            "Test Set Accuracy: 0.921875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 310\n",
            "Train Set Loss: 16.512975692749023\n",
            "Test Set Loss: 15.66776180267334\n",
            "Train Set Accuracy: 0.828125\n",
            "Test Set Accuracy: 0.8359375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 360\n",
            "Train Set Loss: 13.134337425231934\n",
            "Test Set Loss: 14.34703540802002\n",
            "Train Set Accuracy: 0.890625\n",
            "Test Set Accuracy: 0.8671875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 410\n",
            "Train Set Loss: 12.256902694702148\n",
            "Test Set Loss: 11.840015411376953\n",
            "Train Set Accuracy: 0.90625\n",
            "Test Set Accuracy: 0.9140625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 460\n",
            "Train Set Loss: 13.5303955078125\n",
            "Test Set Loss: 18.340906143188477\n",
            "Train Set Accuracy: 0.8671875\n",
            "Test Set Accuracy: 0.8203125\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 42\n",
            "Train Set Loss: 14.878417015075684\n",
            "Test Set Loss: 13.528315544128418\n",
            "Train Set Accuracy: 0.8984375\n",
            "Test Set Accuracy: 0.890625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 92\n",
            "Train Set Loss: 15.971378326416016\n",
            "Test Set Loss: 13.357463836669922\n",
            "Train Set Accuracy: 0.8671875\n",
            "Test Set Accuracy: 0.890625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 142\n",
            "Train Set Loss: 12.478955268859863\n",
            "Test Set Loss: 14.696950912475586\n",
            "Train Set Accuracy: 0.875\n",
            "Test Set Accuracy: 0.8671875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 192\n",
            "Train Set Loss: 14.332098960876465\n",
            "Test Set Loss: 12.723336219787598\n",
            "Train Set Accuracy: 0.8984375\n",
            "Test Set Accuracy: 0.8984375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 242\n",
            "Train Set Loss: 11.552627563476562\n",
            "Test Set Loss: 12.588647842407227\n",
            "Train Set Accuracy: 0.953125\n",
            "Test Set Accuracy: 0.921875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 292\n",
            "Train Set Loss: 13.566926002502441\n",
            "Test Set Loss: 16.983306884765625\n",
            "Train Set Accuracy: 0.8828125\n",
            "Test Set Accuracy: 0.859375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 342\n",
            "Train Set Loss: 13.528095245361328\n",
            "Test Set Loss: 15.300646781921387\n",
            "Train Set Accuracy: 0.84375\n",
            "Test Set Accuracy: 0.875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 392\n",
            "Train Set Loss: 12.73421573638916\n",
            "Test Set Loss: 12.9402437210083\n",
            "Train Set Accuracy: 0.8828125\n",
            "Test Set Accuracy: 0.875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 442\n",
            "Train Set Loss: 14.198121070861816\n",
            "Test Set Loss: 14.075084686279297\n",
            "Train Set Accuracy: 0.8828125\n",
            "Test Set Accuracy: 0.890625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 24\n",
            "Train Set Loss: 13.228813171386719\n",
            "Test Set Loss: 14.858819961547852\n",
            "Train Set Accuracy: 0.875\n",
            "Test Set Accuracy: 0.828125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 74\n",
            "Train Set Loss: 14.819744110107422\n",
            "Test Set Loss: 13.318256378173828\n",
            "Train Set Accuracy: 0.875\n",
            "Test Set Accuracy: 0.890625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 124\n",
            "Train Set Loss: 10.885631561279297\n",
            "Test Set Loss: 14.331174850463867\n",
            "Train Set Accuracy: 0.890625\n",
            "Test Set Accuracy: 0.8671875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 174\n",
            "Train Set Loss: 12.987242698669434\n",
            "Test Set Loss: 12.319938659667969\n",
            "Train Set Accuracy: 0.8984375\n",
            "Test Set Accuracy: 0.875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 224\n",
            "Train Set Loss: 15.886048316955566\n",
            "Test Set Loss: 13.71412467956543\n",
            "Train Set Accuracy: 0.84375\n",
            "Test Set Accuracy: 0.90625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 274\n",
            "Train Set Loss: 15.744436264038086\n",
            "Test Set Loss: 13.881232261657715\n",
            "Train Set Accuracy: 0.8828125\n",
            "Test Set Accuracy: 0.859375\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 324\n",
            "Train Set Loss: 9.58821964263916\n",
            "Test Set Loss: 14.125486373901367\n",
            "Train Set Accuracy: 0.9609375\n",
            "Test Set Accuracy: 0.8984375\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 374\n",
            "Train Set Loss: 17.364582061767578\n",
            "Test Set Loss: 15.479094505310059\n",
            "Train Set Accuracy: 0.8515625\n",
            "Test Set Accuracy: 0.8671875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 424\n",
            "Train Set Loss: 12.97854995727539\n",
            "Test Set Loss: 13.13720417022705\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.8984375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 6\n",
            "Train Set Loss: 10.91820240020752\n",
            "Test Set Loss: 13.742091178894043\n",
            "Train Set Accuracy: 0.8984375\n",
            "Test Set Accuracy: 0.890625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 56\n",
            "Train Set Loss: 12.402298927307129\n",
            "Test Set Loss: 16.77045440673828\n",
            "Train Set Accuracy: 0.8984375\n",
            "Test Set Accuracy: 0.8515625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 106\n",
            "Train Set Loss: 12.989175796508789\n",
            "Test Set Loss: 12.368380546569824\n",
            "Train Set Accuracy: 0.890625\n",
            "Test Set Accuracy: 0.8984375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 156\n",
            "Train Set Loss: 14.185768127441406\n",
            "Test Set Loss: 11.220635414123535\n",
            "Train Set Accuracy: 0.90625\n",
            "Test Set Accuracy: 0.9140625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 206\n",
            "Train Set Loss: 14.93598461151123\n",
            "Test Set Loss: 15.438018798828125\n",
            "Train Set Accuracy: 0.875\n",
            "Test Set Accuracy: 0.8515625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 256\n",
            "Train Set Loss: 13.10276985168457\n",
            "Test Set Loss: 12.920125007629395\n",
            "Train Set Accuracy: 0.8984375\n",
            "Test Set Accuracy: 0.890625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 306\n",
            "Train Set Loss: 12.61320972442627\n",
            "Test Set Loss: 13.604249954223633\n",
            "Train Set Accuracy: 0.8984375\n",
            "Test Set Accuracy: 0.84375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 356\n",
            "Train Set Loss: 15.780526161193848\n",
            "Test Set Loss: 13.32032585144043\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.8828125\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 406\n",
            "Train Set Loss: 13.894617080688477\n",
            "Test Set Loss: 10.216425895690918\n",
            "Train Set Accuracy: 0.8984375\n",
            "Test Set Accuracy: 0.9375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 456\n",
            "Train Set Loss: 12.746037483215332\n",
            "Test Set Loss: 13.85398006439209\n",
            "Train Set Accuracy: 0.8984375\n",
            "Test Set Accuracy: 0.8671875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 38\n",
            "Train Set Loss: 12.38341236114502\n",
            "Test Set Loss: 13.515546798706055\n",
            "Train Set Accuracy: 0.90625\n",
            "Test Set Accuracy: 0.875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 88\n",
            "Train Set Loss: 11.541400909423828\n",
            "Test Set Loss: 14.821267127990723\n",
            "Train Set Accuracy: 0.9375\n",
            "Test Set Accuracy: 0.8828125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 138\n",
            "Train Set Loss: 13.765195846557617\n",
            "Test Set Loss: 13.91506290435791\n",
            "Train Set Accuracy: 0.890625\n",
            "Test Set Accuracy: 0.8828125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 188\n",
            "Train Set Loss: 15.388041496276855\n",
            "Test Set Loss: 13.632957458496094\n",
            "Train Set Accuracy: 0.859375\n",
            "Test Set Accuracy: 0.90625\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 238\n",
            "Train Set Loss: 10.732409477233887\n",
            "Test Set Loss: 11.987128257751465\n",
            "Train Set Accuracy: 0.9296875\n",
            "Test Set Accuracy: 0.90625\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 288\n",
            "Train Set Loss: 15.251077651977539\n",
            "Test Set Loss: 13.249785423278809\n",
            "Train Set Accuracy: 0.890625\n",
            "Test Set Accuracy: 0.90625\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 338\n",
            "Train Set Loss: 15.076704025268555\n",
            "Test Set Loss: 14.132231712341309\n",
            "Train Set Accuracy: 0.8828125\n",
            "Test Set Accuracy: 0.859375\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 388\n",
            "Train Set Loss: 12.330995559692383\n",
            "Test Set Loss: 15.97736930847168\n",
            "Train Set Accuracy: 0.90625\n",
            "Test Set Accuracy: 0.8671875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 438\n",
            "Train Set Loss: 14.055397987365723\n",
            "Test Set Loss: 13.496646881103516\n",
            "Train Set Accuracy: 0.890625\n",
            "Test Set Accuracy: 0.875\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "nVldb0Q9aB5C"
      },
      "source": [
        "## 8. Spiking MNIST Results\n",
        "### 8.1 Plot Training/Test Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "XopHe17ZaB5C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "62e9610b-8276-4811-d292-fc700f46f3b5"
      },
      "source": [
        "# Plot Loss\n",
        "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
        "plt.plot(loss_hist)\n",
        "plt.plot(test_loss_hist)\n",
        "plt.legend([\"Test Loss\", \"Train Loss\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lgu-tzsfaB5C"
      },
      "source": [
        "### 8.2 Test Set Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nyIVun6laB5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966e6cc7-e456-428c-f41f-7a3a334ab88b"
      },
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "  net.eval()\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # If current batch matches batch_size, just do the usual thing\n",
        "    if images.size()[0] == batch_size:\n",
        "      spike_test, spike_targets = spikegen.rate(images, labels, num_outputs=num_outputs, num_steps=num_steps,\n",
        "                                                            gain=1, offset=0)\n",
        "\n",
        "      # outputs, _ = net(spike_test.view(num_steps, batch_size, -1))\n",
        "      outputs, _ = net(spike_test.view(num_steps, batch_size, 1, 28, 28))\n",
        "\n",
        "    # If current batch does not match batch_size (e.g., is the final minibatch),\n",
        "    # modify batch_size in a temp variable and restore it at the end of the else block\n",
        "    else:\n",
        "      temp_bs = batch_size\n",
        "      batch_size = images.size()[0]\n",
        "      spike_test, spike_targets = spikegen.rate(images, labels, num_outputs=num_outputs, num_steps=num_steps,\n",
        "                                                            gain=1, offset=0)\n",
        "      # outputs, _ = net(spike_test.view(num_steps, images.size()[0], -1))\n",
        "      outputs, _ = net(spike_test.view(num_steps, images.size()[0], 1, 28, 28))\n",
        "      batch_size = temp_bs\n",
        "\n",
        "    _, predicted = outputs.sum(dim=0).max(1)\n",
        "    total += spike_targets.size(0)\n",
        "    correct += (predicted == spike_targets).sum().item()\n",
        "\n",
        "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
        "print(f\"Test Set Accuracy: {100 * correct / total}%\")"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total correctly classified test set images: 8746/10000\n",
            "Test Set Accuracy: 87.46%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fuzanqrCaB5C"
      },
      "source": [
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(1):\n",
        "    minibatch_counter = 0\n",
        "    data = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data_it, targets_it in data:\n",
        "        data_it = data_it.to(device)\n",
        "        targets_it = targets_it.to(device)\n",
        "\n",
        "        # Spike generator\n",
        "        spike_data, spike_targets = spikegen.rate(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps,\n",
        "                                                  gain=1, offset=0, convert_targets=False, temporal_targets=False)\n",
        "\n",
        "        # Forward pass\n",
        "        output, mem_rec = net(spike_data.view(num_steps, batch_size, -1))\n",
        "        # output, mem_rec = net(spike_data.view(num_steps, batch_size, 1, 28, 28))\n",
        "        log_p_y = log_softmax_fn(mem_rec)\n",
        "        loss_val = torch.zeros(1, dtype=dtype, device=device)\n",
        "\n",
        "        # Sum loss over time steps to perform BPTT\n",
        "        for step in range(num_steps):\n",
        "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
        "\n",
        "        # Gradient Calculation\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward(retain_graph=True)\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "\n",
        "        # Weight Update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store Loss history\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        test_data = itertools.cycle(test_loader)\n",
        "        testdata_it, testtargets_it = next(test_data)\n",
        "        testdata_it = testdata_it.to(device)\n",
        "        testtargets_it = testtargets_it.to(device)\n",
        "\n",
        "        # Test set spike conversion\n",
        "        test_spike_data, test_spike_targets = spikegen.rate(testdata_it, testtargets_it, num_outputs=num_outputs,\n",
        "                                                            num_steps=num_steps, gain=1, offset=0, convert_targets=False,\n",
        "                                                            temporal_targets=False)\n",
        "\n",
        "        # Test set forward pass\n",
        "        test_output, test_mem_rec = net(test_spike_data.view(num_steps, batch_size, -1))\n",
        "        # test_output, test_mem_rec = net(test_spike_data.view(num_steps, batch_size, 1, 28, 28))\n",
        "\n",
        "        # Test set loss\n",
        "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
        "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
        "        loss_val_test = loss_fn(log_p_ytest, test_spike_targets)\n",
        "        test_loss_hist.append(loss_val_test.item())\n",
        "\n",
        "        # Print test/train loss/accuracy\n",
        "        if counter % 50 == 0:\n",
        "            train_printer()\n",
        "        minibatch_counter += 1\n",
        "        counter += 1\n",
        "\n",
        "loss_hist_true_grad = loss_hist\n",
        "test_loss_hist_true_grad = test_loss_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zIpvq5jdaB5B"
      },
      "source": [
        "# spike_fn = FSS.apply\n",
        "# snn.neuron.slope = 50\n",
        "spike_grad = snn.FastSigmoidSurrogate.apply\n",
        "snn.slope = 50\n",
        "\n",
        "def Binarize(tensor):\n",
        "    tensor[tensor > 0] = 1\n",
        "    tensor[tensor<=0] = 0\n",
        "    return tensor\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#     # initialize layers\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "#         self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=12, out_channels=64, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "#         self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.fc2 = nn.Linear(64*5*5, 10, bias= False)\n",
        "#         self.lif3 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Initialize LIF state variables and spike output tensors\n",
        "#         spk1, syn1, mem1 = self.lif1.init_stein(batch_size, 12, 13, 13)\n",
        "#         spk2, syn2, mem2 = self.lif1.init_stein(batch_size, 64, 5, 5)\n",
        "#         spk3, syn3, mem3 = self.lif2.init_stein(batch_size, 10)\n",
        "\n",
        "#         spk3_rec = []\n",
        "#         mem3_rec = []\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             conv1_bin_weight = self.conv1.weight.data.clone()\n",
        "#             self.conv1.weight.data = Binarize(conv1_bin_weight)\n",
        "            \n",
        "#             cur1 = F.max_pool2d(self.conv1(x[step]), 2)\n",
        "#             # cur1 = F.max_pool2d(F.conv2d(x, self.conv1.weight, bias=None, stride=1,\n",
        "#             #                        padding=1), 2)\n",
        "            \n",
        "#             spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "\n",
        "#             conv2_bin_weight = self.conv2.weight.data.clone()\n",
        "#             self.conv2.weight.data = Binarize(conv2_bin_weight)\n",
        "\n",
        "#             cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
        "#             # cur2 = F.max_pool2d(F.conv2d(spk1, self.conv2.weight, bias=None, stride=1,\n",
        "#             #                        padding=1), 2)\n",
        "            \n",
        "#             spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "\n",
        "#             fc_bin_weight = self.fc2.weight.data.clone()\n",
        "#             self.fc2.weight.data = Binarize(fc_bin_weight)\n",
        "\n",
        "#             cur3 = self.fc2(spk2.view(batch_size, -1))\n",
        "#             # cur3 = F.linear(spk2.view(batch_size, -1), self.fc2.weight)\n",
        "#             spk3, syn3, mem3 = self.lif3(cur3, syn3, mem3)\n",
        "\n",
        "#             spk3_rec.append(spk3)\n",
        "#             mem3_rec.append(mem3)\n",
        "\n",
        "#         return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "# net = Net().to(device)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden, bias=False)\n",
        "        self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs, bias=False)\n",
        "        self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spk1, syn1, mem1 = self.lif1.init_stein(batch_size, num_hidden)\n",
        "        spk2, syn2, mem2 = self.lif2.init_stein(batch_size, num_outputs)\n",
        "\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            fc1_bin_weight = self.fc1.weight.data.clone()\n",
        "            self.fc1.weight.data = Binarize(fc1_bin_weight)\n",
        "            cur1 = self.fc1(x[step])\n",
        "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "\n",
        "            fc2_bin_weight = self.fc2.weight.data.clone()\n",
        "            self.fc2.weight.data = Binarize(fc2_bin_weight)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "            # print(spk2.shape)\n",
        "\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "net = Net().to(device)\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # initialize layers\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "#         self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=12, out_channels=64, kernel_size=5, stride=1, padding=1)\n",
        "#         self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.fc2 = nn.Linear(64*5*5, 10)\n",
        "#         self.lif3 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "#         # self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=0)\n",
        "#         # self.lif1 = LIF(spike_fn=spike_fn, alpha=alpha, beta=beta)\n",
        "#         # self.fc1 = nn.Linear(26*26*3, 10)\n",
        "#         # self.lif2 = LIF(spike_fn=spike_fn, alpha=alpha, beta=beta)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Initialize LIF state variables and spike output tensors\n",
        "#         spk1, syn1, mem1 = self.lif1.init_stein(batch_size, 12, 13, 13)\n",
        "#         spk2, syn2, mem2 = self.lif2.init_stein(batch_size, 64, 5, 5)\n",
        "#         spk3, syn3, mem3 = self.lif3.init_stein(batch_size, 10)\n",
        "\n",
        "#         spk3_rec = []\n",
        "#         mem3_rec = []\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             cur1 = F.max_pool2d(self.conv1(x[step]), 2) # add max-pooling to membrane or spikes?\n",
        "#             spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "#             cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
        "#             spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "#             cur3 = self.fc2(spk2.view(batch_size, -1))\n",
        "#             spk3, syn3, mem3 = self.lif3(cur3, syn3, mem3)\n",
        "\n",
        "#             spk3_rec.append(spk3)\n",
        "#             mem3_rec.append(mem3)\n",
        "\n",
        "#         return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "# net = Net().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "WcBkni9VaB5D"
      },
      "source": [
        "Professor Lu has kidnapped my daughter and won't return her until I hit 99.99% accuracy, please help\n",
        "-JE"
      ]
    }
  ]
}