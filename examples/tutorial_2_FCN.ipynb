{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# snnTorch Test: Making $\\alpha$ and $\\beta$ learnable parameters\n",
    "### By Jason K. Eshraghian"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient-based Learning in Spiking Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install the test PyPi Distribution of snntorch\n",
    "!pip install -i https://test.pypi.org/simple/ snntorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Setting up the Static MNIST Dataset\n",
    "### 1.1. Import packages and setup environment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Define network and SNN parameters\n",
    "We will use a 784-1000-10 FCN architecture for a sequence of 25 time steps.\n",
    "\n",
    "* `alpha` is the decay rate of the synaptic current of a neuron\n",
    "* `beta` is the decay rate of the membrane potential of a neuron"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 28*28\n",
    "num_hidden = 1000\n",
    "num_outputs = 10\n",
    "\n",
    "# Training Parameters\n",
    "batch_size=128\n",
    "data_path='/data/mnist'\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "time_step = 1e-3\n",
    "tau_mem = 3e-3\n",
    "tau_syn = 2.2e-3\n",
    "\n",
    "# these will be overridden\n",
    "alpha = float(np.exp(-time_step/tau_syn))\n",
    "beta = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Download MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Create DataLoaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Define Network\n",
    "\n",
    "Override snn.SRM0:\n",
    "* np.log(alpha) is now torch.log(alpha)\n",
    "* update tau_srm every time alpha and beta are changed by placing it in `forward`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SRM0(snn.LIF):\n",
    "\n",
    "    def __init__(self, alpha, beta, threshold=1.0, spike_grad=None):\n",
    "        super(SRM0, self).__init__(alpha, beta, threshold, spike_grad)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, input_, syn_pre, syn_post, mem):\n",
    "\n",
    "        self.tau_srm = torch.log(self.alpha) / (torch.log(self.beta) - torch.log(self.alpha)) + 1\n",
    "        spk, reset = self.fire(mem)\n",
    "        syn_pre = (self.alpha*syn_pre + input_) * (1 - reset)\n",
    "        syn_post = (self.beta * syn_post - input_) * (1 - reset)\n",
    "        mem = self.tau_srm * (syn_pre + syn_post)*(1-reset) + (mem*reset - reset * self.threshold)\n",
    "\n",
    "        return spk, syn_pre, syn_post, mem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: alpha and beta are a single parameter each, therefore 1x1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # initialize layers\n",
    "        self.alpha = nn.Linear(1, 1, bias=False)\n",
    "        self.beta = nn.Linear(1, 1, bias=False)\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = SRM0(alpha=self.alpha.weight, beta=self.beta.weight, threshold=0.3) # NOTE: lowering threshold induces more spiking, coz SRM0 struggles with spiking.\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = SRM0(alpha=self.alpha.weight, beta=self.beta.weight, threshold=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spk1, pre_syn1, post_syn1, mem1 = self.lif1.init_srm0(batch_size, num_hidden)\n",
    "        spk2, pre_syn2, post_syn2, mem2 = self.lif2.init_srm0(batch_size, num_outputs)\n",
    "\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, pre_syn1, post_syn1, mem1 = self.lif1(cur1, pre_syn1, post_syn1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, pre_syn2, post_syn2, mem2 = self.lif2(cur2, pre_syn2, post_syn2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "net = Net().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, we need to check that alpha > beta AND alpha & beta > 0.\n",
    "So re-run the above till the following works."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(net.alpha.weight)\n",
    "print(net.beta.weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train Set Accuracy: {acc}\")\n",
    "    else:\n",
    "        print(f\"Test Set Accuracy: {acc}\")\n",
    "\n",
    "def train_printer():\n",
    "    print(net.alpha.weight)\n",
    "    print(net.beta.weight)\n",
    "    print(f\"Epoch {epoch}, Minibatch {minibatch_counter}\")\n",
    "    print(f\"Train Set Loss: {loss_hist[counter]}\")\n",
    "    print(f\"Test Set Loss: {test_loss_hist[counter]}\")\n",
    "    print_batch_accuracy(data_it, targets_it, train=True)\n",
    "    print_batch_accuracy(testdata_it, testtargets_it, train=False)\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Training Loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=2e-4, betas=(0.9, 0.999))\n",
    "log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(3):\n",
    "    minibatch_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data_it, targets_it in train_batch:\n",
    "        data_it = data_it.to(device)\n",
    "        targets_it = targets_it.to(device)\n",
    "\n",
    "        output, mem_rec = net(data_it.view(batch_size, -1))\n",
    "        log_p_y = log_softmax_fn(mem_rec)\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "        # Sum loss over time steps: BPTT\n",
    "        for step in range(num_steps):\n",
    "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
    "\n",
    "        # Gradient calculation\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward(retain_graph=True)\n",
    "\n",
    "        # Weight Update\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), 1) # gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        test_data = itertools.cycle(test_loader)\n",
    "        testdata_it, testtargets_it = next(test_data)\n",
    "        testdata_it = testdata_it.to(device)\n",
    "        testtargets_it = testtargets_it.to(device)\n",
    "\n",
    "        # Test set forward pass\n",
    "        test_output, test_mem_rec = net(testdata_it.view(batch_size, -1))\n",
    "\n",
    "        # Test set loss\n",
    "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
    "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
    "        loss_val_test = loss_fn(log_p_ytest, testtargets_it)\n",
    "        test_loss_hist.append(loss_val_test.item())\n",
    "\n",
    "        # Print test/train loss/accuracy\n",
    "        if counter % 50 == 0:\n",
    "            train_printer()\n",
    "        minibatch_counter += 1\n",
    "        counter += 1\n",
    "\n",
    "loss_hist_true_grad = loss_hist\n",
    "test_loss_hist_true_grad = test_loss_hist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Results\n",
    "Observations:\n",
    "* alpha wants to go to 1\n",
    "* beta wants to go to 0\n",
    "* once beta hits 0, losses were all `nan`\n",
    "* Using SRM0, acc went up to 60% then tapered off btwn 35-40%\n",
    "* Perhaps there would be more promise using snn.Stein\n",
    "\n",
    "### 4.1 Plot Training/Test Loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
    "plt.plot(loss_hist)\n",
    "plt.plot(test_loss_hist)\n",
    "plt.legend([\"Test Loss\", \"Train Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Test Set Accuracy\n",
    "This function just iterates over all minibatches to obtain a measure of accuracy over the full 10,000 samples in the test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "  net.eval()\n",
    "  for data in test_loader:\n",
    "    images, labels = data\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # If current batch matches batch_size, just do the usual thing\n",
    "    if images.size()[0] == batch_size:\n",
    "      outputs, _ = net(images.view(batch_size, -1))\n",
    "\n",
    "    # If current batch does not match batch_size (e.g., is the final minibatch),\n",
    "    # modify batch_size in a temp variable and restore it at the end of the else block\n",
    "    else:\n",
    "      temp_bs = batch_size\n",
    "      batch_size = images.size()[0]\n",
    "      outputs, _ = net(images.view(images.size()[0], -1))\n",
    "      batch_size = temp_bs\n",
    "\n",
    "    _, predicted = outputs.sum(dim=0).max(1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "print(f\"Test Set Accuracy: {100 * correct / total}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}