{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/binary_weights/examples/binary_snn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOyd0FJMbWki"
      },
      "source": [
        "# **SNN with binary weights**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "S733HbldaB40"
      },
      "source": [
        "# Gradient-based Learning in Convolutional Spiking Neural Networks\n",
        "In this tutorial, we'll use a convolutional neural network (CNN) to classify the MNIST dataset.\n",
        "We will use the backpropagation through time (BPTT) algorithm to do so. This tutorial is largely the same as tutorial 2, just with a different network architecture to show how to integrate convolutions with snnTorch.\n",
        "\n",
        "If running in Google Colab:\n",
        "* Ensure you are connected to GPU by checking Runtime > Change runtime type > Hardware accelerator: GPU\n",
        "* Next, install the Test PyPi distribution of snnTorch by clicking into the following cell and pressing `Shift+Enter`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YgGUk8EJaB41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea0d32e-260e-4eec-d730-d1b3bf5c9902"
      },
      "source": [
        "# Install the test PyPi Distribution of snntorch\n",
        "!pip install -i https://test.pypi.org/simple/ snntorch\n",
        "# !pip uninstall snntorch\n",
        "# !pip install git+https://github.com/jeshraghian/snntorch.git@binary_weights#egg=snntorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Collecting snntorch\n",
            "  Downloading https://test-files.pythonhosted.org/packages/0a/4b/1fadcfc6be25d336d0e2a82c0e7bcebaeb029728a98594fe95ef2202bb0b/snntorch-0.0.7-py3-none-any.whl\n",
            "Installing collected packages: snntorch\n",
            "Successfully installed snntorch-0.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_UAKn7UoaB41"
      },
      "source": [
        "## 1. Setting up the Static MNIST Dataset\n",
        "### 1.1. Import packages and setup environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JGneJnSiaB42"
      },
      "source": [
        "import snntorch as snn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "PWL3FQSSaB42"
      },
      "source": [
        "### 1.2 Define network and SNN parameters\n",
        "We will use a 2conv-2MaxPool-FCN architecture for a sequence of 25 time steps.\n",
        "\n",
        "* `alpha` is the decay rate of the synaptic current of a neuron\n",
        "* `beta` is the decay rate of the membrane potential of a neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kvzf29tIaB42"
      },
      "source": [
        "# Network Architecture\n",
        "num_inputs = 28*28\n",
        "num_outputs = 10\n",
        "num_hidden = 1000\n",
        "# Training Parameters\n",
        "batch_size=128\n",
        "data_path='/data/mnist'\n",
        "\n",
        "# Temporal Dynamics\n",
        "num_steps = 25\n",
        "time_step = 1e-3\n",
        "# tau_mem = 6.5e-4\n",
        "# tau_syn = 5.5e-4\n",
        "# alpha = float(np.exp(-time_step/tau_syn))\n",
        "# beta = float(np.exp(-time_step/tau_mem))\n",
        "alpha = 0.9\n",
        "beta = 0.5\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "4_5wuLk0aB43"
      },
      "source": [
        "### 1.3 Download MNIST Dataset\n",
        "To see how to construct a validation set, refer to Tutorial 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WJO7iy-7aB43"
      },
      "source": [
        "# Define a transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6_guKA__aB44"
      },
      "source": [
        "### 1.4 Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cQzrUSKRaB44"
      },
      "source": [
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "jHBwr32UaB44"
      },
      "source": [
        "## 2. Define Network\n",
        "To define our network, we will import two functions from the `snntorch.neuron` module, which contains a series of neuron models and related functions.\n",
        "snnTorch treats neurons as activations with recurrent connections, so that it integrates smoothly with PyTorch's pre-existing layer functions.\n",
        "* `snntorch.neuron.LIF` is a simple Leaky Integrate and Fire (LIF) neuron. Specifically, it uses Stein's model which assumes instantaneous rise times for synaptic current and membrane potential.\n",
        "* `snntorch.neuron.FastSigmoidSurrogate` defines separate forward and backward functions. The forward function is a Heaviside step function for spike generation. The backward function is the derivative of a fast sigmoid function, to ensure continuous differentiability.\n",
        "FSS is mostly derived from:\n",
        "\n",
        ">Neftci, E. O., Mostafa, H., and Zenke, F. (2019) Surrogate Gradient Learning in Spiking Neural Networks. https://arxiv.org/abs/1901/09948\n",
        "\n",
        "`snn.neuron.slope` is a variable that defines the slope of the backward surrogate.\n",
        "TO-DO: Include visualisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2JKR_90daB45"
      },
      "source": [
        "spike_grad = snn.FastSigmoidSurrogate.apply\n",
        "snn.slope = 50"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LaXzsI6baB45"
      },
      "source": [
        "Now we can define our SNN. Defining an instance of `LIF` requires three arguments: 1) the surrogate spiking function, 2) $I_{syn}$ decay rate, $\\alpha$, and 3) $V_{mem}$ decay rate, $\\beta$.\n",
        "\n",
        "The LIF neuron is simply treated as a recurrent activation. It requires initialization of the post-synaptic spikes `spk1` and `spk2`, the synaptic current `syn1` and `syn2`, and the membrane potential `mem1` and `mem2`.\n",
        "\n",
        "We will use the final layer spikes and membrane for determining loss and accuracy, so we will record and return their histories in `spk3_rec` and `mem3_rec`.\n",
        "\n",
        "Keep in mind, the dataset we are using is just static MNIST. I.e., it is *not* time-varying.\n",
        "Therefore, we pass the same MNIST sample to the input at each time step.\n",
        "This is handled in the line `cur1 = F.max_pool2d(self.conv1(x), 2)`, where `x` is the same input over the whole for-loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zAnnWfGyfhkf"
      },
      "source": [
        "### Binarized Layer Modules\n",
        "``Binarize`` converts weights to {-1, 1}.\n",
        "Remove `.mul_(2).add_(1)` for {0, 1}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "blVwJakcaB46"
      },
      "source": [
        "import pdb\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def Binarize(tensor,quant_mode='det'):\n",
        "    if quant_mode=='det':\n",
        "        return tensor.sign()\n",
        "        # tmp = tensor.clone()\n",
        "        # tmp[tensor>0] = 1\n",
        "        # tmp[tensor==0] = 0\n",
        "        # tmp[tensor<0] = -1\n",
        "        # return tmp\n",
        "    else:\n",
        "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
        "\n",
        "\n",
        "class HingeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HingeLoss,self).__init__()\n",
        "        self.margin=1.0\n",
        "\n",
        "    def hinge_loss(self,input,target):\n",
        "            #import pdb; pdb.set_trace()\n",
        "            output=self.margin-input.mul(target)\n",
        "            output[output.le(0)]=0\n",
        "            return output.mean()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return self.hinge_loss(input,target)\n",
        "\n",
        "class SqrtHingeLossFunction(Function):\n",
        "    def __init__(self):\n",
        "        super(SqrtHingeLossFunction,self).__init__()\n",
        "        self.margin=1.0\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        output=self.margin-input.mul(target)\n",
        "        output[output.le(0)]=0\n",
        "        self.save_for_backward(input, target)\n",
        "        loss=output.mul(output).sum(0).sum(1).div(target.numel())\n",
        "        return loss\n",
        "\n",
        "    def backward(self,grad_output):\n",
        "       input, target = self.saved_tensors\n",
        "       output=self.margin-input.mul(target)\n",
        "       output[output.le(0)]=0\n",
        "       import pdb; pdb.set_trace()\n",
        "       grad_output.resize_as_(input).copy_(target).mul_(-2).mul_(output)\n",
        "       grad_output.mul_(output.ne(0).float())\n",
        "       grad_output.div_(input.numel())\n",
        "       return grad_output,grad_output\n",
        "\n",
        "def Quantize(tensor,quant_mode='det',  params=None, numBits=8):\n",
        "    tensor.clamp_(-2**(numBits-1),2**(numBits-1))\n",
        "    if quant_mode=='det':\n",
        "        tensor=tensor.mul(2**(numBits-1)).round().div(2**(numBits-1))\n",
        "    else:\n",
        "        tensor=tensor.mul(2**(numBits-1)).round().add(torch.rand(tensor.size()).add(-0.5)).div(2**(numBits-1))\n",
        "        quant_fixed(tensor, params)\n",
        "    return tensor\n",
        "\n",
        "# import torch.nn._functions as tnnf\n",
        "\n",
        "\n",
        "class BinarizeLinear(nn.Linear):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        if input.size(1) != 784:\n",
        "            input.data=Binarize(input.data)\n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()\n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "        out = nn.functional.linear(input, self.weight)\n",
        "        if not self.bias is None:\n",
        "            self.bias.org=self.bias.data.clone()\n",
        "            out += self.bias.view(1, -1).expand_as(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BinarizeConv2d(nn.Conv2d):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(BinarizeConv2d, self).__init__(*kargs, **kwargs)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.size(1) != 3:\n",
        "            input.data = Binarize(input.data)\n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()\n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "\n",
        "        out = nn.functional.conv2d(input, self.weight, None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "\n",
        "        if not self.bias is None:\n",
        "            self.bias.org=self.bias.data.clone()\n",
        "            out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ZNySTB6wfhkk"
      },
      "source": [
        "Network: Low precision forward pass, high precision backprop (Additional Mod in Optimization method)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZAxmcjuefhkn"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # initialize layers\n",
        "        # self.bn0 = nn.BatchNorm1d(784)\n",
        "        self.fc1 = BinarizeLinear(784, num_hidden)\n",
        "        self.lif1 = snn.Stein(alpha=alpha, beta=beta)\n",
        "\n",
        "        self.fc2 = BinarizeLinear(num_hidden, num_outputs)\n",
        "        self.lif2 = snn.Stein(alpha=alpha, beta=beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spk1, syn1, mem1 = self.lif1.init_stein(batch_size, num_hidden)\n",
        "        spk2, syn2, mem2 = self.lif2.init_stein(batch_size, num_outputs)\n",
        "\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "net = Net().to(device)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FOJTim88fhkn"
      },
      "source": [
        "Old network (if using above net, don't run the following code block)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0mLwBng9fhkn"
      },
      "source": [
        "def Binarize(tensor):\n",
        "    tensor[tensor > 0] = 1\n",
        "    tensor[tensor == 0] = 0\n",
        "    tensor[tensor < 0] = -1\n",
        "    return tensor\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#     # initialize layers\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "#         self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=12, out_channels=64, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "#         self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.fc2 = nn.Linear(64*5*5, 10, bias= False)\n",
        "#         self.lif3 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Initialize LIF state variables and spike output tensors\n",
        "#         spk1, syn1, mem1 = self.lif1.init_stein(batch_size, 12, 13, 13)\n",
        "#         spk2, syn2, mem2 = self.lif1.init_stein(batch_size, 64, 5, 5)\n",
        "#         spk3, syn3, mem3 = self.lif2.init_stein(batch_size, 10)\n",
        "\n",
        "#         spk3_rec = []\n",
        "#         mem3_rec = []\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             conv1_bin_weight = self.conv1.weight.data.clone()\n",
        "#             self.conv1.weight.data = Binarize(conv1_bin_weight)\n",
        "            \n",
        "#             # cur1 = F.max_pool2d(self.conv1(x), 2)\n",
        "#             cur1 = F.max_pool2d(F.conv2d(x, self.conv1.weight, bias=None, stride=1,\n",
        "#                                    padding=1), 2)\n",
        "            \n",
        "#             spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "\n",
        "#             conv2_bin_weight = self.conv2.weight.data.clone()\n",
        "#             self.conv2.weight.data = Binarize(conv2_bin_weight)\n",
        "\n",
        "#             # cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
        "#             cur2 = F.max_pool2d(F.conv2d(spk1, self.conv2.weight, bias=None, stride=1,\n",
        "#                                    padding=1), 2)\n",
        "            \n",
        "#             spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "\n",
        "#             fc_bin_weight = self.fc2.weight.data.clone()\n",
        "#             self.fc2.weight.data = Binarize(fc_bin_weight)\n",
        "\n",
        "#             # cur3 = self.fc2(spk2.view(batch_size, -1))\n",
        "#             cur3 = F.linear(spk2.view(batch_size, -1), self.fc2.weight)\n",
        "#             spk3, syn3, mem3 = self.lif3(cur3, syn3, mem3)\n",
        "\n",
        "#             spk3_rec.append(spk3)\n",
        "#             mem3_rec.append(mem3)\n",
        "\n",
        "#         return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "# net = Net().to(device)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden, bias=False)\n",
        "        self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs, bias=False)\n",
        "        self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spk1, syn1, mem1 = self.lif1.init_stein(batch_size, num_hidden)\n",
        "        spk2, syn2, mem2 = self.lif2.init_stein(batch_size, num_outputs)\n",
        "\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            fc1_bin_weight = self.fc1.weight.data.clone()\n",
        "            self.fc1.weight.data = Binarize(fc1_bin_weight)\n",
        "            cur1 = self.fc1(x)\n",
        "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "\n",
        "            fc2_bin_weight = self.fc2.weight.data.clone()\n",
        "            self.fc2.weight.data = Binarize(fc2_bin_weight)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "            # print(spk2.shape)\n",
        "\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "net = Net().to(device)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1CFB6LAJaB47"
      },
      "source": [
        "## 3. Training\n",
        "Time for training! Let's first define a couple of functions to print out test/train accuracy for each minibatch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7V0_NGKSaB48"
      },
      "source": [
        "def print_batch_accuracy(data, targets, train=False):\n",
        "    output, _ = net(data.view(batch_size, -1))\n",
        "    # output, _ = net(data.view(batch_size, 1, 28, 28))\n",
        "    _, am = output.sum(dim=0).max(1)\n",
        "    acc = np.mean((targets == am). detach().cpu().numpy())\n",
        "\n",
        "    if train is True:\n",
        "        print(f\"Train Set Accuracy: {acc}\")\n",
        "    else:\n",
        "        print(f\"Test Set Accuracy: {acc}\")\n",
        "\n",
        "def train_printer():\n",
        "    print(f\"Epoch {epoch}, Minibatch {minibatch_counter}\")\n",
        "    print(f\"Train Set Loss: {loss_hist[counter]}\")\n",
        "    print(f\"Test Set Loss: {test_loss_hist[counter]}\")\n",
        "    print_batch_accuracy(data_it, targets_it, train=True)\n",
        "    print_batch_accuracy(testdata_it, testtargets_it, train=False)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9uVWFKdXaB48"
      },
      "source": [
        "### 3.1 Optimizer & Loss\n",
        "We'll apply the softmax function to the membrane potentials of the output layer in calculating a negative log-likelihood loss.\n",
        "The Adam optimizer is used for weight updates.\n",
        "\n",
        "Accuracy is measured by counting the spikes of the output neurons. The neuron that fires the most frequently will be our predicted class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "I4BNFQkBaB48"
      },
      "source": [
        "lr = 2e-3\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
        "loss_fn = nn.NLLLoss()\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "0enleDrOaB49"
      },
      "source": [
        "### 3.2 Training Loop\n",
        "Now just sit back, relax, and wait for convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "JpdYo0G6fhkp"
      },
      "source": [
        "High precision BPTT training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P225z9fafhkq",
        "outputId": "d227cb78-fbb4-4d0a-8cf1-84ceca929354"
      },
      "source": [
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    new_lr = lr * (0.9 ** epoch)\n",
        "    # lr = lr\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(10):\n",
        "\n",
        "    minibatch_counter = 0\n",
        "    train_batch = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data_it, targets_it in train_batch:\n",
        "        data_it = data_it.to(device)\n",
        "        targets_it = targets_it.to(device)\n",
        "\n",
        "        output, mem_rec = net(data_it.view(batch_size, -1))\n",
        "        # output, mem_rec = net(data_it.view(batch_size, 1, 28, 28)) # [28x28] or [1x28x28]?\n",
        "        log_p_y = log_softmax_fn(mem_rec)\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "\n",
        "        # Sum loss over time steps to perform BPTT\n",
        "        for step in range(num_steps):\n",
        "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
        "\n",
        "        # adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "        # BNN OPTimization\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        for p in list(net.parameters()):\n",
        "                if hasattr(p,'org'):\n",
        "                    p.data.copy_(p.org)\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        for p in list(net.parameters()):\n",
        "                if hasattr(p,'org'):\n",
        "                    p.org.copy_(p.data.clamp_(-1,1))\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        test_data = itertools.cycle(test_loader)\n",
        "        testdata_it, testtargets_it = next(test_data)\n",
        "        testdata_it = testdata_it.to(device)\n",
        "        testtargets_it = testtargets_it.to(device)\n",
        "\n",
        "        # Test set forward pass\n",
        "        test_output, test_mem_rec = net(testdata_it.view(batch_size, -1))\n",
        "        # test_output, test_mem_rec = net(testdata_it.view(batch_size, 1, 28, 28))\n",
        "\n",
        "        # Test set loss\n",
        "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
        "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
        "        loss_val_test = loss_fn(log_p_ytest, testtargets_it)\n",
        "        test_loss_hist.append(loss_val_test.item())\n",
        "\n",
        "        # Print test/train loss/accuracy\n",
        "        if counter % 50 == 0:\n",
        "          train_printer()\n",
        "        minibatch_counter += 1\n",
        "        counter += 1\n",
        "\n",
        "loss_hist_true_grad = loss_hist\n",
        "test_loss_hist_true_grad = test_loss_hist"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 10796.68359375\n",
            "Test Set Loss: 17861.892578125\n",
            "Train Set Accuracy: 0.109375\n",
            "Test Set Accuracy: 0.15625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 50\n",
            "Train Set Loss: 317.25482177734375\n",
            "Test Set Loss: 218.36973571777344\n",
            "Train Set Accuracy: 0.3359375\n",
            "Test Set Accuracy: 0.421875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 100\n",
            "Train Set Loss: 95.0941390991211\n",
            "Test Set Loss: 61.56174087524414\n",
            "Train Set Accuracy: 0.4140625\n",
            "Test Set Accuracy: 0.4296875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 150\n",
            "Train Set Loss: 46.45771026611328\n",
            "Test Set Loss: 56.98231887817383\n",
            "Train Set Accuracy: 0.2421875\n",
            "Test Set Accuracy: 0.3125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 200\n",
            "Train Set Loss: 49.7154655456543\n",
            "Test Set Loss: 59.393150329589844\n",
            "Train Set Accuracy: 0.3125\n",
            "Test Set Accuracy: 0.21875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 250\n",
            "Train Set Loss: 52.68403625488281\n",
            "Test Set Loss: 65.54632568359375\n",
            "Train Set Accuracy: 0.1484375\n",
            "Test Set Accuracy: 0.1875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 300\n",
            "Train Set Loss: 53.00328826904297\n",
            "Test Set Loss: 50.27574157714844\n",
            "Train Set Accuracy: 0.25\n",
            "Test Set Accuracy: 0.2421875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 350\n",
            "Train Set Loss: 47.239505767822266\n",
            "Test Set Loss: 54.49031448364258\n",
            "Train Set Accuracy: 0.3125\n",
            "Test Set Accuracy: 0.1875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 400\n",
            "Train Set Loss: 51.384925842285156\n",
            "Test Set Loss: 53.370697021484375\n",
            "Train Set Accuracy: 0.21875\n",
            "Test Set Accuracy: 0.203125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 450\n",
            "Train Set Loss: 52.75345993041992\n",
            "Test Set Loss: 52.330352783203125\n",
            "Train Set Accuracy: 0.2109375\n",
            "Test Set Accuracy: 0.2109375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 32\n",
            "Train Set Loss: 50.631988525390625\n",
            "Test Set Loss: 53.5438346862793\n",
            "Train Set Accuracy: 0.2578125\n",
            "Test Set Accuracy: 0.1484375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 82\n",
            "Train Set Loss: 51.134281158447266\n",
            "Test Set Loss: 48.471351623535156\n",
            "Train Set Accuracy: 0.234375\n",
            "Test Set Accuracy: 0.2265625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 132\n",
            "Train Set Loss: 53.45356369018555\n",
            "Test Set Loss: 52.15940475463867\n",
            "Train Set Accuracy: 0.265625\n",
            "Test Set Accuracy: 0.2265625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 182\n",
            "Train Set Loss: 53.02230453491211\n",
            "Test Set Loss: 52.66652297973633\n",
            "Train Set Accuracy: 0.1640625\n",
            "Test Set Accuracy: 0.1640625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 232\n",
            "Train Set Loss: 52.164485931396484\n",
            "Test Set Loss: 53.69426727294922\n",
            "Train Set Accuracy: 0.2421875\n",
            "Test Set Accuracy: 0.1796875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 282\n",
            "Train Set Loss: 53.19879913330078\n",
            "Test Set Loss: 52.95183563232422\n",
            "Train Set Accuracy: 0.1953125\n",
            "Test Set Accuracy: 0.1484375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 332\n",
            "Train Set Loss: 53.191287994384766\n",
            "Test Set Loss: 55.696693420410156\n",
            "Train Set Accuracy: 0.1796875\n",
            "Test Set Accuracy: 0.171875\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 382\n",
            "Train Set Loss: 51.926856994628906\n",
            "Test Set Loss: 49.80576705932617\n",
            "Train Set Accuracy: 0.1640625\n",
            "Test Set Accuracy: 0.203125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 432\n",
            "Train Set Loss: 51.941776275634766\n",
            "Test Set Loss: 55.842533111572266\n",
            "Train Set Accuracy: 0.1875\n",
            "Test Set Accuracy: 0.1640625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 14\n",
            "Train Set Loss: 51.41839599609375\n",
            "Test Set Loss: 51.28276824951172\n",
            "Train Set Accuracy: 0.2265625\n",
            "Test Set Accuracy: 0.21875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 64\n",
            "Train Set Loss: 53.68316650390625\n",
            "Test Set Loss: 61.129173278808594\n",
            "Train Set Accuracy: 0.1484375\n",
            "Test Set Accuracy: 0.2578125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 114\n",
            "Train Set Loss: 50.21407699584961\n",
            "Test Set Loss: 50.162147521972656\n",
            "Train Set Accuracy: 0.2265625\n",
            "Test Set Accuracy: 0.1875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 164\n",
            "Train Set Loss: 51.50436782836914\n",
            "Test Set Loss: 50.80643081665039\n",
            "Train Set Accuracy: 0.234375\n",
            "Test Set Accuracy: 0.2421875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 214\n",
            "Train Set Loss: 48.888214111328125\n",
            "Test Set Loss: 46.814735412597656\n",
            "Train Set Accuracy: 0.25\n",
            "Test Set Accuracy: 0.2890625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 264\n",
            "Train Set Loss: 53.849430084228516\n",
            "Test Set Loss: 50.931732177734375\n",
            "Train Set Accuracy: 0.140625\n",
            "Test Set Accuracy: 0.2265625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 314\n",
            "Train Set Loss: 53.069007873535156\n",
            "Test Set Loss: 53.05659103393555\n",
            "Train Set Accuracy: 0.171875\n",
            "Test Set Accuracy: 0.140625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 364\n",
            "Train Set Loss: 54.26735305786133\n",
            "Test Set Loss: 53.31357192993164\n",
            "Train Set Accuracy: 0.1796875\n",
            "Test Set Accuracy: 0.234375\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 414\n",
            "Train Set Loss: 49.8393440246582\n",
            "Test Set Loss: 49.72377014160156\n",
            "Train Set Accuracy: 0.265625\n",
            "Test Set Accuracy: 0.2109375\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 464\n",
            "Train Set Loss: 47.09468078613281\n",
            "Test Set Loss: 49.701847076416016\n",
            "Train Set Accuracy: 0.25\n",
            "Test Set Accuracy: 0.1640625\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 46\n",
            "Train Set Loss: 48.741119384765625\n",
            "Test Set Loss: 49.82305145263672\n",
            "Train Set Accuracy: 0.1953125\n",
            "Test Set Accuracy: 0.21875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 96\n",
            "Train Set Loss: 50.702293395996094\n",
            "Test Set Loss: 50.7263298034668\n",
            "Train Set Accuracy: 0.203125\n",
            "Test Set Accuracy: 0.2109375\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-56fe51c0439e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Test set forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mem_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata_it\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# test_output, test_mem_rec = net(testdata_it.view(batch_size, 1, 28, 28))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-ac77efd2a82a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mcur1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mspk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlif1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mcur2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mspk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlif2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/snntorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, syn, mem)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mspk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0msyn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/snntorch/__init__.py\u001b[0m in \u001b[0;36mfire\u001b[0;34m(self, mem)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmem_shift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mspk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspike_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mspk_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmem_shift\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/snntorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input_)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "g5y4NZAgaB49",
        "outputId": "68e0e0ec-0a40-4682-c014-a17fc8258d23"
      },
      "source": [
        "Old Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Ghbg4Ga2fhkq"
      },
      "source": [
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(4):\n",
        "\n",
        "    minibatch_counter = 0\n",
        "    train_batch = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data_it, targets_it in train_batch:\n",
        "        data_it = data_it.to(device)\n",
        "        targets_it = targets_it.to(device)\n",
        "\n",
        "        output, mem_rec = net(data_it.view(batch_size, -1))\n",
        "        # output, mem_rec = net(data_it.view(batch_size, 1, 28, 28)) # [28x28] or [1x28x28]?\n",
        "        log_p_y = log_softmax_fn(mem_rec)\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "\n",
        "        # Sum loss over time steps to perform BPTT\n",
        "        for step in range(num_steps):\n",
        "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
        "\n",
        "        # Gradient calculation\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward(retain_graph=True)\n",
        "\n",
        "        # Weight Update\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        test_data = itertools.cycle(test_loader)\n",
        "        testdata_it, testtargets_it = next(test_data)\n",
        "        testdata_it = testdata_it.to(device)\n",
        "        testtargets_it = testtargets_it.to(device)\n",
        "\n",
        "        # Test set forward pass\n",
        "        test_output, test_mem_rec = net(testdata_it.view(batch_size, -1))\n",
        "        # test_output, test_mem_rec = net(testdata_it.view(batch_size, 1, 28, 28))\n",
        "\n",
        "        # Test set loss\n",
        "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
        "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
        "        loss_val_test = loss_fn(log_p_ytest, testtargets_it)\n",
        "        test_loss_hist.append(loss_val_test.item())\n",
        "\n",
        "        # Print test/train loss/accuracy\n",
        "        if counter % 50 == 0:\n",
        "          train_printer()\n",
        "        minibatch_counter += 1\n",
        "        counter += 1\n",
        "\n",
        "loss_hist_true_grad = loss_hist\n",
        "test_loss_hist_true_grad = test_loss_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "DpTsZZLmaB4-"
      },
      "source": [
        "## 4. Results\n",
        "### 4.1 Plot Training/Test Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Yw9hpEi7aB4-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "ab006a6b-5343-46d9-d4c0-b92241d63382"
      },
      "source": [
        "# Plot Loss\n",
        "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
        "plt.plot(loss_hist)\n",
        "plt.plot(test_loss_hist)\n",
        "plt.legend([\"Test Loss\", \"Train Loss\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6ww2lt6paB4_"
      },
      "source": [
        "### 4.2 Test Set Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1KZ7WVqhaB4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71413ab5-f5a3-4e24-8232-944eae26cbdf"
      },
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "  net.eval()\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # If current batch matches batch_size, just do the usual thing\n",
        "    if images.size()[0] == batch_size:\n",
        "      outputs, _ = net(images.view(batch_size, -1))\n",
        "      # outputs, _ = net(images.view(batch_size, 1, 28, 28))\n",
        "\n",
        "    # If current batch does not match batch_size (e.g., is the final minibatch),\n",
        "    # modify batch_size in a temp variable and restore it at the end of the else block\n",
        "    else:\n",
        "      temp_bs = batch_size\n",
        "      batch_size = images.size()[0]\n",
        "      outputs, _ = net(images.view(images.size()[0], -1))\n",
        "      # outputs, _ = net(images.view(images.size()[0], 1, 28, 28))\n",
        "      batch_size = temp_bs\n",
        "\n",
        "    _, predicted = outputs.sum(dim=0).max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
        "print(f\"Test Set Accuracy: {100 * correct / total}%\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total correctly classified test set images: 7358/10000\n",
            "Test Set Accuracy: 73.58%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "hDQg3UsUaB4_"
      },
      "source": [
        "Voila! That's it for static MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Rh7ZwFs4aB4_"
      },
      "source": [
        "## 5. Spiking MNIST\n",
        "Part of the appeal of SNNs is their ability to handle time-varying spiking data. So let's use rate-coding to convert MNIST into spiking MNIST using the `spikegen` module in Tutorial 1, and train our network with that instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DYGblNEgaB5A"
      },
      "source": [
        "from snntorch import spikegen\n",
        "\n",
        "# MNIST to spiking-MNIST\n",
        "spike_data, spike_targets = spikegen.rate(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps,\n",
        "                                                      gain=1, offset=0, convert_targets=False, temporal_targets=False)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "xgRdm4jRaB5A"
      },
      "source": [
        "### 5.1 Visualiser\n",
        "Just so you're damn sure it's a spiking input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "geVqzyksaB5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e25269-67d2-4b41-efe1-659c860ddb6e"
      },
      "source": [
        "!pip install celluloid # animating matplotlib plots made easy"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting celluloid\n",
            "  Downloading https://files.pythonhosted.org/packages/60/a7/7fbe80721c6f1b7370c4e50c77abe31b4d5cfeb58873d4d32f48ae5a0bae/celluloid-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from celluloid) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->celluloid) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->celluloid) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->celluloid) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->celluloid) (1.19.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->celluloid) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->celluloid) (1.15.0)\n",
            "Installing collected packages: celluloid\n",
            "Successfully installed celluloid-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PbLMA0pgaB5A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "outputId": "46453b05-cb38-449b-f2e6-0005f308541d"
      },
      "source": [
        "from celluloid import Camera\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Animator\n",
        "spike_data_sample = spike_data[:, 0, 0].cpu()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "camera = Camera(fig)\n",
        "plt.axis('off')\n",
        "\n",
        "for step in range(num_steps):\n",
        "    im = ax.imshow(spike_data_sample[step, :, :], cmap='plasma')\n",
        "    camera.snap()\n",
        "\n",
        "# interval=40 specifies 40ms delay between frames\n",
        "a = camera.animate(interval=40)\n",
        "HTML(a.to_html5_video())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video width=\"432\" height=\"288\" controls autoplay loop>\n",
              "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAmnm1kYXQAAAKuBgX//6rcRem9\n",
              "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTUyIHIyODU0IGU5YTU5MDMgLSBILjI2NC9NUEVHLTQg\n",
              "QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE3IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n",
              "eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n",
              "MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n",
              "PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n",
              "b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MyBsb29r\n",
              "YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n",
              "ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n",
              "bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n",
              "aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVz\n",
              "aD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBx\n",
              "cG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAXkZYiE\n",
              "ADv//vdOvwKbRZdqA5JXCvbKpCZZuVJrAfKmAAADAAARsG58lw15q9IkAACtobhz+EtXgAGbPfnA\n",
              "6vBq2y4KD1VFSYr8QiGU1RZ6il6SV2xj+fxPL8Qg2QVKmNew+PYqvakP5XzlvzKFuFPko/2sMpj8\n",
              "1HMKuFMDNP6BQzOTFADkOh9E7gFlDtmhQviIFG3v81CsTQB6eSs3/v3ytq7PgKKa2iYruIdu5PZ2\n",
              "c920Bugn1asFHvo2jGl4Xbp5OOmGBa5G9rm4xOa3783vdsnjhjSBclIlanR75DRjaErwyt5LdNWN\n",
              "4AYV0PdZmyD9RtMN3bCFI9QqFD9eyD+Qp6F34MykJQR5fxgPLp1nqFXpIIQoNDcWKTTAK0IklyCA\n",
              "Qqtd/0iiZ84OLtPzqDxMV78HtbKb7Oi4pwmvZ/L4gV9qYnqAmn9L+hlCNaEK5h0+9iK0jywufMET\n",
              "n6KBDCFc8fYpxuPEQoW7DhiVp6Zx/+ml4YCyq6KbYfxTQQgLVS7vaI6SQ1k0O6WZGznmwk0jRwMQ\n",
              "0sbPE2P9CUg33wz5vGnJnTFYu01g+h1ZJyjScm2q2NPrpbvz/Wi7dPBh/f7IVqTILvUbKCfRNSq1\n",
              "Ua7XOGoaCALXnwvk4jTI/38fqVxBUHN3UBGmUjThCw9GwOghx/2gAqNIERXQjqhyz1lM63im7SOw\n",
              "89FTCiqbq5CRVh2L5HC9qgqONyc29U3OkdPXQcDe9PfnG+Jii8hgJQvtp5/xb2goY7A8utsWn67L\n",
              "fhmWXsGsDfX9lNWysAGV/8eVo6NUQYu477DF9rkuBOpAMn0ATrCkX5mOEVNtG9L0Z80BDuXh/9YM\n",
              "+z/toym+IyMOD9X+k5fiu/d4+oCDyuukle0k1IBnk8VutSZ3ctdwnYT4yLlnDa7DIAEndx6chcZK\n",
              "SviKkVo7mNIdSZycFCVrpFFwhA/7YYgfgb0LaRNmAUY56/YI8U/HrwAcdgRutZHJdbNfWBkR8UzO\n",
              "Zz7VIbcK++E58P6UaLnB9+SykD5LkX2v6m6cXjrgjWujIyKBic2K6yO9HA4gp/9fYBMzZVtjB+1c\n",
              "UVua3fht4tq6mgzIAIXD/WdRpnzeekK4Abubf+cRIrU6ZNkaK+SdR+JMJnq4FunK92FAD/hitpaL\n",
              "5weivkbgoIMxtWWomPsdD9MKr+gzBmvJykw1ZNDYhefIddGSR/zqXXHGv8pkbLu2buctC2rJ8CGA\n",
              "dlB6mm/+P37xVKMfiGzynnUHkn1TxM+s3cjNWdKow82QUB68utPpXE8MSbTNKU8m405CS4+wqDEN\n",
              "Ng5AzVLWBUzFcs8PcLXb/ma2x35k3VXjtU8kLsyaITjGESmsxXT7sPwjutAj6COAPVB8U4P+bB4j\n",
              "DHdkWmtfyXx2csNU4KxPt4K41mI7yWAo5aQqxq3fZSo4U4DASq/Nifr6BSCge6hR3a227MfytKpS\n",
              "KuQLwK7a0vo0emgBqMuH/TZd75ukGoai5uQ0hp7fZVGh6luWOtT+rck0sX7gBooYY3YT1p0+C/sp\n",
              "AZCmfIjf1jH0ryIXZ5al4Aiw0ewb3Y2DT2xcA14qKHFoCIu9q1UfD1tcPZdFF6BAS5DqbpKL50NC\n",
              "rLvzqzkrayYdxrkoKbGT+FIKVplw9yRH25h78HbPJckQOxblEy2/tvyZeF11lWM37XTT0I1zBIys\n",
              "8yaNFzuNDhygoC27ilsqTu486wrp1/bbm5ZJsmWoA73CZKtVcEDw2eKnQ8QRkdFy3EV50vnpRMoo\n",
              "+R9UgS12GUDaPnGAhUqeP8L1litg4Z+W4R08ktqVPO3lbzgZ5j9d5pvcfJgndV5+JLR8LtZFE0yM\n",
              "7Lcdce3CAzvFGwrcDeUp2bjooBt94POdGzzb8VtkNaDQZu/9hnibPGEwyDvf+XNdMyhIyoGcQpcn\n",
              "EJmldW6QlRsvr0xcWSMO7EmLDAP+r+vfK9KI0pYIX5dr0DTnxG3TBDO79XSuunZiRsIh8I+HvbJn\n",
              "c7iB/vunIoFF9C9YyyALYAABe6C+zHkAAAJiQZokbEO//qmWACuA3OOP1mDKHno6QVvAjg7oM652\n",
              "2bJTL9H0RedSbvMp3OK6XARMjD6z2tbqoGNZGxSZVT3+cHw4lH8q9qf0y8uInQhMEAULEROXpEDg\n",
              "RGmGcCfq+fJgYiLkSN1X5zA4DN6gLpVNpaPmCnrjEBGWXS8Lhc3rdPCq9LN9D/uux8SwPIOKwq7D\n",
              "EajDd7KC3Zj+NbRRlKLhxifttwSshANxAjYovusS84sHCJrv8SBfkhH91MTqHKsHJyEAPAOSsyKM\n",
              "7UwwsF06EEOntYRG/ehxPEFhWVq9roGUkRnxjmhpJayfS1q1EQiJfzsN+xI/Ai6Okf0TcWrBk/Ye\n",
              "fl1R/Q03jBqfO30ko2xfJ32vHQTK1XIvsn73g5jqIc4JVpvJb3FlmFEzXNYLAzkPCZuDruIVIVss\n",
              "BsZ+17o3WWuq9kdUGYdSrKrT0heGxB6DC8I9RqR8f7vP4A1Sg6QU2xJEYKkjrzwscpfEGU6ABTkJ\n",
              "JxEotvDMIks2IccrSt5jlST0BAJorpnEGSEK9EhS8IB+gVvyHjgnVmS1gc8NdBln7pfOCf0dszEp\n",
              "wfCv7yRQvHLk3MEmK38tc5txAH+Dzt7bx1IpwghXJcBbMA3AUAgJBoSkQRpkouVGLUbKG58g91JV\n",
              "isw6dB3NIqZEBFz/zTwEO67xWPM5S+0Ll4lKW8+2qBg1+F0Jx5LZL3e755aSsBGeOTIqitsRPYei\n",
              "tgnD8fJ8/P8g1/N3j0tGpJzrt7jcw90Bexd+8o6UHjm0UEGJd4JXnciIYhTZ0bZX+GQ/o6cGpWDn\n",
              "pnqs7oSMOscgoAAAAZlBnkJ4hf8AM559EFmzQDaboXxlbXBOADz+Umyqyb5X7/dFQGgUV5NTOqop\n",
              "cUox48FdeDaTUtwOuJo3PQCQNlbo6ik44ZuehXfqS87CFfkRRp2q7ePJVgKrGf+sdtWXkFhQ7pH0\n",
              "ucNtsWqCiaSz7gf6ngK4vRnPVfuqg6F85GZIoTvBiLbVp3RpzCtYto9K0YdqbnhTwPu/D4+4QNUJ\n",
              "hj34oPB2tS1gPv75UCq+CpPyGNMhhiC8IzvKovtzSbaQKnoaXJ0ahqbCXZCQAEDrVlatYWqBcxwk\n",
              "t2jFbf1urWyp2Aun3+DJdi4lK6IndBOKBY3HBS8osAfsM+NFW0x73MInT1W10AhLxpgtLEFI5Mrv\n",
              "uvuihi8VzIs7sgSYbWBOThZAqCRuE6sOjmTxmaVzbgTLdnFRDbZrV98uFQxggQNPjDQMsvHeFZZg\n",
              "7zOAJBHVeVFpFibYVwFzOJFi7jJCEG0FI9Uo0lNI+Gmlf4i5U8pyzlFJppr6hqo12FbrFRAiiGA2\n",
              "SR76kx+5/UMK+MKt8/2acP1FAR/bcE3ZAAABNgGeYXRCvwA/kQQDk9ogILeIHj3bGNxoAOU8lc7i\n",
              "7XVKtCPErDLDDPa32sf754jHIVtiPMHP5ZcDYT9NPeR+3dT3X5Vq+1L7+Vw2S+Z8XJDQw57eA4Y4\n",
              "h95xsCjcT+44YIuPcTzvF0X+ZS3KFIpMZurcwljQ0/Ayv8XTTzRPy6Jw4ktvL1swHGyLsFAARoj/\n",
              "a/ye3BHXbUwsNfpwK2FttAZyHu628UHP3yemdsCWzCDrqN4yjDTnxwAjcg8aFeN0cpZjEU6k1xnA\n",
              "VOlWfoFQTpTpdh3+xY0T1R7QQjVlF+tQd96asNowo1m6suxJm80xClILTYv8oCTEJ38nnDWFJuz/\n",
              "tLhgB2x25nu0H3CHX/CqqAHL1j8jsBVUf/gfaSbK9kc+xGeGLavIgA1mxOJv24DtUs8skaAAAAEi\n",
              "AZ5jakK/AEWWNUTmP8A2zRsUMrJGwDcvxu232UEVQAhkM8pwdlCPuas03vnxKzrsKCUuSqz+JzL/\n",
              "Ry0qGq7BcenKXqLP6spQN+o47qwNl//Doqh7ZmEzs9I7TkvmlpaxnHZIozjr24eBs9o9bHun+YL5\n",
              "O1xtKbNkTv/qgTSvyeIVizLGnP2MGmsqL9tpFhCXAxKSgcdfyP4+vPLaJ+/NR+Z8AIW0BN8CPNgx\n",
              "1CCAIQKbxf18XQUr1FE43MVO/1o54iXAkLZt6Z/sItIZTd3n002dTX6qpfodOVrku8cICQGnqkpv\n",
              "Y/IHP/WwhASUiV1fjr+jKnG06CMNM36VoWUypCnJq2t85KP9GH2pwBXhWiAnRgvbjcUNyzxmiCoX\n",
              "6nElvmEAAAGxQZpoSahBaJlMCHf//qmWACu/AoSvrlLAS1f5UsSFq61o66AQj6+5qEdKcO5i0tga\n",
              "zqZ44gC/hPd1YaF/eJAgAT1kCKg2aeEH0CzFq6NJup92lsTg8dXS73iZJAGSLQzuSMcofzeHm5t5\n",
              "E/Zg/S7tCX5h3fzQO24MlDS2RW0kXR/b+9gBgFlctd5iF6ZtwgdOlaY3l5V++8s0QEqfLL16TbH/\n",
              "8IkSKI0t7+PZRPcRH7XKtROBRYvq75Vc+Ggr5hVOsFfax0O8heV8fjLPBw9IB0m93lQ6fYHfRXK3\n",
              "wuSxAKLCSxK1zM2sHLY/cTUc1D1nx3Je9z7VVwoBLJ1WdjyQ4dqaqzRKzL+IfdVBd8VrP8QuSasZ\n",
              "9NG9bm3Uj+WvJyuwMf6gmepjM5kqG6OPeN6ztEtlK8OY7hiF5AbLK1kVLn9dZwhD7qSIfVC1ezss\n",
              "BttUtqEu5XxArhGimewBdZbwnlBL7rBjE9N4DoyAoYkwFqVoUzONyJlT88eTYxCNzlR0X4Qg4pAl\n",
              "IGqG9OieD4m2IeLetiWeRVxf9M7covag7LGpTMqJqJ0kK0naqPP00BCrZQAAAVhBnoZFESwv/wAz\n",
              "nn0QV9uFDpIhe489MUeYGzvdSLHfYjXd4rZSB6o6wAXru0VQzv33zqZD92zWgKBfBtEpvM8Q9R/s\n",
              "h8LISQuBwHN6irLWbKgNk7MAoP9N9ia0YJPtxn0CxbF08GrlD6MxWHjr2/r9AJKJ+xFzWnt0ZljX\n",
              "8ZIfd0u3FtLFuTqeWZ9LG2OJU8bynONUPHKmfXOBwa9HjQh3UG0xXo9HlGw1DL5blmV0nGOir6wV\n",
              "jRg4SNPyhiqHBETNGIv9M2HHkv8Nj6iPir79u22HZxWRORQ0uE+hrBqpoULoIAC6sF2c2jesATtr\n",
              "o7C1G40XGZdvBGbnu1A9YHiTFQXtE1WYO7xTlHjfzF1tOB5XXOmo5DhVHI7cmnKYsuVTwGwL1t2N\n",
              "RPMPRnSMrF9wKlcDpFhABxLUt0DlmMLOl8/aBrowylFM4Fs9wU9tA/K3ndJT9nXKIQAAAPQBnqV0\n",
              "Qr8APOFM6uKeyBwRpwzNaR4dr0tE6EcDIAOtAJWXKfYXH8v7ro5j1K5a/yj7R8qsM29XW4Csu1A6\n",
              "CGB5DRSKC/uHLAen7O4daDHsiLWXzR/VzMkpt/+5nCtDJe18E7JFF+VR7p96Sc42Ytl1+2Sz0hQB\n",
              "SCNWd9X2ajg9mi22XAwpEbKHM6MM9+Ph/ZNvAXczzOrlG7/Pin2QdADZi+UUKG4s6REkdKteu+cH\n",
              "juuoXHIFC4pZshQefW7r93Gx4A2efnNbRJGGLuX9SXZcHgqOrydBw2uxbFZ9wjKjsG5suDWxnZ/T\n",
              "67GFQwfQ7XqXGpNBAAABJgGep2pCvwBDk7eKnYwGX3wzd+1XRAL+09JWoOEeEQmYO75VIN6TOQ1d\n",
              "vxvLEwuVxnI3FbODbOrHgwr/aG4xD/yIOTsbBfP2HACGLJczp9ykZX3j6UmAym8Ss/mAIvb3fbnD\n",
              "SbsP3v+tdGiYm321cXrrwOqZTI91m8SDXbcrt8myroLRvIDvG8xpfeYqeiSzC1j8TzQWtl9pqMvW\n",
              "9H4OIkEzDGpIlaUaCBWYY6S25wiYcQVbS7KYX8UytwS94NGtnVXhlPS1CwbuomfQOBs1Ruksbfwx\n",
              "1WHn9RHQXk6mjQnO/zXFZRFBpFvxfdaM1nCmOMTI1iHi+vvZzyWiRE+i+DWbJsJd2m5gFvRAzEAi\n",
              "3sT+qPRtWP91kefLyga27mMj6f5PhKmfgAAAAmtBmqxJqEFsmUwId//+qZYAK5A3v70pxgE/lrD8\n",
              "DaQDib0PcXlKVgGMt+0FMh2/BU5TRQ8erx0g2Uym41eKWkGtkPm34M+guBMPplQb4x9maQEDMR5Z\n",
              "MxnhNkDzdg4d+gnh8lPDqcJFsbTujqmR4SsVFSOFrPQouKfeRJ90CqKghU0CkApLGeX4F/EHJEl3\n",
              "/ne6GLN/lOgAsKlb6qYzxbze0xPKaV+Jlw9bSgTv6gw24E6YLIod2eV6zdwaMN2njtmJXzq4UJJH\n",
              "3Su8OfqwBhOhGXqBez52d1t7w8sVs9kMS7Ma9t7wltJZopfOkMUQf9roGwU6kCrwwMMk3hIZBNI/\n",
              "4RXTkf1I4iVccihspDAntwHBSuTNklB9aU1Q/nqKDWhEKFLImWiYCmwD4GW/xNR0ijcNtrvGsAZV\n",
              "AhM3oUVdsepkHRxUV1BoTqnA7/JKnHwqeLJoEaWD8rEVYBCJIpBeCCO69Hvc49yCmOuhM05RjtRZ\n",
              "BjDBRcHJOeWpGvUMrrVWkXQ9gUYBt1DYuJgCmVjF5iRoLXTavT5A5ajKBT56ZEjBjwEvbLsKFvkN\n",
              "lVr8WimYNMGo2QfGcGylNy6k/waYssdnAbvKqc7wHmVw9n3XmQm7Byy7ThyD/UO/iYICfVFGiikN\n",
              "VxLd/8qRgyglQqpRvigMVDdiyw9boWwBLOielh6XlZa9ho06tuZgFgnIWigv4++jXIZ1/8DbT3YV\n",
              "ni4GmHp5jQNJi5cOw2PNvWJZPoYvWZSRqCj8RfbOqhJWE1KYpmoBG8bjLA8eRCw/6VM4QPKNwPj0\n",
              "FjvIauoO/6fev9qomIvxhWyHZfxAAAAA1kGeykUVLC//ADOgcJ1fY2DIPFzxnyLLL5kXLL9CuMZ5\n",
              "CTddE+mGPAAtFrhMexVGwcqWKvLvU8gsAdfgj8REewwjX8kdnA5hjbmFfcZkvzZrEn2nxFG9XPMD\n",
              "y1qS0dL9oyZlWX8fJfyOCTT6WtaFaTQxHneH+Tkf3+tpqMrcCGmaz5/4KGfb3AJ+P1NjtKI/LVHQ\n",
              "R7WBF2icLl79ImylCCNUUnynavcQr6s8xP+cTHeIUHopaQWxx1qeBHuLoHhnfOjrezQKRFbPM6ik\n",
              "J466GgL1ziW+D98AAACSAZ7pdEK/AEFc466SOIYp+vDTJWIObDYgAhvVHS+KPyOVeYWdoOOnl5LD\n",
              "pmv0zPDTqz/tdTmD8ZulvqBqk16rlP4kWQhqdQauloYWDT9Mg+7vI4mEfyi+NQmd4nHv7A+yM6KH\n",
              "KdpyOxofj1Sf/G68lUkYX2YMVTwUqsDZaNFvxMnRPHlxntMVPeYF3EsherxJrDsAAAEEAZ7rakK/\n",
              "AD5EbAoc6buJu8+2ba/nyvUY/u0fQlv2QJphS5eQ2nKfNSlotD/8Hcqn57Nu7RnAVWtFS67WRHM/\n",
              "tebdahaJOmOYkjW9AAa8D5b7TJY3FVAH3srptqxEO8IICsoF2R2vxavbKK6paFIXq60tRc/egrMF\n",
              "ROlyKn5t3if3M0a+s2miNk6RaPcvDgoMdHH6Cn2O3+q/Kp/k1iJuqZomX93QT31W0+12SHwB9c8/\n",
              "W3tTojPZNU3qs+u4K62UajmNYRhEp49ilotjB5hc6bDcjqpfZpQnEGbCABspv20bUORcPq3msp1F\n",
              "wFxoDJuZAmut7651/y80H3xaAxREwouJcWAAAAD/QZrwSahBbJlMCG///qeEAFZBDKqw/chqjMd6\n",
              "rxAK/E8nNIIKtTlRCi5eb5/nZCltadg4VHuEIUvGuBt2Nw/tpHZ+LVf4HI8UnwOeq/zt3DFaM/4m\n",
              "oLBJoPg1JyKqJr4lb0A+TsbAfROb9IlG3BR5NKQxLiv8Px3swIbrKe04AHcI3oeRi2ht2K1zccgp\n",
              "PHQA+IUVn3xwiELea1tj4nL2UuW47IvSn8pfHSpCDLr1U9QDaT2bf/GgC6XoHSeoibBnR7hD48x0\n",
              "eV+h3gyNtCXPiqLUuM0X7cKiT36KlGxIBXuPFnkKJMM3Fj+Yat68rOozMfL9FmN5LAykuCdWPEC5\n",
              "AAAAzUGfDkUVLC//ADOevZqoYS0Q5NglTnEEAYFRL8k+KVrnI6lHRpMkBWm5XJboSCGtSK/2hoBo\n",
              "8RO143P8af9gURJusOhv/XCGw16IUw9BcmQwDkyN2ki7lXdF7WzFG51nNdc4G5PsHIsqW045Xvew\n",
              "zwRZloFu014CLCqHQsN5EEUsQdRIu8vdtfyPCxAvkWa8t/9Hz4UXTzZsFfmdGwzS3TdffHvXPbBL\n",
              "7+jxFe/GsT+si/NnwiGDWh4+s/kwsbqkt8g8d51KKxoZP9BeOlEAAADWAZ8tdEK/ADzhGkxgKOEw\n",
              "4hqPTgGweQLMkcUfhoSwAHWgErQbFdeBJknJas9JuSVsHG1NOCPfG5lVJUeoB//4POTQ+7uvbitp\n",
              "A1Nbq1JPVWnc/sHSRPJ3ePxFyUXDttqIW9DvkvH8cr6zEZF6D+Wx8cofhcsS5ICkyTD0a40+dlDT\n",
              "LG/9LpUDQB6YtS9ybB4h5PEbmUSNiSiAZz4OO1b1jAkUV0LFT/4fN8DqcZGUnBajH12x3y0j9IKy\n",
              "EkCUk0f/Qm484hXbQkPV5dKX1qMfoJUXqOdAlwAAAVABny9qQr8ARZY0yDU0EtmOlA+e1B7nmA7H\n",
              "QronQjgfAByoBKy5T7C4/l/ddHRLA6caovhEG5Wke1qpBEcgqDhhn/wdymC10jSy5M6K6oBWQHGI\n",
              "uFQ9pK6ddLtouzYRP9ClVtppAFOfxVg1ay9WjRSNO6pPVFr8D4Cbs6qYTViQphAm2LsLAP5VthNX\n",
              "4wVfiyXSjhG3zsQdeTGX9ooudjPRkjShTYzEhOiyVQt4/szA6/xRvuuLB+wXdRZQJowcGLnVWv+q\n",
              "zziRgiEWdhfQOwwAZKl7clpHShY2LI4v+4zVNr1Qz67qO7XzPYYXPLMVov3wgCZd8MIvMYFRCNdj\n",
              "7RoWWUjy51jzRo1cRrdrVA1Xod/mXJwAP+qXR5rooCIXA3sYlpk/wXGaOVqdLpdh4uiLVrZb4ENA\n",
              "76zjoj9gM30chttKnizhO4v94FW6oNpWkX4AAAFbQZszSahBbJlMCG///qeEAFa94VZ/PrCbl40Y\n",
              "7FRinAm+OXAr55CnACaqRdlxJcLISt0pLL+B/ST7Kt9cvQMKtNIvvH1MsbQdj3KmKtEV4d/Pis+e\n",
              "QND8cv/BV7a/hDlIf8Lp04vrkvnJLnjdyzb/wZYpQPjw9KXFboMIxRyPylP/nINTC3jZzgG0j5oF\n",
              "noVRLYJin+R9G07m2J4NFAJW7QNtVFJ1ZxPTfGzA2SkIE5Z3Ido6BArkWubgUVf6lXFG3GLLzumH\n",
              "vsS4nhJU3Vwy8rXrpkDrqAeWfYc/HwJlsJSPYo4X9h9OgA/39uyXFo/A4aaQO2BSgf7AR+e0zgXJ\n",
              "1pr6p1WLOybqzVnuBSbsax+sslTdYrdO/NEoc/8MlCNh9unzsrNtJyTuxENNn8dRz9D3ARJbTnaT\n",
              "l2M3rggyB1nlcwaQlo+PkU+6Dmh8BUuw21qJgtO54Qwxbg4AAAELQZ9RRRUsK/8ARZZ/UsaSGVeJ\n",
              "0oIwN/jxYecPKbjcdx/j9SgHLFY+ds6NwAgKlE5l/o5aVDVdhavdruvSD7IV+04WU9iQEmtX/8IG\n",
              "QvikkJHFqlv6ybjJJGcsGShoYMXw1Rp053x3/blyBkr6LF15Tlju5LG3tnxu5QON4e/2YK3kWmte\n",
              "OaeXIQQzfkIoP+rM4fv9ItPrcVlGfa9C6v1vJ1EWwRzdzCNVqh8uxaOGrZ3vy7XELLjESjofMKV1\n",
              "A/368UfygHPcVY3u6aGH/6Aiyb1jwlrCzPvTr5BWe7p+kdOkF3aTSuFfWB0zBegLBdSpHx3U/fUt\n",
              "6XSZRrYaEpRR8gziKsavFazqQuSbAAABLAGfcmpCvwBFloBjXPO13kQJ3oY7IpQnLrN9mUlkgfKR\n",
              "b36YI2e5S6enB9eUkd3qILWoi+cQAcy703iJjs7XP2QV3uY44mKHb2mjq4PiyTLV9ZwYl6IIlr45\n",
              "IXcbku8bN1wBtuUbAJSUz2FehHFy9xXWd6SKD5HaOoB0miJK5KZp4/Mb9bciltDgxNmUcav+ET6k\n",
              "upx17wOfzMXuZw4jRfU+8P4d+dO8mYborXWrvPcreSMU6lvEdqJ+IMiNTAn1MuTn27y3CJPgpqL3\n",
              "8u+LZ3oPM8LJloNEEdEVt1ELWjmeNOPA8s7JAHjRQr/qqZ1YLyy/ws9ne43bGD7L1nx0LjegpvqA\n",
              "aMVR0GXNTzsLrpuyqiHldSP+VjhqngOmG7c0Csn8Yq+Vr+J+04zmmAAAAQdBm3dJqEFsmUwIX//+\n",
              "jLABKBxuBUlR+4ZWTOGSaOe6zDygeyX1rR1Mh+7ZrQKj/2D7BNroIot4wfOgrlLTSf0KvfBoKCtc\n",
              "kV4NRLHoQFZ1weFybwDI/eme77qJa3Meb8tLfbqNLOWgi1+mtgKnnvAAtg32Sytlc65RUp/V0TIE\n",
              "7y7bJpANf1QEqA+xSwTcpG+TvxVsNxb+N+iti0lEAoC/ZSt8zAcF+CjYYdPnqm6+851dnUkdoC+v\n",
              "tkvtr4a0xWnop/jF0pn+qWHa218r6GXupI84bLvb8nTDVRereHtkdMWtk+5WAcCYlSpe5syQgp2X\n",
              "H+g6yARlTko4jvMWn/gv8trOITHVQAAAANFBn5VFFSwv/wAuphB1+cEduBds2LSTWN4Lr7tatCvj\n",
              "qlIgi7f5+SHiDvVbpzBz2+NvAppJpEGdFYwnRHsU6/Dqxm+N/asdikVWf8nKGPkPhhhZtaXyzXek\n",
              "UKigap4SeHAw7RaXPqC1j9Bs9GTNsQSy3Ij87EGnjVxgyMMDOTYL21uqIIxiGJj0OpsgvGrbobUw\n",
              "dUmu1mDsSBArmmIWrnczFBujhj/CIndXn7VAjVo8TmL7bpUSevWUuuuJi8xdbETiReOQ19TcLve2\n",
              "eKLsS9c1OQAAASoBn7R0Qr8APjFBCezY1pJ7u01OLgbEzkoagOZq9egXGmvWq0AJZfRd+BlP5Gs2\n",
              "xMYhqxutqxOs9N4VcJVcxFe7JAo27SXOVC4eXweuMmD6Tz+8rjf2ZwtWFiWPLAuq1Jmujkjtuh8e\n",
              "1/sHX+Mgs8r95XMiiq4v6TQRnJb9qJMImqU1hybcrDu7XEfJJBnMW5wXqLjXJ0hJkvjQ5SsQTclP\n",
              "VGbDYkzMZlFDkiWKHkIOi4PoKtK/vjJx7YIJ1HYSKsy7OCToqCRpZFZqoV5XE13sxshwozZftf3B\n",
              "ZAlpTKhJd10maeKzhmUuRBvjb+kZjHnI0+ScFbm5j8L0/RV7SYRsVzAU+RoG5kybV+Q0jHaP49ln\n",
              "ASzj+OfgJwSlWsAr67wrgkCj+R1vbzGAAAABewGftmpCvwA+ILsJ7Ni+KUe94dlJSgdYkrWU+S1v\n",
              "UZdInwIK+/qQ6RZ0AfXU8uPTry8B+jirwU1V7eCHO0HHa7sJTbV2jvQFQ734W+PpQalaNaUoUooI\n",
              "z22no7lKqeyVY5QKFPDTsVHEsKoEkg21nq25Dt78C2TLPkGSTPcSlnPbvSb/aKkkT9hi7e8iaLQG\n",
              "AyGcodXvR8rd7nW0o5+FuVXgN+vRzJzRnY9WCgoZ33oE1hukfQRNcRvbTzJX/FTlaMHNyuKntnTu\n",
              "uXa2Rl2KfwfE13e6XBOfUnnk9wjbCBUszvqg34S5c29p4ZKu/cKsGT9iKE4BqBmat69BI6ZFuriH\n",
              "OGeA5Ggx89s7YWbYId1h7M25nhcpJyegvlnchEhh23htqVwvLVLva8URCCp/qCTHkrU3uDuxbciw\n",
              "1+gRtFdm0939NFJ7AHial4RionogwHv1teuMsPRSvftvEBGz8jLz91BUJKQFST1Qiamt1oNaq8dA\n",
              "PT/cBNiAqDEAAAFOQZu4SahBbJlMCFf//jhABHVE9y903JUvxzrc6A+e+9s175QTyQQGcFaG1sqq\n",
              "ZCc81qXvoghAJsF/FFHMMzvBNd4EKDYw0vimFlREddkIBas/Ct3SMFgnXXq9uQ4O6juqHLox5SYY\n",
              "pe1kZ2H93YFM97qyNioFhiHC4XI2ExwYOLL/RjuZITuV8AR1rGc1lo9NEE/7ygNf3EmMHTo0BHhN\n",
              "Ruk9UEJGp8fMF5LpeByca7E40pUJmGpDCSvv05ODYY65TT9+omHFSOiOlg9qHfHJcuENqQYABLfs\n",
              "t8jJOS1P6ekAiRwZ/IeqWsy/wZbf7SgRu9zTa2cgw8n4IUIBJkrW2nk0cCj0TpetuYvfm0583XQz\n",
              "WqYzAKvAmxvgWm8wc8eha2uhUVZfJIJf/CAgI4J2IqZCGq42BNA/h+/RkI3BqHfShDOBNjJ5qV0w\n",
              "JhYF0QAABFJtb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAAD6AABAAABAAAAAAAAAAAAAAAA\n",
              "AQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
              "AAAAAAACAAADfHRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAAD6AAAAAAAAAAAAAAA\n",
              "AAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAABsAAAASAAAAAAACRlZHRz\n",
              "AAAAHGVsc3QAAAAAAAAAAQAAA+gAAAQAAAEAAAAAAvRtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAA\n",
              "ADIAAAAyAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAA\n",
              "AAKfbWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVy\n",
              "bCAAAAABAAACX3N0YmwAAACzc3RzZAAAAAAAAAABAAAAo2F2YzEAAAAAAAAAAQAAAAAAAAAAAAAA\n",
              "AAAAAAABsAEgAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY\n",
              "//8AAAAxYXZjQwFkABX/4QAYZ2QAFazZQbCWhAAAAwAEAAADAMg8WLZYAQAGaOvjyyLAAAAAHHV1\n",
              "aWRraEDyXyRPxbo5pRvPAyPzAAAAAAAAABhzdHRzAAAAAAAAAAEAAAAZAAACAAAAABRzdHNzAAAA\n",
              "AAAAAAEAAAABAAAA0GN0dHMAAAAAAAAAGAAAAAEAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAA\n",
              "AAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAA\n",
              "AAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAIAAAA\n",
              "AAIAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAEAAAAABxzdHNjAAAA\n",
              "AAAAAAEAAAABAAAAGQAAAAEAAAB4c3RzegAAAAAAAAAAAAAAGQAACJoAAAJmAAABnQAAAToAAAEm\n",
              "AAABtQAAAVwAAAD4AAABKgAAAm8AAADaAAAAlgAAAQgAAAEDAAAA0QAAANoAAAFUAAABXwAAAQ8A\n",
              "AAEwAAABCwAAANUAAAEuAAABfwAAAVIAAAAUc3RjbwAAAAAAAAABAAAALAAAAGJ1ZHRhAAAAWm1l\n",
              "dGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAA\n",
              "AB1kYXRhAAAAAQAAAABMYXZmNTcuODMuMTAw\n",
              "\">\n",
              "  Your browser does not support the video tag.\n",
              "</video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACtklEQVR4nO3TMQEAIAzAMMC/52GAnx6Jgj7dM7OAnvM7AHgzJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNClDkhypwQZU6IMidEmROizAlR5oQoc0KUOSHKnBBlTogyJ0SZE6LMCVHmhChzQpQ5IcqcEGVOiDInRJkToswJUeaEKHNC1AVcegTL+uSnUAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "tWw9n9H0aB5A"
      },
      "source": [
        "## 6. Define Network\n",
        "The network is the same as before. The one difference is that the for-loop iterates through the first dimension of the input:\n",
        "`cur1 = F.max_pool2d(self.conv1(x[step]), 2)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9rOLmqFPjc5k"
      },
      "source": [
        "### Binarized Layer Modules\n",
        "``Binarize`` converts weights to {-1, 1}.\n",
        "Remove `.mul_(2).add_(1)` for {0, 1}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiR-vDHfjdw1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7Z92BOo4j7Ul"
      },
      "source": [
        "import pdb\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def Binarize(tensor,quant_mode='det'):\n",
        "    if quant_mode=='det':\n",
        "        return tensor.sign()\n",
        "        # tmp = tensor.clone()\n",
        "        # tmp[tensor>0] = 1\n",
        "        # tmp[tensor==0] = 0\n",
        "        # tmp[tensor<0] = -1\n",
        "        # return tmp\n",
        "    else:\n",
        "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
        "\n",
        "\n",
        "class HingeLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HingeLoss,self).__init__()\n",
        "        self.margin=1.0\n",
        "\n",
        "    def hinge_loss(self,input,target):\n",
        "            #import pdb; pdb.set_trace()\n",
        "            output=self.margin-input.mul(target)\n",
        "            output[output.le(0)]=0\n",
        "            return output.mean()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return self.hinge_loss(input,target)\n",
        "\n",
        "class SqrtHingeLossFunction(Function):\n",
        "    def __init__(self):\n",
        "        super(SqrtHingeLossFunction,self).__init__()\n",
        "        self.margin=1.0\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        output=self.margin-input.mul(target)\n",
        "        output[output.le(0)]=0\n",
        "        self.save_for_backward(input, target)\n",
        "        loss=output.mul(output).sum(0).sum(1).div(target.numel())\n",
        "        return loss\n",
        "\n",
        "    def backward(self,grad_output):\n",
        "       input, target = self.saved_tensors\n",
        "       output=self.margin-input.mul(target)\n",
        "       output[output.le(0)]=0\n",
        "       import pdb; pdb.set_trace()\n",
        "       grad_output.resize_as_(input).copy_(target).mul_(-2).mul_(output)\n",
        "       grad_output.mul_(output.ne(0).float())\n",
        "       grad_output.div_(input.numel())\n",
        "       return grad_output,grad_output\n",
        "\n",
        "def Quantize(tensor,quant_mode='det',  params=None, numBits=8):\n",
        "    tensor.clamp_(-2**(numBits-1),2**(numBits-1))\n",
        "    if quant_mode=='det':\n",
        "        tensor=tensor.mul(2**(numBits-1)).round().div(2**(numBits-1))\n",
        "    else:\n",
        "        tensor=tensor.mul(2**(numBits-1)).round().add(torch.rand(tensor.size()).add(-0.5)).div(2**(numBits-1))\n",
        "        quant_fixed(tensor, params)\n",
        "    return tensor\n",
        "\n",
        "# import torch.nn._functions as tnnf\n",
        "\n",
        "\n",
        "class BinarizeLinear(nn.Linear):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        if input.size(1) != 784:\n",
        "            input.data=Binarize(input.data)\n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()\n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "        out = nn.functional.linear(input, self.weight)\n",
        "        if not self.bias is None:\n",
        "            self.bias.org=self.bias.data.clone()\n",
        "            out += self.bias.view(1, -1).expand_as(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BinarizeConv2d(nn.Conv2d):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(BinarizeConv2d, self).__init__(*kargs, **kwargs)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        if input.size(1) != 3:\n",
        "            input.data = Binarize(input.data)\n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()\n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "\n",
        "        out = nn.functional.conv2d(input, self.weight, None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "\n",
        "        if not self.bias is None:\n",
        "            self.bias.org=self.bias.data.clone()\n",
        "            out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QO_sFhGsj_9Y"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # initialize layers\n",
        "        # self.bn0 = nn.BatchNorm1d(784)\n",
        "        self.fc1 = BinarizeLinear(784, num_hidden)\n",
        "        self.lif1 = snn.Stein(alpha=alpha, beta=beta)\n",
        "\n",
        "        self.fc2 = BinarizeLinear(num_hidden, num_outputs)\n",
        "        self.lif2 = snn.Stein(alpha=alpha, beta=beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spk1, syn1, mem1 = self.lif1.init_stein(batch_size, num_hidden)\n",
        "        spk2, syn2, mem2 = self.lif2.init_stein(batch_size, num_outputs)\n",
        "\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x[step])\n",
        "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "net = Net().to(device)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zIpvq5jdaB5B"
      },
      "source": [
        "# spike_fn = FSS.apply\n",
        "# snn.neuron.slope = 50\n",
        "spike_grad = snn.FastSigmoidSurrogate.apply\n",
        "snn.slope = 50\n",
        "\n",
        "def Binarize(tensor):\n",
        "    tensor[tensor > 0] = 1\n",
        "    tensor[tensor<=0] = 0\n",
        "    return tensor\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#     # initialize layers\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "#         self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=12, out_channels=64, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "#         self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.fc2 = nn.Linear(64*5*5, 10, bias= False)\n",
        "#         self.lif3 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Initialize LIF state variables and spike output tensors\n",
        "#         spk1, syn1, mem1 = self.lif1.init_stein(batch_size, 12, 13, 13)\n",
        "#         spk2, syn2, mem2 = self.lif1.init_stein(batch_size, 64, 5, 5)\n",
        "#         spk3, syn3, mem3 = self.lif2.init_stein(batch_size, 10)\n",
        "\n",
        "#         spk3_rec = []\n",
        "#         mem3_rec = []\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             conv1_bin_weight = self.conv1.weight.data.clone()\n",
        "#             self.conv1.weight.data = Binarize(conv1_bin_weight)\n",
        "            \n",
        "#             cur1 = F.max_pool2d(self.conv1(x[step]), 2)\n",
        "#             # cur1 = F.max_pool2d(F.conv2d(x, self.conv1.weight, bias=None, stride=1,\n",
        "#             #                        padding=1), 2)\n",
        "            \n",
        "#             spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "\n",
        "#             conv2_bin_weight = self.conv2.weight.data.clone()\n",
        "#             self.conv2.weight.data = Binarize(conv2_bin_weight)\n",
        "\n",
        "#             cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
        "#             # cur2 = F.max_pool2d(F.conv2d(spk1, self.conv2.weight, bias=None, stride=1,\n",
        "#             #                        padding=1), 2)\n",
        "            \n",
        "#             spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "\n",
        "#             fc_bin_weight = self.fc2.weight.data.clone()\n",
        "#             self.fc2.weight.data = Binarize(fc_bin_weight)\n",
        "\n",
        "#             cur3 = self.fc2(spk2.view(batch_size, -1))\n",
        "#             # cur3 = F.linear(spk2.view(batch_size, -1), self.fc2.weight)\n",
        "#             spk3, syn3, mem3 = self.lif3(cur3, syn3, mem3)\n",
        "\n",
        "#             spk3_rec.append(spk3)\n",
        "#             mem3_rec.append(mem3)\n",
        "\n",
        "#         return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "# net = Net().to(device)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    # initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden, bias=False)\n",
        "        self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs, bias=False)\n",
        "        self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spk1, syn1, mem1 = self.lif1.init_stein(batch_size, num_hidden)\n",
        "        spk2, syn2, mem2 = self.lif2.init_stein(batch_size, num_outputs)\n",
        "\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            fc1_bin_weight = self.fc1.weight.data.clone()\n",
        "            self.fc1.weight.data = Binarize(fc1_bin_weight)\n",
        "            cur1 = self.fc1(x[step])\n",
        "            spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "\n",
        "            fc2_bin_weight = self.fc2.weight.data.clone()\n",
        "            self.fc2.weight.data = Binarize(fc2_bin_weight)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "            # print(spk2.shape)\n",
        "\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "net = Net().to(device)\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         # initialize layers\n",
        "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "#         self.lif1 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.conv2 = nn.Conv2d(in_channels=12, out_channels=64, kernel_size=5, stride=1, padding=1)\n",
        "#         self.lif2 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "#         self.fc2 = nn.Linear(64*5*5, 10)\n",
        "#         self.lif3 = snn.Stein(alpha=alpha, beta=beta, spike_grad=spike_grad)\n",
        "\n",
        "#         # self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=1, padding=0)\n",
        "#         # self.lif1 = LIF(spike_fn=spike_fn, alpha=alpha, beta=beta)\n",
        "#         # self.fc1 = nn.Linear(26*26*3, 10)\n",
        "#         # self.lif2 = LIF(spike_fn=spike_fn, alpha=alpha, beta=beta)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Initialize LIF state variables and spike output tensors\n",
        "#         spk1, syn1, mem1 = self.lif1.init_stein(batch_size, 12, 13, 13)\n",
        "#         spk2, syn2, mem2 = self.lif2.init_stein(batch_size, 64, 5, 5)\n",
        "#         spk3, syn3, mem3 = self.lif3.init_stein(batch_size, 10)\n",
        "\n",
        "#         spk3_rec = []\n",
        "#         mem3_rec = []\n",
        "\n",
        "#         for step in range(num_steps):\n",
        "#             cur1 = F.max_pool2d(self.conv1(x[step]), 2) # add max-pooling to membrane or spikes?\n",
        "#             spk1, syn1, mem1 = self.lif1(cur1, syn1, mem1)\n",
        "#             cur2 = F.max_pool2d(self.conv2(spk1), 2)\n",
        "#             spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "#             cur3 = self.fc2(spk2.view(batch_size, -1))\n",
        "#             spk3, syn3, mem3 = self.lif3(cur3, syn3, mem3)\n",
        "\n",
        "#             spk3_rec.append(spk3)\n",
        "#             mem3_rec.append(mem3)\n",
        "\n",
        "#         return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "# net = Net().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nzpf57Q1aB5B"
      },
      "source": [
        "## 7. Training\n",
        "We make a slight modification to our print-out functions to handle the new first dimension of the input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "w1saOFIFaB5B"
      },
      "source": [
        "def print_batch_accuracy(data, targets, train=False):\n",
        "    output, _ = net(data.view(num_steps, batch_size, -1))\n",
        "    # output, _ = net(data.view(num_steps, batch_size, 1, 28, 28))\n",
        "    _, idx = output.sum(dim=0).max(1)\n",
        "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
        "\n",
        "    if train:\n",
        "        print(f\"Train Set Accuracy: {acc}\")\n",
        "    else:\n",
        "        print(f\"Test Set Accuracy: {acc}\")\n",
        "\n",
        "def train_printer():\n",
        "    print(f\"Epoch {epoch}, Minibatch {minibatch_counter}\")\n",
        "    print(f\"Train Set Loss: {loss_hist[counter]}\")\n",
        "    print(f\"Test Set Loss: {test_loss_hist[counter]}\")\n",
        "    print_batch_accuracy(spike_data, spike_targets, train=True)\n",
        "    print_batch_accuracy(test_spike_data, test_spike_targets, train=False)\n",
        "    print(\"\\n\")\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "V9qnIq0MaB5B"
      },
      "source": [
        "### 7.1 Optimizer & Loss\n",
        "We'll keep our optimizer and loss the exact same as the static MNIST case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KeWZQlzEaB5B"
      },
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=3e-3)\n",
        "log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
        "loss_fn = nn.NLLLoss()"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "VCHfUz7waB5C"
      },
      "source": [
        "### 7.2 Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "H_P1ffX-kIvR"
      },
      "source": [
        "High precision BPTT training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noSeE-t4kN33",
        "outputId": "e9d06af1-4d67-4cca-8671-37ad50e79d5a"
      },
      "source": [
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(10):\n",
        "\n",
        "    minibatch_counter = 0\n",
        "    train_batch = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data_it, targets_it in train_batch:\n",
        "        data_it = data_it.to(device)\n",
        "        targets_it = targets_it.to(device)\n",
        "\n",
        "        # Spike generator\n",
        "        spike_data, spike_targets = spikegen.rate(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps,\n",
        "                                                  gain=1, offset=0, convert_targets=False, temporal_targets=False)\n",
        "        \n",
        "\n",
        "        # Forward pass\n",
        "        output, mem_rec = net(spike_data.view(num_steps, batch_size, -1))\n",
        "        # output, mem_rec = net(spike_data.view(num_steps, batch_size, 1, 28, 28))\n",
        "        log_p_y = log_softmax_fn(mem_rec)\n",
        "        loss_val = torch.zeros(1, dtype=dtype, device=device)\n",
        "\n",
        "        # Sum loss over time steps to perform BPTT\n",
        "        for step in range(num_steps):\n",
        "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
        "\n",
        "\n",
        "        # BNN OPTimization\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        for p in list(net.parameters()):\n",
        "                if hasattr(p,'org'):\n",
        "                    p.data.copy_(p.org)\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        for p in list(net.parameters()):\n",
        "                if hasattr(p,'org'):\n",
        "                    p.org.copy_(p.data.clamp_(-1,1))\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        test_data = itertools.cycle(test_loader)\n",
        "        testdata_it, testtargets_it = next(test_data)\n",
        "        testdata_it = testdata_it.to(device)\n",
        "        testtargets_it = testtargets_it.to(device)\n",
        "\n",
        "        # Test set spike conversion\n",
        "        test_spike_data, test_spike_targets = spikegen.rate(testdata_it, testtargets_it, num_outputs=num_outputs,\n",
        "                                                            num_steps=num_steps, gain=1, offset=0, convert_targets=False,\n",
        "                                                            temporal_targets=False)\n",
        "\n",
        "        # Test set forward pass\n",
        "        test_output, test_mem_rec = net(test_spike_data.view(num_steps, batch_size, -1))\n",
        "        # test_output, test_mem_rec = net(test_spike_data.view(num_steps, batch_size, 1, 28, 28))\n",
        "\n",
        "        # Test set loss\n",
        "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
        "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
        "        loss_val_test = loss_fn(log_p_ytest, testtargets_it)\n",
        "        test_loss_hist.append(loss_val_test.item())\n",
        "\n",
        "        # Print test/train loss/accuracy\n",
        "        if counter % 50 == 0:\n",
        "          train_printer()\n",
        "        minibatch_counter += 1\n",
        "        counter += 1\n",
        "\n",
        "loss_hist_true_grad = loss_hist\n",
        "test_loss_hist_true_grad = test_loss_hist"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 752.9143676757812\n",
            "Test Set Loss: 716.4063720703125\n",
            "Train Set Accuracy: 0.1953125\n",
            "Test Set Accuracy: 0.1640625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 50\n",
            "Train Set Loss: 43.61306381225586\n",
            "Test Set Loss: 38.27437973022461\n",
            "Train Set Accuracy: 0.359375\n",
            "Test Set Accuracy: 0.375\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 100\n",
            "Train Set Loss: 32.950523376464844\n",
            "Test Set Loss: 34.62779998779297\n",
            "Train Set Accuracy: 0.484375\n",
            "Test Set Accuracy: 0.4921875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 150\n",
            "Train Set Loss: 29.86268424987793\n",
            "Test Set Loss: 33.03606414794922\n",
            "Train Set Accuracy: 0.6328125\n",
            "Test Set Accuracy: 0.5\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 200\n",
            "Train Set Loss: 26.673429489135742\n",
            "Test Set Loss: 28.088821411132812\n",
            "Train Set Accuracy: 0.578125\n",
            "Test Set Accuracy: 0.578125\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 250\n",
            "Train Set Loss: 29.636688232421875\n",
            "Test Set Loss: 33.76441955566406\n",
            "Train Set Accuracy: 0.59375\n",
            "Test Set Accuracy: 0.6015625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 300\n",
            "Train Set Loss: 27.4667911529541\n",
            "Test Set Loss: 22.622310638427734\n",
            "Train Set Accuracy: 0.65625\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 350\n",
            "Train Set Loss: 25.72867202758789\n",
            "Test Set Loss: 33.07038879394531\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 400\n",
            "Train Set Loss: 28.624004364013672\n",
            "Test Set Loss: 31.2625675201416\n",
            "Train Set Accuracy: 0.6328125\n",
            "Test Set Accuracy: 0.6171875\n",
            "\n",
            "\n",
            "Epoch 0, Minibatch 450\n",
            "Train Set Loss: 25.17841339111328\n",
            "Test Set Loss: 30.514135360717773\n",
            "Train Set Accuracy: 0.625\n",
            "Test Set Accuracy: 0.5703125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 32\n",
            "Train Set Loss: 31.2055606842041\n",
            "Test Set Loss: 23.983671188354492\n",
            "Train Set Accuracy: 0.6484375\n",
            "Test Set Accuracy: 0.65625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 82\n",
            "Train Set Loss: 30.996675491333008\n",
            "Test Set Loss: 25.478729248046875\n",
            "Train Set Accuracy: 0.6328125\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 132\n",
            "Train Set Loss: 27.169286727905273\n",
            "Test Set Loss: 25.72322654724121\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 182\n",
            "Train Set Loss: 31.067522048950195\n",
            "Test Set Loss: 27.161983489990234\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.609375\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 232\n",
            "Train Set Loss: 31.200347900390625\n",
            "Test Set Loss: 25.43088722229004\n",
            "Train Set Accuracy: 0.734375\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 282\n",
            "Train Set Loss: 30.37548065185547\n",
            "Test Set Loss: 28.510570526123047\n",
            "Train Set Accuracy: 0.59375\n",
            "Test Set Accuracy: 0.578125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 332\n",
            "Train Set Loss: 26.144594192504883\n",
            "Test Set Loss: 27.988683700561523\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.578125\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 382\n",
            "Train Set Loss: 27.44136619567871\n",
            "Test Set Loss: 27.02566909790039\n",
            "Train Set Accuracy: 0.6328125\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 1, Minibatch 432\n",
            "Train Set Loss: 24.701690673828125\n",
            "Test Set Loss: 30.69260597229004\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.5859375\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 14\n",
            "Train Set Loss: 26.066532135009766\n",
            "Test Set Loss: 27.520660400390625\n",
            "Train Set Accuracy: 0.609375\n",
            "Test Set Accuracy: 0.6640625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 64\n",
            "Train Set Loss: 30.841732025146484\n",
            "Test Set Loss: 24.268699645996094\n",
            "Train Set Accuracy: 0.609375\n",
            "Test Set Accuracy: 0.65625\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 114\n",
            "Train Set Loss: 30.468189239501953\n",
            "Test Set Loss: 31.632041931152344\n",
            "Train Set Accuracy: 0.5546875\n",
            "Test Set Accuracy: 0.578125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 164\n",
            "Train Set Loss: 26.323307037353516\n",
            "Test Set Loss: 30.30994415283203\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.671875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 214\n",
            "Train Set Loss: 27.919254302978516\n",
            "Test Set Loss: 27.613746643066406\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.6328125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 264\n",
            "Train Set Loss: 24.745738983154297\n",
            "Test Set Loss: 27.427621841430664\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.71875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 314\n",
            "Train Set Loss: 29.383081436157227\n",
            "Test Set Loss: 22.17011070251465\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 364\n",
            "Train Set Loss: 26.749290466308594\n",
            "Test Set Loss: 31.971370697021484\n",
            "Train Set Accuracy: 0.6484375\n",
            "Test Set Accuracy: 0.5703125\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 414\n",
            "Train Set Loss: 26.082006454467773\n",
            "Test Set Loss: 32.157047271728516\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.6171875\n",
            "\n",
            "\n",
            "Epoch 2, Minibatch 464\n",
            "Train Set Loss: 29.748281478881836\n",
            "Test Set Loss: 26.797348022460938\n",
            "Train Set Accuracy: 0.484375\n",
            "Test Set Accuracy: 0.53125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 46\n",
            "Train Set Loss: 27.376720428466797\n",
            "Test Set Loss: 25.536895751953125\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 96\n",
            "Train Set Loss: 23.90923309326172\n",
            "Test Set Loss: 25.20994758605957\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.6171875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 146\n",
            "Train Set Loss: 27.064958572387695\n",
            "Test Set Loss: 31.4851131439209\n",
            "Train Set Accuracy: 0.5703125\n",
            "Test Set Accuracy: 0.59375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 196\n",
            "Train Set Loss: 29.12230110168457\n",
            "Test Set Loss: 32.907432556152344\n",
            "Train Set Accuracy: 0.6171875\n",
            "Test Set Accuracy: 0.5859375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 246\n",
            "Train Set Loss: 27.985109329223633\n",
            "Test Set Loss: 25.88252067565918\n",
            "Train Set Accuracy: 0.5859375\n",
            "Test Set Accuracy: 0.6171875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 296\n",
            "Train Set Loss: 25.226215362548828\n",
            "Test Set Loss: 25.13644790649414\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 346\n",
            "Train Set Loss: 28.606117248535156\n",
            "Test Set Loss: 21.34741973876953\n",
            "Train Set Accuracy: 0.6484375\n",
            "Test Set Accuracy: 0.734375\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 396\n",
            "Train Set Loss: 25.17705535888672\n",
            "Test Set Loss: 25.650043487548828\n",
            "Train Set Accuracy: 0.625\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 3, Minibatch 446\n",
            "Train Set Loss: 28.25714111328125\n",
            "Test Set Loss: 25.705591201782227\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 28\n",
            "Train Set Loss: 29.14191246032715\n",
            "Test Set Loss: 25.998950958251953\n",
            "Train Set Accuracy: 0.5546875\n",
            "Test Set Accuracy: 0.65625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 78\n",
            "Train Set Loss: 26.298175811767578\n",
            "Test Set Loss: 22.467193603515625\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.75\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 128\n",
            "Train Set Loss: 25.275991439819336\n",
            "Test Set Loss: 32.31241989135742\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.6015625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 178\n",
            "Train Set Loss: 30.065265655517578\n",
            "Test Set Loss: 33.85177993774414\n",
            "Train Set Accuracy: 0.65625\n",
            "Test Set Accuracy: 0.5859375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 228\n",
            "Train Set Loss: 24.661718368530273\n",
            "Test Set Loss: 33.382198333740234\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.5625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 278\n",
            "Train Set Loss: 29.64558982849121\n",
            "Test Set Loss: 31.694049835205078\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.59375\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 328\n",
            "Train Set Loss: 26.05467987060547\n",
            "Test Set Loss: 28.92366600036621\n",
            "Train Set Accuracy: 0.6953125\n",
            "Test Set Accuracy: 0.640625\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 378\n",
            "Train Set Loss: 27.08283233642578\n",
            "Test Set Loss: 30.927797317504883\n",
            "Train Set Accuracy: 0.5859375\n",
            "Test Set Accuracy: 0.546875\n",
            "\n",
            "\n",
            "Epoch 4, Minibatch 428\n",
            "Train Set Loss: 29.229860305786133\n",
            "Test Set Loss: 32.82595443725586\n",
            "Train Set Accuracy: 0.5\n",
            "Test Set Accuracy: 0.5234375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 10\n",
            "Train Set Loss: 28.273096084594727\n",
            "Test Set Loss: 26.871707916259766\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 60\n",
            "Train Set Loss: 27.283592224121094\n",
            "Test Set Loss: 24.9007511138916\n",
            "Train Set Accuracy: 0.65625\n",
            "Test Set Accuracy: 0.65625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 110\n",
            "Train Set Loss: 24.09494400024414\n",
            "Test Set Loss: 32.959285736083984\n",
            "Train Set Accuracy: 0.5546875\n",
            "Test Set Accuracy: 0.5546875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 160\n",
            "Train Set Loss: 26.274925231933594\n",
            "Test Set Loss: 21.839412689208984\n",
            "Train Set Accuracy: 0.6015625\n",
            "Test Set Accuracy: 0.59375\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 210\n",
            "Train Set Loss: 25.9981632232666\n",
            "Test Set Loss: 25.007932662963867\n",
            "Train Set Accuracy: 0.7265625\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 260\n",
            "Train Set Loss: 31.51630973815918\n",
            "Test Set Loss: 30.47732162475586\n",
            "Train Set Accuracy: 0.609375\n",
            "Test Set Accuracy: 0.6015625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 310\n",
            "Train Set Loss: 32.64159393310547\n",
            "Test Set Loss: 29.65479850769043\n",
            "Train Set Accuracy: 0.578125\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 360\n",
            "Train Set Loss: 21.88081932067871\n",
            "Test Set Loss: 31.29279327392578\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.5546875\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 410\n",
            "Train Set Loss: 28.92993927001953\n",
            "Test Set Loss: 23.82870864868164\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.640625\n",
            "\n",
            "\n",
            "Epoch 5, Minibatch 460\n",
            "Train Set Loss: 28.40509033203125\n",
            "Test Set Loss: 32.1148567199707\n",
            "Train Set Accuracy: 0.6015625\n",
            "Test Set Accuracy: 0.515625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 42\n",
            "Train Set Loss: 24.700973510742188\n",
            "Test Set Loss: 25.976665496826172\n",
            "Train Set Accuracy: 0.5859375\n",
            "Test Set Accuracy: 0.5625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 92\n",
            "Train Set Loss: 32.961429595947266\n",
            "Test Set Loss: 27.388416290283203\n",
            "Train Set Accuracy: 0.6796875\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 142\n",
            "Train Set Loss: 40.12074661254883\n",
            "Test Set Loss: 30.993873596191406\n",
            "Train Set Accuracy: 0.4921875\n",
            "Test Set Accuracy: 0.5703125\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 192\n",
            "Train Set Loss: 31.645002365112305\n",
            "Test Set Loss: 25.74726104736328\n",
            "Train Set Accuracy: 0.6328125\n",
            "Test Set Accuracy: 0.703125\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 242\n",
            "Train Set Loss: 25.26706314086914\n",
            "Test Set Loss: 26.042781829833984\n",
            "Train Set Accuracy: 0.6875\n",
            "Test Set Accuracy: 0.640625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 292\n",
            "Train Set Loss: 25.34914779663086\n",
            "Test Set Loss: 29.918628692626953\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 342\n",
            "Train Set Loss: 24.02049446105957\n",
            "Test Set Loss: 30.95746612548828\n",
            "Train Set Accuracy: 0.6171875\n",
            "Test Set Accuracy: 0.5703125\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 392\n",
            "Train Set Loss: 28.0577335357666\n",
            "Test Set Loss: 27.069076538085938\n",
            "Train Set Accuracy: 0.609375\n",
            "Test Set Accuracy: 0.609375\n",
            "\n",
            "\n",
            "Epoch 6, Minibatch 442\n",
            "Train Set Loss: 27.654016494750977\n",
            "Test Set Loss: 28.04557228088379\n",
            "Train Set Accuracy: 0.6484375\n",
            "Test Set Accuracy: 0.65625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 24\n",
            "Train Set Loss: 22.177717208862305\n",
            "Test Set Loss: 28.003116607666016\n",
            "Train Set Accuracy: 0.703125\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 74\n",
            "Train Set Loss: 33.29718017578125\n",
            "Test Set Loss: 31.92647361755371\n",
            "Train Set Accuracy: 0.578125\n",
            "Test Set Accuracy: 0.59375\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 124\n",
            "Train Set Loss: 21.750160217285156\n",
            "Test Set Loss: 26.1025390625\n",
            "Train Set Accuracy: 0.7109375\n",
            "Test Set Accuracy: 0.6875\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 174\n",
            "Train Set Loss: 27.17746925354004\n",
            "Test Set Loss: 32.8870735168457\n",
            "Train Set Accuracy: 0.546875\n",
            "Test Set Accuracy: 0.515625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 224\n",
            "Train Set Loss: 24.799179077148438\n",
            "Test Set Loss: 27.828086853027344\n",
            "Train Set Accuracy: 0.5859375\n",
            "Test Set Accuracy: 0.578125\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 274\n",
            "Train Set Loss: 28.718557357788086\n",
            "Test Set Loss: 27.356754302978516\n",
            "Train Set Accuracy: 0.6171875\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 324\n",
            "Train Set Loss: 30.892410278320312\n",
            "Test Set Loss: 26.13656997680664\n",
            "Train Set Accuracy: 0.5390625\n",
            "Test Set Accuracy: 0.640625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 374\n",
            "Train Set Loss: 21.52770233154297\n",
            "Test Set Loss: 28.763883590698242\n",
            "Train Set Accuracy: 0.75\n",
            "Test Set Accuracy: 0.65625\n",
            "\n",
            "\n",
            "Epoch 7, Minibatch 424\n",
            "Train Set Loss: 30.194398880004883\n",
            "Test Set Loss: 23.35780143737793\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 6\n",
            "Train Set Loss: 26.538063049316406\n",
            "Test Set Loss: 28.60269546508789\n",
            "Train Set Accuracy: 0.5546875\n",
            "Test Set Accuracy: 0.6171875\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 56\n",
            "Train Set Loss: 28.641393661499023\n",
            "Test Set Loss: 25.26190948486328\n",
            "Train Set Accuracy: 0.578125\n",
            "Test Set Accuracy: 0.6953125\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 106\n",
            "Train Set Loss: 28.924856185913086\n",
            "Test Set Loss: 31.477819442749023\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.609375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 156\n",
            "Train Set Loss: 28.241647720336914\n",
            "Test Set Loss: 24.534820556640625\n",
            "Train Set Accuracy: 0.6640625\n",
            "Test Set Accuracy: 0.7265625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 206\n",
            "Train Set Loss: 28.519399642944336\n",
            "Test Set Loss: 27.60616111755371\n",
            "Train Set Accuracy: 0.5859375\n",
            "Test Set Accuracy: 0.5546875\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 256\n",
            "Train Set Loss: 24.24768829345703\n",
            "Test Set Loss: 23.876750946044922\n",
            "Train Set Accuracy: 0.6484375\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 306\n",
            "Train Set Loss: 30.70842933654785\n",
            "Test Set Loss: 27.625436782836914\n",
            "Train Set Accuracy: 0.5546875\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 356\n",
            "Train Set Loss: 33.05601501464844\n",
            "Test Set Loss: 33.46600341796875\n",
            "Train Set Accuracy: 0.671875\n",
            "Test Set Accuracy: 0.5234375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 406\n",
            "Train Set Loss: 27.60590934753418\n",
            "Test Set Loss: 24.41185760498047\n",
            "Train Set Accuracy: 0.625\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 8, Minibatch 456\n",
            "Train Set Loss: 25.15268325805664\n",
            "Test Set Loss: 24.992982864379883\n",
            "Train Set Accuracy: 0.6875\n",
            "Test Set Accuracy: 0.640625\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 38\n",
            "Train Set Loss: 26.702516555786133\n",
            "Test Set Loss: 24.57956886291504\n",
            "Train Set Accuracy: 0.609375\n",
            "Test Set Accuracy: 0.7421875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 88\n",
            "Train Set Loss: 22.535926818847656\n",
            "Test Set Loss: 24.539493560791016\n",
            "Train Set Accuracy: 0.71875\n",
            "Test Set Accuracy: 0.7109375\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 138\n",
            "Train Set Loss: 34.88665008544922\n",
            "Test Set Loss: 29.645694732666016\n",
            "Train Set Accuracy: 0.546875\n",
            "Test Set Accuracy: 0.5078125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 188\n",
            "Train Set Loss: 27.61542510986328\n",
            "Test Set Loss: 26.856874465942383\n",
            "Train Set Accuracy: 0.53125\n",
            "Test Set Accuracy: 0.6015625\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 238\n",
            "Train Set Loss: 38.416404724121094\n",
            "Test Set Loss: 26.82184600830078\n",
            "Train Set Accuracy: 0.625\n",
            "Test Set Accuracy: 0.625\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 288\n",
            "Train Set Loss: 30.68377685546875\n",
            "Test Set Loss: 27.67571449279785\n",
            "Train Set Accuracy: 0.5625\n",
            "Test Set Accuracy: 0.578125\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 338\n",
            "Train Set Loss: 26.161394119262695\n",
            "Test Set Loss: 35.10206985473633\n",
            "Train Set Accuracy: 0.6328125\n",
            "Test Set Accuracy: 0.6484375\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 388\n",
            "Train Set Loss: 28.99748992919922\n",
            "Test Set Loss: 25.72416114807129\n",
            "Train Set Accuracy: 0.640625\n",
            "Test Set Accuracy: 0.6796875\n",
            "\n",
            "\n",
            "Epoch 9, Minibatch 438\n",
            "Train Set Loss: 27.927608489990234\n",
            "Test Set Loss: 27.187501907348633\n",
            "Train Set Accuracy: 0.6171875\n",
            "Test Set Accuracy: 0.609375\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fuzanqrCaB5C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "49fec2d5-6aaf-4bc2-9f36-1dffff939b32"
      },
      "source": [
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(1):\n",
        "    minibatch_counter = 0\n",
        "    data = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data_it, targets_it in data:\n",
        "        data_it = data_it.to(device)\n",
        "        targets_it = targets_it.to(device)\n",
        "\n",
        "        # Spike generator\n",
        "        spike_data, spike_targets = spikegen.rate(data_it, targets_it, num_outputs=num_outputs, num_steps=num_steps,\n",
        "                                                  gain=1, offset=0, convert_targets=False, temporal_targets=False)\n",
        "\n",
        "        # Forward pass\n",
        "        output, mem_rec = net(spike_data.view(num_steps, batch_size, -1))\n",
        "        # output, mem_rec = net(spike_data.view(num_steps, batch_size, 1, 28, 28))\n",
        "        log_p_y = log_softmax_fn(mem_rec)\n",
        "        loss_val = torch.zeros(1, dtype=dtype, device=device)\n",
        "\n",
        "        # Sum loss over time steps to perform BPTT\n",
        "        for step in range(num_steps):\n",
        "          loss_val += loss_fn(log_p_y[step], targets_it)\n",
        "\n",
        "        # Gradient Calculation\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward(retain_graph=True)\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
        "\n",
        "        # Weight Update\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store Loss history\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        test_data = itertools.cycle(test_loader)\n",
        "        testdata_it, testtargets_it = next(test_data)\n",
        "        testdata_it = testdata_it.to(device)\n",
        "        testtargets_it = testtargets_it.to(device)\n",
        "\n",
        "        # Test set spike conversion\n",
        "        test_spike_data, test_spike_targets = spikegen.rate(testdata_it, testtargets_it, num_outputs=num_outputs,\n",
        "                                                            num_steps=num_steps, gain=1, offset=0, convert_targets=False,\n",
        "                                                            temporal_targets=False)\n",
        "\n",
        "        # Test set forward pass\n",
        "        test_output, test_mem_rec = net(test_spike_data.view(num_steps, batch_size, -1))\n",
        "        # test_output, test_mem_rec = net(test_spike_data.view(num_steps, batch_size, 1, 28, 28))\n",
        "\n",
        "        # Test set loss\n",
        "        log_p_ytest = log_softmax_fn(test_mem_rec)\n",
        "        log_p_ytest = log_p_ytest.sum(dim=0)\n",
        "        loss_val_test = loss_fn(log_p_ytest, test_spike_targets)\n",
        "        test_loss_hist.append(loss_val_test.item())\n",
        "\n",
        "        # Print test/train loss/accuracy\n",
        "        if counter % 50 == 0:\n",
        "            train_printer()\n",
        "        minibatch_counter += 1\n",
        "        counter += 1\n",
        "\n",
        "loss_hist_true_grad = loss_hist\n",
        "test_loss_hist_true_grad = test_loss_hist"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Minibatch 0\n",
            "Train Set Loss: 17.53070068359375\n",
            "Test Set Loss: 19.592514038085938\n",
            "Train Set Accuracy: 0.8359375\n",
            "Test Set Accuracy: 0.796875\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-00bc68d644bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# output, mem_rec = net(spike_data.view(num_steps, batch_size, 1, 28, 28))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlog_p_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_softmax_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_rec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-9bfae6026904>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mspk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlif1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mcur2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mspk2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlif2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mspk2_rec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspk2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/snntorch/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, syn, mem)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mspk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0msyn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/snntorch/__init__.py\u001b[0m in \u001b[0;36mfire\u001b[0;34m(self, mem)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mspk_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmem_shift\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mreset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspk_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspk_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mspk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "nVldb0Q9aB5C"
      },
      "source": [
        "## 8. Spiking MNIST Results\n",
        "### 8.1 Plot Training/Test Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "XopHe17ZaB5C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "80effe7e-ff5e-4d7d-a48e-6d6739066257"
      },
      "source": [
        "# Plot Loss\n",
        "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
        "plt.plot(loss_hist)\n",
        "plt.plot(test_loss_hist)\n",
        "plt.legend([\"Test Loss\", \"Train Loss\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lgu-tzsfaB5C"
      },
      "source": [
        "### 8.2 Test Set Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nyIVun6laB5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ebd372-9b45-49b5-833d-4c9dadfc5553"
      },
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "  net.eval()\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # If current batch matches batch_size, just do the usual thing\n",
        "    if images.size()[0] == batch_size:\n",
        "      spike_test, spike_targets = spikegen.rate(images, labels, num_outputs=num_outputs, num_steps=num_steps,\n",
        "                                                            gain=1, offset=0, convert_targets=False, temporal_targets=False)\n",
        "\n",
        "      outputs, _ = net(spike_test.view(num_steps, batch_size, -1))\n",
        "      # outputs, _ = net(spike_test.view(num_steps, batch_size, 1, 28, 28))\n",
        "\n",
        "    # If current batch does not match batch_size (e.g., is the final minibatch),\n",
        "    # modify batch_size in a temp variable and restore it at the end of the else block\n",
        "    else:\n",
        "      temp_bs = batch_size\n",
        "      batch_size = images.size()[0]\n",
        "      spike_test, spike_targets = spikegen.rate(images, labels, num_outputs=num_outputs, num_steps=num_steps,\n",
        "                                                            gain=1, offset=0, convert_targets=False, temporal_targets=False)\n",
        "      outputs, _ = net(spike_test.view(num_steps, images.size()[0], -1))\n",
        "      # outputs, _ = net(spike_test.view(num_steps, images.size()[0], 1, 28, 28))\n",
        "      batch_size = temp_bs\n",
        "\n",
        "    _, predicted = outputs.sum(dim=0).max(1)\n",
        "    total += spike_targets.size(0)\n",
        "    correct += (predicted == spike_targets).sum().item()\n",
        "\n",
        "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
        "print(f\"Test Set Accuracy: {100 * correct / total}%\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total correctly classified test set images: 8466/10000\n",
            "Test Set Accuracy: 84.66%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "WcBkni9VaB5D"
      },
      "source": [
        "Professor Lu has kidnapped my daughter and won't return her until I hit 99.99% accuracy, please help\n",
        "-JE"
      ]
    }
  ]
}